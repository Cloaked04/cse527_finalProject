{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyN6mxZFGbQEbbreLX4PMk9G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"6f1c2be67b2141f8a8ffd808e9030e24":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_15ad07f1bfcc4a76a86f2ed3cb439d11","IPY_MODEL_4eed288f8e77411f93466ce1b2a1048b"],"layout":"IPY_MODEL_de4fc95200f84c50aba856ddae735c07"}},"15ad07f1bfcc4a76a86f2ed3cb439d11":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_85ff53b51fab4baf849698346a70f8d9","placeholder":"​","style":"IPY_MODEL_5407aeb9ddf24c78b98b85296fcf3dce","value":"0.030 MB of 0.030 MB uploaded\r"}},"4eed288f8e77411f93466ce1b2a1048b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_b472f630c01641a998d72ead91a8b620","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_18cb4da94ff1475383b4225fea10038f","value":1}},"de4fc95200f84c50aba856ddae735c07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85ff53b51fab4baf849698346a70f8d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5407aeb9ddf24c78b98b85296fcf3dce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b472f630c01641a998d72ead91a8b620":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18cb4da94ff1475383b4225fea10038f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"KTR3M0Rm4E-c","executionInfo":{"status":"ok","timestamp":1732784840751,"user_tz":300,"elapsed":10764,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"}}},"outputs":[],"source":["import torch\n","from torch import nn, optim\n","from torch.utils.data import DataLoader\n","import torchvision\n","from torchvision import datasets, transforms\n","import wandb\n","import os\n","import numpy as np\n","from einops import rearrange\n","from einops.layers.torch import Rearrange\n","import math\n","import random\n","import copy"]},{"cell_type":"code","source":["# Define MixUp Function\n","def mixup_data(x, y, alpha=0.2):\n","    '''Returns mixed inputs, pairs of targets, and lambda'''\n","    if alpha > 0:\n","        lam = np.random.beta(alpha, alpha)\n","    else:\n","        lam = 1\n","    batch_size = x.size()[0]\n","    index = torch.randperm(batch_size).to(x.device)\n","\n","    mixed_x = lam * x + (1 - lam) * x[index, :]\n","    y_a, y_b = y, y[index]\n","    return mixed_x, y_a, y_b, lam\n","\n","# Define MixUp Criterion\n","def mixup_criterion(criterion, pred, y_a, y_b, lam):\n","    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n"],"metadata":{"id":"M2O3CR366mru","executionInfo":{"status":"ok","timestamp":1732784840755,"user_tz":300,"elapsed":33,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class Attention(nn.Module):\n","    def __init__(self, dim, *, dim_head=64, heads=8, dropout=0.0):\n","        super().__init__()\n","        inner_dim = dim_head * heads\n","        self.heads = heads\n","        self.dim_head = dim_head\n","        self.scale = dim_head ** -0.5\n","\n","        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n","        self.attn_drop = nn.Dropout(dropout)\n","        self.proj = nn.Linear(inner_dim, dim)\n","        self.proj_drop = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        B, N, C = x.shape\n","        qkv = self.to_qkv(x)\n","        qkv = qkv.reshape(B, N, 3, self.heads, self.dim_head)\n","        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, heads, N, dim_head)\n","        q, k, v = qkv[0], qkv[1], qkv[2]\n","        q = q * self.scale\n","        attn = torch.matmul(q, k.transpose(-2, -1))\n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","        out = torch.matmul(attn, v)\n","        out = out.transpose(1, 2).reshape(B, N, -1)\n","        out = self.proj(out)\n","        out = self.proj_drop(out)\n","        return out\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, dim, dim_inner, dropout=0.0):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(dim, dim_inner),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(dim_inner, dim),\n","            nn.Dropout(dropout)\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","def pair(t):\n","    return t if isinstance(t, tuple) else (t, t)"],"metadata":{"id":"mRbBAYEb6rxb","executionInfo":{"status":"ok","timestamp":1732784840756,"user_tz":300,"elapsed":30,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Modified ViT for CIFAR-100\n","class ViT(nn.Module):\n","    def __init__(\n","        self,\n","        *,\n","        image_size,\n","        patch_size,\n","        num_classes,\n","        dim,\n","        depth,\n","        heads,\n","        mlp_dim,\n","        dropout=0.0,\n","        emb_dropout=0.0,\n","        channels=3,\n","        dim_head=64\n","    ):\n","        super().__init__()\n","        image_height, image_width = pair(image_size)\n","        patch_height, patch_width = pair(patch_size)\n","\n","        assert image_height % patch_height == 0 and image_width % patch_width == 0, \\\n","            'Image dimensions must be divisible by the patch size.'\n","\n","        num_patches = (image_height // patch_height) * (image_width // patch_width)\n","        patch_dim = channels * patch_height * patch_width\n","\n","        self.patch_size = patch_size\n","        self.dim = dim\n","\n","        self.to_patch_embedding = nn.Sequential(\n","            Rearrange('b c (h ph) (w pw) -> b (h w) (ph pw c)', ph=patch_height, pw=patch_width),\n","            nn.Linear(patch_dim, dim)\n","        )\n","\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n","        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n","        self.dropout = nn.Dropout(emb_dropout)\n","\n","        self.transformer = nn.ModuleList([])\n","        for _ in range(depth):\n","            self.transformer.append(nn.ModuleList([\n","                nn.LayerNorm(dim),\n","                Attention(dim, dim_head=dim_head, heads=heads, dropout=dropout),\n","                nn.LayerNorm(dim),\n","                FeedForward(dim, dim_inner=mlp_dim, dropout=dropout)\n","            ]))\n","\n","        self.mlp_head = nn.Sequential(\n","            nn.LayerNorm(dim),\n","            nn.Linear(dim, num_classes)\n","        )\n","\n","    def forward(self, img):\n","        x = self.to_patch_embedding(img)\n","        B, N, _ = x.shape\n","\n","        cls_tokens = self.cls_token.expand(B, -1, -1)\n","        x = torch.cat((cls_tokens, x), dim=1)\n","        x = x + self.pos_embedding[:, :N + 1, :]\n","        x = self.dropout(x)\n","\n","        for norm1, attn, norm2, ff in self.transformer:\n","            x = x + attn(norm1(x))\n","            x = x + ff(norm2(x))\n","\n","        x = x[:, 0]\n","        x = self.mlp_head(x)\n","        return x"],"metadata":{"id":"5iouVTpx6uqW","executionInfo":{"status":"ok","timestamp":1732784840757,"user_tz":300,"elapsed":29,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def main():\n","    # Set random seeds for reproducibility\n","    torch.manual_seed(42)\n","    torch.cuda.manual_seed_all(42)\n","    np.random.seed(42)\n","    random.seed(42)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","    # Initialize wandb\n","    wandb.init(project='vit-cifar100', config={\n","        'model': 'ViT',\n","        'dataset': 'CIFAR-100',\n","        'epochs': 200,  # Increased epochs for better convergence\n","        'batch_size': 128,\n","        'learning_rate': 3e-4,\n","        'weight_decay': 5e-4,  # Reduced from 5e-2\n","        'image_size': 32,\n","        'patch_size': 4,\n","        'dim': 256,             # Reduced from 384\n","        'depth': 8,             # Reduced from 12\n","        'heads': 8,             # Increased from 6\n","        'mlp_dim': 256 * 4,     # Adjusted to match dim\n","        'dropout': 0.1,\n","        'emb_dropout': 0.1,\n","        'num_classes': 100,\n","        'mixup_alpha': 0.2,     # Added MixUp alpha\n","        'label_smoothing': 0.1  # Added label smoothing\n","    })\n","    config = wandb.config\n","\n","    # Set device\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    # Data transforms for CIFAR-100 with additional augmentations\n","    transform_train = transforms.Compose([\n","        transforms.RandomCrop(32, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","        transforms.RandomRotation(15),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n","    ])\n","\n","    transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n","    ])\n","\n","    # Load CIFAR-100 dataset\n","    train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n","    test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2, pin_memory=True)\n","    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n","\n","    # Initialize model\n","    model = ViT(\n","        image_size=config.image_size,\n","        patch_size=config.patch_size,\n","        num_classes=config.num_classes,\n","        dim=config.dim,\n","        depth=config.depth,\n","        heads=config.heads,\n","        mlp_dim=config.mlp_dim,\n","        dropout=config.dropout,\n","        emb_dropout=config.emb_dropout,\n","        channels=3,\n","        dim_head=64\n","    ).to(device)\n","\n","    # Define Loss Function with Label Smoothing\n","    criterion = nn.CrossEntropyLoss(label_smoothing=config.label_smoothing)\n","\n","    # Optimizer with parameter-wise weight decay (no decay for bias and norm parameters)\n","    no_decay = ['bias', 'LayerNorm.weight']\n","    optimizer_grouped_parameters = [\n","        {\n","            'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            'weight_decay': config.weight_decay\n","        },\n","        {\n","            'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","            'weight_decay': 0.0\n","        }\n","    ]\n","    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=config.learning_rate)\n","\n","    # Learning rate scheduler with cosine annealing and warmup\n","    total_steps = config.epochs * len(train_loader)\n","    warmup_steps = int(0.1 * total_steps)  # 10% of total steps for warmup\n","\n","    def lr_lambda(current_step):\n","        if current_step < warmup_steps:\n","            return float(current_step) / float(max(1, warmup_steps))\n","        return 0.5 * (1. + math.cos(math.pi * (current_step - warmup_steps) / (total_steps - warmup_steps)))\n","\n","    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n","\n","    # Training loop with Early Stopping\n","    best_acc = 0.0\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    patience = 20  # Number of epochs to wait for improvement\n","    trigger_times = 0\n","\n","    for epoch in range(config.epochs):\n","        model.train()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        for batch_idx, (inputs, targets) in enumerate(train_loader):\n","            # Apply MixUp\n","            inputs, targets_a, targets_b, lam = mixup_data(inputs, targets, alpha=config.mixup_alpha)\n","            inputs, targets_a, targets_b = map(lambda x: x.to(device), (inputs, targets_a, targets_b))\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n","            loss.backward()\n","\n","            # Gradient clipping\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","            optimizer.step()\n","            scheduler.step()\n","\n","            running_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            # Approximate correct predictions with MixUp\n","            correct += (lam * predicted.eq(targets_a).sum().item() + (1 - lam) * predicted.eq(targets_b).sum().item())\n","\n","            if batch_idx % 100 == 0:\n","                wandb.log({\n","                    'train_loss': running_loss / (batch_idx + 1),\n","                    'train_acc': 100. * correct / total,\n","                    'learning_rate': optimizer.param_groups[0]['lr']\n","                })\n","\n","        train_loss = running_loss / len(train_loader)\n","        train_acc = 100. * correct / total\n","\n","        # Validation Phase\n","        model.eval()\n","        test_loss = 0.0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, targets in test_loader:\n","                inputs, targets = inputs.to(device), targets.to(device)\n","                outputs = model(inputs)\n","                loss = criterion(outputs, targets)\n","\n","                test_loss += loss.item()\n","                _, predicted = outputs.max(1)\n","                total += targets.size(0)\n","                correct += predicted.eq(targets).sum().item()\n","\n","        acc = 100. * correct / total\n","        avg_test_loss = test_loss / len(test_loader)\n","        wandb.log({\n","            'test_loss': avg_test_loss,\n","            'test_acc': acc,\n","            'epoch': epoch\n","        })\n","\n","        # Early Stopping Check\n","        if acc > best_acc:\n","            best_acc = acc\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","            trigger_times = 0\n","            # Save the best model\n","            torch.save(model.state_dict(), 'best_cifar100_vit.pth')\n","        else:\n","            trigger_times += 1\n","            if trigger_times >= patience:\n","                print(\"Early stopping triggered!\")\n","                break\n","\n","        print(f\"Epoch {epoch + 1}/{config.epochs} - \"\n","              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n","              f\"Test Loss: {avg_test_loss:.4f}, Test Acc: {acc:.2f}%\")\n","\n","        # Log additional hyperparameters and metrics at the end of each epoch\n","        wandb.log({\n","            'epoch': epoch,\n","            'best_test_acc': best_acc\n","        })\n","\n","    # Load best model weights\n","    model.load_state_dict(best_model_wts)\n","    print(f\"Training completed. Best Test Accuracy: {best_acc:.2f}%\")\n","    wandb.finish()\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"id":"cJUWfciv6xel","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["6f1c2be67b2141f8a8ffd808e9030e24","15ad07f1bfcc4a76a86f2ed3cb439d11","4eed288f8e77411f93466ce1b2a1048b","de4fc95200f84c50aba856ddae735c07","85ff53b51fab4baf849698346a70f8d9","5407aeb9ddf24c78b98b85296fcf3dce","b472f630c01641a998d72ead91a8b620","18cb4da94ff1475383b4225fea10038f"]},"executionInfo":{"status":"ok","timestamp":1732790676978,"user_tz":300,"elapsed":1418792,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"}},"outputId":"7441c9d4-548e-49f6-aab0-e29f7b2af95a"},"execution_count":5,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"]},{"data":{"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["Tracking run with wandb version 0.18.7"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20241128_090750-x1rf9hz9</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100/runs/x1rf9hz9' target=\"_blank\">earthy-valley-9</a></strong> to <a href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100/runs/x1rf9hz9' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100/runs/x1rf9hz9</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 169M/169M [00:14<00:00, 12.0MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-100-python.tar.gz to ./data\n","Files already downloaded and verified\n","Epoch 1/200 - Train Loss: 4.6189, Train Acc: 1.76%, Test Loss: 4.4302, Test Acc: 3.60%\n","Epoch 2/200 - Train Loss: 4.4196, Train Acc: 3.72%, Test Loss: 4.2983, Test Acc: 5.44%\n","Epoch 3/200 - Train Loss: 4.3081, Train Acc: 5.62%, Test Loss: 4.1055, Test Acc: 8.44%\n","Epoch 4/200 - Train Loss: 4.1747, Train Acc: 8.08%, Test Loss: 3.9421, Test Acc: 12.25%\n","Epoch 5/200 - Train Loss: 4.0586, Train Acc: 10.59%, Test Loss: 3.7716, Test Acc: 15.75%\n","Epoch 6/200 - Train Loss: 3.9679, Train Acc: 12.57%, Test Loss: 3.6162, Test Acc: 18.72%\n","Epoch 7/200 - Train Loss: 3.8567, Train Acc: 15.03%, Test Loss: 3.5115, Test Acc: 21.76%\n","Epoch 8/200 - Train Loss: 3.7644, Train Acc: 16.96%, Test Loss: 3.4175, Test Acc: 23.43%\n","Epoch 9/200 - Train Loss: 3.6967, Train Acc: 18.54%, Test Loss: 3.3444, Test Acc: 25.05%\n","Epoch 10/200 - Train Loss: 3.6455, Train Acc: 19.78%, Test Loss: 3.2459, Test Acc: 28.17%\n","Epoch 11/200 - Train Loss: 3.5962, Train Acc: 20.97%, Test Loss: 3.1972, Test Acc: 29.02%\n","Epoch 12/200 - Train Loss: 3.5618, Train Acc: 21.82%, Test Loss: 3.1589, Test Acc: 29.97%\n","Epoch 13/200 - Train Loss: 3.5481, Train Acc: 22.22%, Test Loss: 3.1219, Test Acc: 30.71%\n","Epoch 14/200 - Train Loss: 3.4313, Train Acc: 24.73%, Test Loss: 3.0351, Test Acc: 33.29%\n","Epoch 15/200 - Train Loss: 3.4160, Train Acc: 25.03%, Test Loss: 3.0267, Test Acc: 33.48%\n","Epoch 16/200 - Train Loss: 3.4092, Train Acc: 25.82%, Test Loss: 3.0004, Test Acc: 34.32%\n","Epoch 17/200 - Train Loss: 3.3368, Train Acc: 27.18%, Test Loss: 2.9873, Test Acc: 33.85%\n","Epoch 18/200 - Train Loss: 3.3238, Train Acc: 27.70%, Test Loss: 2.9061, Test Acc: 36.02%\n","Epoch 19/200 - Train Loss: 3.2906, Train Acc: 28.63%, Test Loss: 2.8802, Test Acc: 36.78%\n","Epoch 20/200 - Train Loss: 3.2354, Train Acc: 30.10%, Test Loss: 2.8413, Test Acc: 37.90%\n","Epoch 21/200 - Train Loss: 3.2443, Train Acc: 30.33%, Test Loss: 2.8487, Test Acc: 37.38%\n","Epoch 22/200 - Train Loss: 3.1988, Train Acc: 31.23%, Test Loss: 2.7864, Test Acc: 39.60%\n","Epoch 23/200 - Train Loss: 3.1496, Train Acc: 32.39%, Test Loss: 2.7127, Test Acc: 41.30%\n","Epoch 24/200 - Train Loss: 3.0941, Train Acc: 33.95%, Test Loss: 2.7065, Test Acc: 41.63%\n","Epoch 25/200 - Train Loss: 3.0259, Train Acc: 35.28%, Test Loss: 2.6390, Test Acc: 43.63%\n","Epoch 26/200 - Train Loss: 3.0253, Train Acc: 35.81%, Test Loss: 2.5950, Test Acc: 44.59%\n","Epoch 27/200 - Train Loss: 2.9719, Train Acc: 37.24%, Test Loss: 2.6043, Test Acc: 44.31%\n","Epoch 28/200 - Train Loss: 2.9706, Train Acc: 37.38%, Test Loss: 2.5987, Test Acc: 44.79%\n","Epoch 29/200 - Train Loss: 2.8795, Train Acc: 39.61%, Test Loss: 2.5423, Test Acc: 46.28%\n","Epoch 30/200 - Train Loss: 2.8535, Train Acc: 40.16%, Test Loss: 2.5300, Test Acc: 46.85%\n","Epoch 31/200 - Train Loss: 2.8749, Train Acc: 40.26%, Test Loss: 2.5119, Test Acc: 47.44%\n","Epoch 32/200 - Train Loss: 2.8536, Train Acc: 40.86%, Test Loss: 2.4966, Test Acc: 48.11%\n","Epoch 33/200 - Train Loss: 2.7969, Train Acc: 42.14%, Test Loss: 2.4878, Test Acc: 48.15%\n","Epoch 34/200 - Train Loss: 2.7341, Train Acc: 43.58%, Test Loss: 2.4761, Test Acc: 48.32%\n","Epoch 35/200 - Train Loss: 2.6763, Train Acc: 45.44%, Test Loss: 2.4479, Test Acc: 48.88%\n","Epoch 36/200 - Train Loss: 2.7111, Train Acc: 44.70%, Test Loss: 2.4482, Test Acc: 49.00%\n","Epoch 37/200 - Train Loss: 2.6726, Train Acc: 46.14%, Test Loss: 2.4217, Test Acc: 49.80%\n","Epoch 38/200 - Train Loss: 2.6118, Train Acc: 47.54%, Test Loss: 2.4053, Test Acc: 50.56%\n","Epoch 39/200 - Train Loss: 2.5023, Train Acc: 50.02%, Test Loss: 2.3921, Test Acc: 51.27%\n","Epoch 40/200 - Train Loss: 2.6057, Train Acc: 47.99%, Test Loss: 2.3945, Test Acc: 51.36%\n","Epoch 41/200 - Train Loss: 2.5565, Train Acc: 49.24%, Test Loss: 2.3667, Test Acc: 52.01%\n","Epoch 42/200 - Train Loss: 2.5394, Train Acc: 50.19%, Test Loss: 2.4098, Test Acc: 50.79%\n","Epoch 43/200 - Train Loss: 2.5565, Train Acc: 49.89%, Test Loss: 2.3835, Test Acc: 51.75%\n","Epoch 44/200 - Train Loss: 2.4594, Train Acc: 52.09%, Test Loss: 2.3786, Test Acc: 51.94%\n","Epoch 45/200 - Train Loss: 2.4178, Train Acc: 53.28%, Test Loss: 2.3793, Test Acc: 52.20%\n","Epoch 46/200 - Train Loss: 2.5116, Train Acc: 51.48%, Test Loss: 2.3592, Test Acc: 52.20%\n","Epoch 47/200 - Train Loss: 2.3660, Train Acc: 55.15%, Test Loss: 2.3661, Test Acc: 52.12%\n","Epoch 48/200 - Train Loss: 2.4046, Train Acc: 54.81%, Test Loss: 2.3657, Test Acc: 52.89%\n","Epoch 49/200 - Train Loss: 2.2610, Train Acc: 58.13%, Test Loss: 2.3176, Test Acc: 54.36%\n","Epoch 50/200 - Train Loss: 2.3076, Train Acc: 57.44%, Test Loss: 2.3286, Test Acc: 54.28%\n","Epoch 51/200 - Train Loss: 2.3462, Train Acc: 56.66%, Test Loss: 2.3586, Test Acc: 53.09%\n","Epoch 52/200 - Train Loss: 2.3553, Train Acc: 56.37%, Test Loss: 2.3482, Test Acc: 53.32%\n","Epoch 53/200 - Train Loss: 2.3042, Train Acc: 57.93%, Test Loss: 2.3239, Test Acc: 54.69%\n","Epoch 54/200 - Train Loss: 2.2252, Train Acc: 60.17%, Test Loss: 2.3399, Test Acc: 54.13%\n","Epoch 55/200 - Train Loss: 2.2605, Train Acc: 59.53%, Test Loss: 2.3474, Test Acc: 53.77%\n","Epoch 56/200 - Train Loss: 2.2155, Train Acc: 60.83%, Test Loss: 2.3483, Test Acc: 53.55%\n","Epoch 57/200 - Train Loss: 2.2051, Train Acc: 61.40%, Test Loss: 2.3483, Test Acc: 53.99%\n","Epoch 58/200 - Train Loss: 2.1151, Train Acc: 63.64%, Test Loss: 2.3854, Test Acc: 53.18%\n","Epoch 59/200 - Train Loss: 2.0740, Train Acc: 65.22%, Test Loss: 2.3371, Test Acc: 54.45%\n","Epoch 60/200 - Train Loss: 2.1626, Train Acc: 63.47%, Test Loss: 2.3691, Test Acc: 53.54%\n","Epoch 61/200 - Train Loss: 2.1031, Train Acc: 64.92%, Test Loss: 2.3539, Test Acc: 54.25%\n","Epoch 62/200 - Train Loss: 2.1208, Train Acc: 64.74%, Test Loss: 2.3510, Test Acc: 53.54%\n","Epoch 63/200 - Train Loss: 2.0697, Train Acc: 65.82%, Test Loss: 2.3558, Test Acc: 54.24%\n","Epoch 64/200 - Train Loss: 2.0402, Train Acc: 66.94%, Test Loss: 2.3775, Test Acc: 54.29%\n","Epoch 65/200 - Train Loss: 2.0422, Train Acc: 67.04%, Test Loss: 2.3506, Test Acc: 54.58%\n","Epoch 66/200 - Train Loss: 2.0699, Train Acc: 66.67%, Test Loss: 2.3288, Test Acc: 55.17%\n","Epoch 67/200 - Train Loss: 2.0219, Train Acc: 67.89%, Test Loss: 2.3735, Test Acc: 54.32%\n","Epoch 68/200 - Train Loss: 2.0194, Train Acc: 68.33%, Test Loss: 2.3255, Test Acc: 55.26%\n","Epoch 69/200 - Train Loss: 1.9650, Train Acc: 69.64%, Test Loss: 2.3418, Test Acc: 55.18%\n","Epoch 70/200 - Train Loss: 1.9135, Train Acc: 71.25%, Test Loss: 2.3371, Test Acc: 55.35%\n","Epoch 71/200 - Train Loss: 1.9743, Train Acc: 69.94%, Test Loss: 2.3515, Test Acc: 55.22%\n","Epoch 72/200 - Train Loss: 1.9557, Train Acc: 70.50%, Test Loss: 2.3477, Test Acc: 54.96%\n","Epoch 73/200 - Train Loss: 1.9417, Train Acc: 70.94%, Test Loss: 2.3497, Test Acc: 54.94%\n","Epoch 74/200 - Train Loss: 1.9115, Train Acc: 72.22%, Test Loss: 2.3614, Test Acc: 55.56%\n","Epoch 75/200 - Train Loss: 1.9655, Train Acc: 70.83%, Test Loss: 2.3312, Test Acc: 55.65%\n","Epoch 76/200 - Train Loss: 1.9208, Train Acc: 71.72%, Test Loss: 2.3215, Test Acc: 55.50%\n","Epoch 77/200 - Train Loss: 1.9178, Train Acc: 72.03%, Test Loss: 2.3487, Test Acc: 55.65%\n","Epoch 78/200 - Train Loss: 1.8846, Train Acc: 72.98%, Test Loss: 2.3553, Test Acc: 55.83%\n","Epoch 79/200 - Train Loss: 1.8685, Train Acc: 73.71%, Test Loss: 2.3247, Test Acc: 56.70%\n","Epoch 80/200 - Train Loss: 1.8631, Train Acc: 74.01%, Test Loss: 2.3117, Test Acc: 55.93%\n","Epoch 81/200 - Train Loss: 1.8664, Train Acc: 73.79%, Test Loss: 2.3283, Test Acc: 55.82%\n","Epoch 82/200 - Train Loss: 1.8550, Train Acc: 74.21%, Test Loss: 2.3539, Test Acc: 55.53%\n","Epoch 83/200 - Train Loss: 1.8608, Train Acc: 74.12%, Test Loss: 2.3446, Test Acc: 55.80%\n","Epoch 84/200 - Train Loss: 1.8297, Train Acc: 74.96%, Test Loss: 2.3304, Test Acc: 56.22%\n","Epoch 85/200 - Train Loss: 1.8819, Train Acc: 73.70%, Test Loss: 2.3501, Test Acc: 55.87%\n","Epoch 86/200 - Train Loss: 1.8058, Train Acc: 75.92%, Test Loss: 2.3671, Test Acc: 56.24%\n","Epoch 87/200 - Train Loss: 1.8151, Train Acc: 76.02%, Test Loss: 2.3297, Test Acc: 56.33%\n","Epoch 88/200 - Train Loss: 1.8582, Train Acc: 74.76%, Test Loss: 2.3302, Test Acc: 55.84%\n","Epoch 89/200 - Train Loss: 1.8571, Train Acc: 74.75%, Test Loss: 2.3109, Test Acc: 56.80%\n","Epoch 90/200 - Train Loss: 1.7662, Train Acc: 77.22%, Test Loss: 2.3273, Test Acc: 56.90%\n","Epoch 91/200 - Train Loss: 1.6896, Train Acc: 79.22%, Test Loss: 2.3171, Test Acc: 56.60%\n","Epoch 92/200 - Train Loss: 1.8489, Train Acc: 75.06%, Test Loss: 2.3175, Test Acc: 56.69%\n","Epoch 93/200 - Train Loss: 1.7796, Train Acc: 76.93%, Test Loss: 2.3348, Test Acc: 56.16%\n","Epoch 94/200 - Train Loss: 1.8007, Train Acc: 76.57%, Test Loss: 2.3353, Test Acc: 56.47%\n","Epoch 95/200 - Train Loss: 1.8002, Train Acc: 76.78%, Test Loss: 2.3247, Test Acc: 56.93%\n","Epoch 96/200 - Train Loss: 1.8010, Train Acc: 76.71%, Test Loss: 2.2931, Test Acc: 56.98%\n","Epoch 97/200 - Train Loss: 1.7924, Train Acc: 76.93%, Test Loss: 2.3121, Test Acc: 57.03%\n","Epoch 98/200 - Train Loss: 1.7988, Train Acc: 76.79%, Test Loss: 2.3005, Test Acc: 56.92%\n","Epoch 99/200 - Train Loss: 1.8073, Train Acc: 76.74%, Test Loss: 2.3026, Test Acc: 56.78%\n","Epoch 100/200 - Train Loss: 1.6968, Train Acc: 79.52%, Test Loss: 2.3277, Test Acc: 57.10%\n","Epoch 101/200 - Train Loss: 1.6200, Train Acc: 81.48%, Test Loss: 2.3306, Test Acc: 56.63%\n","Epoch 102/200 - Train Loss: 1.6898, Train Acc: 79.91%, Test Loss: 2.3067, Test Acc: 57.25%\n","Epoch 103/200 - Train Loss: 1.7023, Train Acc: 79.34%, Test Loss: 2.3023, Test Acc: 57.34%\n","Epoch 104/200 - Train Loss: 1.7005, Train Acc: 79.53%, Test Loss: 2.3050, Test Acc: 56.88%\n","Epoch 105/200 - Train Loss: 1.5993, Train Acc: 82.23%, Test Loss: 2.3063, Test Acc: 57.27%\n","Epoch 106/200 - Train Loss: 1.6848, Train Acc: 79.74%, Test Loss: 2.2964, Test Acc: 57.33%\n","Epoch 107/200 - Train Loss: 1.7005, Train Acc: 79.60%, Test Loss: 2.3113, Test Acc: 57.20%\n","Epoch 108/200 - Train Loss: 1.6727, Train Acc: 80.28%, Test Loss: 2.2974, Test Acc: 57.83%\n","Epoch 109/200 - Train Loss: 1.6668, Train Acc: 80.19%, Test Loss: 2.3045, Test Acc: 57.10%\n","Epoch 110/200 - Train Loss: 1.6513, Train Acc: 81.01%, Test Loss: 2.3034, Test Acc: 57.76%\n","Epoch 111/200 - Train Loss: 1.6225, Train Acc: 81.81%, Test Loss: 2.2958, Test Acc: 57.66%\n","Epoch 112/200 - Train Loss: 1.6499, Train Acc: 80.85%, Test Loss: 2.3013, Test Acc: 57.38%\n","Epoch 113/200 - Train Loss: 1.7388, Train Acc: 78.78%, Test Loss: 2.3072, Test Acc: 57.20%\n","Epoch 114/200 - Train Loss: 1.6183, Train Acc: 81.78%, Test Loss: 2.2974, Test Acc: 57.86%\n","Epoch 115/200 - Train Loss: 1.5586, Train Acc: 83.47%, Test Loss: 2.2973, Test Acc: 57.71%\n","Epoch 116/200 - Train Loss: 1.5380, Train Acc: 83.90%, Test Loss: 2.3053, Test Acc: 57.03%\n","Epoch 117/200 - Train Loss: 1.5552, Train Acc: 83.37%, Test Loss: 2.3001, Test Acc: 57.44%\n","Epoch 118/200 - Train Loss: 1.7201, Train Acc: 79.11%, Test Loss: 2.3091, Test Acc: 57.09%\n","Epoch 119/200 - Train Loss: 1.6623, Train Acc: 80.70%, Test Loss: 2.2880, Test Acc: 58.34%\n","Epoch 120/200 - Train Loss: 1.5736, Train Acc: 83.26%, Test Loss: 2.3125, Test Acc: 57.57%\n","Epoch 121/200 - Train Loss: 1.6221, Train Acc: 81.71%, Test Loss: 2.2915, Test Acc: 57.28%\n","Epoch 122/200 - Train Loss: 1.6621, Train Acc: 80.30%, Test Loss: 2.2868, Test Acc: 57.74%\n","Epoch 123/200 - Train Loss: 1.5714, Train Acc: 83.25%, Test Loss: 2.2972, Test Acc: 57.69%\n","Epoch 124/200 - Train Loss: 1.6403, Train Acc: 81.58%, Test Loss: 2.2968, Test Acc: 58.12%\n","Epoch 125/200 - Train Loss: 1.6421, Train Acc: 81.18%, Test Loss: 2.3004, Test Acc: 57.21%\n","Epoch 126/200 - Train Loss: 1.7214, Train Acc: 79.22%, Test Loss: 2.2694, Test Acc: 58.12%\n","Epoch 127/200 - Train Loss: 1.5941, Train Acc: 82.57%, Test Loss: 2.3088, Test Acc: 57.54%\n","Epoch 128/200 - Train Loss: 1.6236, Train Acc: 81.77%, Test Loss: 2.2819, Test Acc: 57.90%\n","Epoch 129/200 - Train Loss: 1.5914, Train Acc: 82.85%, Test Loss: 2.2958, Test Acc: 57.58%\n","Epoch 130/200 - Train Loss: 1.5697, Train Acc: 83.31%, Test Loss: 2.2917, Test Acc: 57.92%\n","Epoch 131/200 - Train Loss: 1.5506, Train Acc: 83.47%, Test Loss: 2.2867, Test Acc: 57.98%\n","Epoch 132/200 - Train Loss: 1.6548, Train Acc: 80.81%, Test Loss: 2.2725, Test Acc: 58.37%\n","Epoch 133/200 - Train Loss: 1.5479, Train Acc: 83.39%, Test Loss: 2.2729, Test Acc: 58.31%\n","Epoch 134/200 - Train Loss: 1.5693, Train Acc: 83.11%, Test Loss: 2.2670, Test Acc: 58.44%\n","Epoch 135/200 - Train Loss: 1.6550, Train Acc: 80.60%, Test Loss: 2.2777, Test Acc: 58.15%\n","Epoch 136/200 - Train Loss: 1.5521, Train Acc: 83.55%, Test Loss: 2.2823, Test Acc: 58.32%\n","Epoch 137/200 - Train Loss: 1.4828, Train Acc: 85.53%, Test Loss: 2.2795, Test Acc: 57.88%\n","Epoch 138/200 - Train Loss: 1.5918, Train Acc: 82.41%, Test Loss: 2.2630, Test Acc: 58.33%\n","Epoch 139/200 - Train Loss: 1.5401, Train Acc: 83.55%, Test Loss: 2.2806, Test Acc: 57.67%\n","Epoch 140/200 - Train Loss: 1.5132, Train Acc: 84.34%, Test Loss: 2.2782, Test Acc: 58.42%\n","Epoch 141/200 - Train Loss: 1.5305, Train Acc: 84.24%, Test Loss: 2.2746, Test Acc: 57.98%\n","Epoch 142/200 - Train Loss: 1.5184, Train Acc: 84.05%, Test Loss: 2.2700, Test Acc: 58.42%\n","Epoch 143/200 - Train Loss: 1.5952, Train Acc: 82.03%, Test Loss: 2.2656, Test Acc: 58.28%\n","Epoch 144/200 - Train Loss: 1.5891, Train Acc: 82.62%, Test Loss: 2.2815, Test Acc: 58.34%\n","Epoch 145/200 - Train Loss: 1.6133, Train Acc: 81.79%, Test Loss: 2.2680, Test Acc: 58.20%\n","Epoch 146/200 - Train Loss: 1.5498, Train Acc: 83.56%, Test Loss: 2.2576, Test Acc: 58.74%\n","Epoch 147/200 - Train Loss: 1.5483, Train Acc: 83.43%, Test Loss: 2.2633, Test Acc: 58.34%\n","Epoch 148/200 - Train Loss: 1.5589, Train Acc: 83.05%, Test Loss: 2.2590, Test Acc: 58.94%\n","Epoch 149/200 - Train Loss: 1.5004, Train Acc: 84.69%, Test Loss: 2.2697, Test Acc: 58.76%\n","Epoch 150/200 - Train Loss: 1.5363, Train Acc: 83.97%, Test Loss: 2.2637, Test Acc: 59.03%\n","Epoch 151/200 - Train Loss: 1.5186, Train Acc: 84.27%, Test Loss: 2.2693, Test Acc: 58.89%\n","Epoch 152/200 - Train Loss: 1.6099, Train Acc: 81.82%, Test Loss: 2.2528, Test Acc: 58.90%\n","Epoch 153/200 - Train Loss: 1.5575, Train Acc: 83.56%, Test Loss: 2.2680, Test Acc: 58.58%\n","Epoch 154/200 - Train Loss: 1.5467, Train Acc: 83.67%, Test Loss: 2.2715, Test Acc: 58.33%\n","Epoch 155/200 - Train Loss: 1.4737, Train Acc: 85.27%, Test Loss: 2.2666, Test Acc: 58.66%\n","Epoch 156/200 - Train Loss: 1.5095, Train Acc: 84.44%, Test Loss: 2.2600, Test Acc: 58.93%\n","Epoch 157/200 - Train Loss: 1.5241, Train Acc: 84.13%, Test Loss: 2.2628, Test Acc: 58.95%\n","Epoch 158/200 - Train Loss: 1.5187, Train Acc: 84.05%, Test Loss: 2.2636, Test Acc: 58.80%\n","Epoch 159/200 - Train Loss: 1.5277, Train Acc: 84.03%, Test Loss: 2.2683, Test Acc: 58.95%\n","Epoch 160/200 - Train Loss: 1.5776, Train Acc: 82.95%, Test Loss: 2.2575, Test Acc: 58.88%\n","Epoch 161/200 - Train Loss: 1.5463, Train Acc: 83.39%, Test Loss: 2.2672, Test Acc: 58.87%\n","Epoch 162/200 - Train Loss: 1.5088, Train Acc: 84.52%, Test Loss: 2.2606, Test Acc: 58.76%\n","Epoch 163/200 - Train Loss: 1.5408, Train Acc: 83.49%, Test Loss: 2.2719, Test Acc: 58.87%\n","Epoch 164/200 - Train Loss: 1.5479, Train Acc: 83.50%, Test Loss: 2.2641, Test Acc: 58.87%\n","Epoch 165/200 - Train Loss: 1.5579, Train Acc: 83.20%, Test Loss: 2.2545, Test Acc: 58.83%\n","Epoch 166/200 - Train Loss: 1.5581, Train Acc: 83.48%, Test Loss: 2.2645, Test Acc: 59.03%\n","Epoch 167/200 - Train Loss: 1.5027, Train Acc: 84.62%, Test Loss: 2.2605, Test Acc: 58.91%\n","Epoch 168/200 - Train Loss: 1.5140, Train Acc: 84.37%, Test Loss: 2.2630, Test Acc: 58.88%\n","Epoch 169/200 - Train Loss: 1.5333, Train Acc: 83.60%, Test Loss: 2.2573, Test Acc: 59.16%\n","Epoch 170/200 - Train Loss: 1.5572, Train Acc: 82.97%, Test Loss: 2.2555, Test Acc: 59.11%\n","Epoch 171/200 - Train Loss: 1.5517, Train Acc: 83.35%, Test Loss: 2.2525, Test Acc: 58.83%\n","Epoch 172/200 - Train Loss: 1.5697, Train Acc: 82.85%, Test Loss: 2.2531, Test Acc: 58.94%\n","Epoch 173/200 - Train Loss: 1.5463, Train Acc: 83.64%, Test Loss: 2.2600, Test Acc: 58.94%\n","Epoch 174/200 - Train Loss: 1.5735, Train Acc: 82.94%, Test Loss: 2.2577, Test Acc: 58.85%\n","Epoch 175/200 - Train Loss: 1.4972, Train Acc: 84.75%, Test Loss: 2.2596, Test Acc: 59.05%\n","Epoch 176/200 - Train Loss: 1.5342, Train Acc: 83.57%, Test Loss: 2.2564, Test Acc: 59.08%\n","Epoch 177/200 - Train Loss: 1.5323, Train Acc: 83.93%, Test Loss: 2.2588, Test Acc: 59.02%\n","Epoch 178/200 - Train Loss: 1.5344, Train Acc: 83.88%, Test Loss: 2.2594, Test Acc: 58.96%\n","Epoch 179/200 - Train Loss: 1.5663, Train Acc: 82.83%, Test Loss: 2.2592, Test Acc: 58.86%\n","Epoch 180/200 - Train Loss: 1.4849, Train Acc: 85.23%, Test Loss: 2.2590, Test Acc: 59.04%\n","Epoch 181/200 - Train Loss: 1.5507, Train Acc: 83.22%, Test Loss: 2.2562, Test Acc: 59.05%\n","Epoch 182/200 - Train Loss: 1.6047, Train Acc: 82.11%, Test Loss: 2.2594, Test Acc: 58.90%\n","Epoch 183/200 - Train Loss: 1.5583, Train Acc: 82.91%, Test Loss: 2.2568, Test Acc: 59.11%\n","Epoch 184/200 - Train Loss: 1.5622, Train Acc: 82.99%, Test Loss: 2.2525, Test Acc: 59.05%\n","Epoch 185/200 - Train Loss: 1.5000, Train Acc: 84.46%, Test Loss: 2.2516, Test Acc: 59.22%\n","Epoch 186/200 - Train Loss: 1.5244, Train Acc: 84.07%, Test Loss: 2.2520, Test Acc: 59.11%\n","Epoch 187/200 - Train Loss: 1.5819, Train Acc: 82.29%, Test Loss: 2.2517, Test Acc: 59.12%\n","Epoch 188/200 - Train Loss: 1.4973, Train Acc: 84.65%, Test Loss: 2.2502, Test Acc: 59.25%\n","Epoch 189/200 - Train Loss: 1.4668, Train Acc: 85.37%, Test Loss: 2.2534, Test Acc: 59.13%\n","Epoch 190/200 - Train Loss: 1.6074, Train Acc: 81.99%, Test Loss: 2.2536, Test Acc: 59.04%\n","Epoch 191/200 - Train Loss: 1.4818, Train Acc: 85.08%, Test Loss: 2.2532, Test Acc: 59.10%\n","Epoch 192/200 - Train Loss: 1.4703, Train Acc: 85.52%, Test Loss: 2.2529, Test Acc: 59.16%\n","Epoch 193/200 - Train Loss: 1.4615, Train Acc: 85.61%, Test Loss: 2.2529, Test Acc: 59.09%\n","Epoch 194/200 - Train Loss: 1.5228, Train Acc: 83.93%, Test Loss: 2.2522, Test Acc: 59.14%\n","Epoch 195/200 - Train Loss: 1.4905, Train Acc: 85.01%, Test Loss: 2.2521, Test Acc: 59.15%\n","Epoch 196/200 - Train Loss: 1.4466, Train Acc: 86.12%, Test Loss: 2.2522, Test Acc: 59.17%\n","Epoch 197/200 - Train Loss: 1.4950, Train Acc: 84.89%, Test Loss: 2.2523, Test Acc: 59.15%\n","Epoch 198/200 - Train Loss: 1.5205, Train Acc: 83.84%, Test Loss: 2.2523, Test Acc: 59.14%\n","Epoch 199/200 - Train Loss: 1.4928, Train Acc: 85.08%, Test Loss: 2.2523, Test Acc: 59.16%\n","Epoch 200/200 - Train Loss: 1.4747, Train Acc: 85.23%, Test Loss: 2.2523, Test Acc: 59.16%\n","Training completed. Best Test Accuracy: 59.25%\n"]},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.030 MB of 0.030 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f1c2be67b2141f8a8ffd808e9030e24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <style>\n","        .wandb-row {\n","            display: flex;\n","            flex-direction: row;\n","            flex-wrap: wrap;\n","            justify-content: flex-start;\n","            width: 100%;\n","        }\n","        .wandb-col {\n","            display: flex;\n","            flex-direction: column;\n","            flex-basis: 100%;\n","            flex: 1;\n","            padding: 10px;\n","        }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_test_acc</td><td>▁▁▂▂▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇███████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>▂▂▆██████▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁▂▃▃▄▆▇▇▇▇▇███▇█████████████████████████</td></tr><tr><td>test_loss</td><td>█▇▆▄▂▂▂▂▂▂▂▁▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▂▂▃▃▃▅▄▅▆▅▇▅▆▆▆▆▆▇▇▇▇▇█▇█▂▇▅▇▇▅▇▇▇▇▇▇</td></tr><tr><td>train_loss</td><td>█▇▆▅▄▄▄▄▄▄▃▃▃▆▃▃▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_test_acc</td><td>59.25</td></tr><tr><td>epoch</td><td>199</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>test_acc</td><td>59.16</td></tr><tr><td>test_loss</td><td>2.25232</td></tr><tr><td>train_acc</td><td>86.10011</td></tr><tr><td>train_loss</td><td>1.44382</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">earthy-valley-9</strong> at: <a href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100/runs/x1rf9hz9' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100/runs/x1rf9hz9</a><br/> View project at: <a href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20241128_090750-x1rf9hz9/logs</code>"]},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"EZfDa_oQ8plO"},"execution_count":null,"outputs":[]}]}