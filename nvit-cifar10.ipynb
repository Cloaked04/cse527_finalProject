{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":10141,"status":"ok","timestamp":1732734551210,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"},"user_tz":300},"id":"nni-Mvy9zuuf"},"outputs":[],"source":["import torch\n","from torch import nn, optim\n","from torch.utils.data import DataLoader\n","import torchvision\n","from torchvision import datasets, transforms\n","import wandb\n","import os\n","import numpy as np\n","import torch.nn.functional as F\n","from einops import rearrange\n","from einops.layers.torch import Rearrange\n","import torch.nn.utils.parametrize as parametrize\n","import math"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":70,"status":"ok","timestamp":1732734551212,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"},"user_tz":300},"id":"fmAVqBm9z3af"},"outputs":[],"source":["# Set random seeds for reproducibility\n","torch.manual_seed(42)\n","torch.cuda.manual_seed_all(42)\n","np.random.seed(42)\n","\n","# Helper functions\n","def exists(v):\n","    return v is not None\n","\n","def default(v, d):\n","    return v if exists(v) else d\n","\n","def pair(t):\n","    return t if isinstance(t, tuple) else (t, t)\n","\n","def divisible_by(numer, denom):\n","    return (numer % denom) == 0\n","\n","def l2norm(t, dim=-1):\n","    return F.normalize(t, dim=dim, p=2)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":69,"status":"ok","timestamp":1732734551213,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"},"user_tz":300},"id":"3TJJRcSN09NB"},"outputs":[],"source":["# For use with parametrize\n","class L2Norm(nn.Module):\n","    def __init__(self, dim=-1):\n","        super().__init__()\n","        self.dim = dim\n","\n","    def forward(self, t):\n","        return l2norm(t, dim=self.dim)\n","\n","class NormLinear(nn.Module):\n","    def __init__(self, dim, dim_out, norm_dim_in=True):\n","        super().__init__()\n","        self.linear = nn.Linear(dim, dim_out, bias=False)\n","\n","        parametrize.register_parametrization(\n","            self.linear,\n","            'weight',\n","            L2Norm(dim=-1 if norm_dim_in else 0)\n","        )\n","\n","    @property\n","    def weight(self):\n","        return self.linear.weight\n","\n","    def forward(self, x):\n","        return self.linear(x)\n","\n","# Scaled dot product attention function\n","def scaled_dot_product_attention(q, k, v, dropout_p=0., training=True):\n","    d_k = q.size(-1)\n","    attn_weights = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n","    attn_weights = F.softmax(attn_weights, dim=-1)\n","    if training and dropout_p > 0.0:\n","        attn_weights = F.dropout(attn_weights, p=dropout_p)\n","    output = torch.matmul(attn_weights, v)\n","    return output"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":67,"status":"ok","timestamp":1732734551213,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"},"user_tz":300},"id":"GYNWADjc1AVY"},"outputs":[],"source":["# Attention and FeedForward classes\n","class Attention(nn.Module):\n","    def __init__(self, dim, *, dim_head=64, heads=8, dropout=0.):\n","        super().__init__()\n","        dim_inner = dim_head * heads\n","        self.to_q = NormLinear(dim, dim_inner)\n","        self.to_k = NormLinear(dim, dim_inner)\n","        self.to_v = NormLinear(dim, dim_inner)\n","\n","        self.dropout = dropout\n","\n","        self.q_scale = nn.Parameter(torch.ones(heads, 1, dim_head) * (dim_head ** 0.25))\n","        self.k_scale = nn.Parameter(torch.ones(heads, 1, dim_head) * (dim_head ** 0.25))\n","\n","        self.split_heads = Rearrange('b n (h d) -> b h n d', h=heads)\n","        self.merge_heads = Rearrange('b h n d -> b n (h d)')\n","\n","        self.to_out = NormLinear(dim_inner, dim, norm_dim_in=False)\n","\n","    def forward(self, x):\n","        q, k, v = self.to_q(x), self.to_k(x), self.to_v(x)\n","\n","        q, k, v = map(self.split_heads, (q, k, v))\n","\n","        # Query key rmsnorm\n","        q, k = map(l2norm, (q, k))\n","\n","        q = q * self.q_scale\n","        k = k * self.k_scale\n","\n","        out = scaled_dot_product_attention(\n","            q, k, v,\n","            dropout_p=self.dropout,\n","            training=self.training\n","        )\n","\n","        out = self.merge_heads(out)\n","        return self.to_out(out)\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, dim, *, dim_inner, dropout=0.):\n","        super().__init__()\n","        dim_inner = int(dim_inner * 2 / 3)\n","\n","        self.dim = dim\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.to_hidden = NormLinear(dim, dim_inner)\n","        self.to_gate = NormLinear(dim, dim_inner)\n","\n","        self.hidden_scale = nn.Parameter(torch.ones(dim_inner))\n","        self.gate_scale = nn.Parameter(torch.ones(dim_inner))\n","\n","        self.to_out = NormLinear(dim_inner, dim, norm_dim_in=False)\n","\n","    def forward(self, x):\n","        hidden, gate = self.to_hidden(x), self.to_gate(x)\n","\n","        hidden = hidden * self.hidden_scale\n","        gate = gate * self.gate_scale * (self.dim ** 0.5)\n","\n","        hidden = F.silu(gate) * hidden\n","\n","        hidden = self.dropout(hidden)\n","        return self.to_out(hidden)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":66,"status":"ok","timestamp":1732734551214,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"},"user_tz":300},"id":"i2PkGd_K1KiY"},"outputs":[],"source":["# nViT base class\n","class nViT(nn.Module):\n","    def __init__(\n","        self,\n","        *,\n","        image_size,\n","        patch_size,\n","        num_classes,\n","        dim,\n","        depth,\n","        heads,\n","        mlp_dim,\n","        dropout=0.,\n","        channels=3,\n","        dim_head=64,\n","        residual_lerp_scale_init=None\n","    ):\n","        super().__init__()\n","        image_height, image_width = pair(image_size)\n","\n","        assert divisible_by(image_height, patch_size) and divisible_by(image_width, patch_size), 'Image dimensions must be divisible by the patch size.'\n","\n","        patch_height_dim, patch_width_dim = (image_height // patch_size), (image_width // patch_size)\n","        patch_dim = channels * (patch_size ** 2)\n","        num_patches = patch_height_dim * patch_width_dim\n","\n","        self.channels = channels\n","        self.patch_size = patch_size\n","        self.num_patches = num_patches\n","        self.image_size = image_size\n","\n","        self.to_patch_embedding = nn.Sequential(\n","            Rearrange('b c (h p1) (w p2) -> b (h w) (c p1 p2)', p1=patch_size, p2=patch_size),\n","            NormLinear(patch_dim, dim, norm_dim_in=False),\n","        )\n","\n","        self.abs_pos_emb = NormLinear(dim, num_patches)\n","\n","        residual_lerp_scale_init = default(residual_lerp_scale_init, 1. / depth)\n","\n","        self.dim = dim\n","        self.scale = dim ** 0.5\n","\n","        self.layers = nn.ModuleList([])\n","        self.residual_lerp_scales = nn.ModuleList([])\n","\n","        for _ in range(depth):\n","            self.layers.append(nn.ModuleList([\n","                Attention(dim, dim_head=dim_head, heads=heads, dropout=dropout),\n","                FeedForward(dim, dim_inner=mlp_dim, dropout=dropout),\n","            ]))\n","\n","            self.residual_lerp_scales.append(nn.ParameterList([\n","                nn.Parameter(torch.ones(dim) * residual_lerp_scale_init / self.scale),\n","                nn.Parameter(torch.ones(dim) * residual_lerp_scale_init / self.scale),\n","            ]))\n","\n","        # Classification head\n","        self.mlp_head = nn.Sequential(\n","            nn.LayerNorm(dim),\n","            nn.Linear(dim, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        device = x.device\n","\n","        tokens = self.to_patch_embedding(x)\n","\n","        seq_len = tokens.shape[-2]\n","        pos_emb = self.abs_pos_emb.weight[torch.arange(seq_len, device=device)]\n","\n","        tokens = l2norm(tokens + pos_emb)\n","\n","        for (attn, ff), residual_scales in zip(self.layers, self.residual_lerp_scales):\n","            attn_alpha, ff_alpha = residual_scales\n","\n","            attn_out = l2norm(attn(tokens))\n","            tokens = l2norm(tokens.lerp(attn_out, attn_alpha * self.scale))\n","\n","            ff_out = l2norm(ff(tokens))\n","            tokens = l2norm(tokens.lerp(ff_out, ff_alpha * self.scale))\n","\n","        # Classification token (mean pooling)\n","        tokens = tokens.mean(dim=1)\n","        logits = self.mlp_head(tokens)\n","        return logits"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"zPmNW8Fm1NDO","executionInfo":{"status":"ok","timestamp":1732740588668,"user_tz":300,"elapsed":6037518,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"}},"outputId":"dfe7a39b-03b1-40bb-c3e9-8fb430bc53c5"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":[" ··········\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.18.7"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20241127_190920-i300qqlx</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar10/runs/i300qqlx' target=\"_blank\">snowy-water-5</a></strong> to <a href='https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar10' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar10</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar10/runs/i300qqlx' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar10/runs/i300qqlx</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170M/170M [00:03<00:00, 43.5MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","Epoch 1/100 - Train Loss: 1.6681, Train Acc: 38.45%, Test Loss: 1.3639, Test Acc: 50.21%\n","Epoch 2/100 - Train Loss: 1.1663, Train Acc: 58.07%, Test Loss: 1.1066, Test Acc: 60.24%\n","Epoch 3/100 - Train Loss: 0.9902, Train Acc: 64.96%, Test Loss: 0.9229, Test Acc: 67.34%\n","Epoch 4/100 - Train Loss: 0.8831, Train Acc: 68.66%, Test Loss: 0.8530, Test Acc: 70.06%\n","Epoch 5/100 - Train Loss: 0.8035, Train Acc: 71.73%, Test Loss: 0.7801, Test Acc: 73.06%\n","Epoch 6/100 - Train Loss: 0.7408, Train Acc: 73.89%, Test Loss: 0.7289, Test Acc: 74.94%\n","Epoch 7/100 - Train Loss: 0.6865, Train Acc: 75.84%, Test Loss: 0.7228, Test Acc: 74.84%\n","Epoch 8/100 - Train Loss: 0.6447, Train Acc: 77.35%, Test Loss: 0.6851, Test Acc: 76.58%\n","Epoch 9/100 - Train Loss: 0.6055, Train Acc: 78.54%, Test Loss: 0.6569, Test Acc: 76.97%\n","Epoch 10/100 - Train Loss: 0.5677, Train Acc: 79.93%, Test Loss: 0.6203, Test Acc: 78.93%\n","Epoch 11/100 - Train Loss: 0.5406, Train Acc: 81.03%, Test Loss: 0.5946, Test Acc: 79.50%\n","Epoch 12/100 - Train Loss: 0.5070, Train Acc: 82.01%, Test Loss: 0.5677, Test Acc: 80.16%\n","Epoch 13/100 - Train Loss: 0.4827, Train Acc: 82.82%, Test Loss: 0.5693, Test Acc: 80.35%\n","Epoch 14/100 - Train Loss: 0.4582, Train Acc: 83.82%, Test Loss: 0.5337, Test Acc: 81.62%\n","Epoch 15/100 - Train Loss: 0.4316, Train Acc: 84.83%, Test Loss: 0.5324, Test Acc: 81.71%\n","Epoch 16/100 - Train Loss: 0.4081, Train Acc: 85.51%, Test Loss: 0.5491, Test Acc: 81.62%\n","Epoch 17/100 - Train Loss: 0.3911, Train Acc: 86.06%, Test Loss: 0.5013, Test Acc: 82.85%\n","Epoch 18/100 - Train Loss: 0.3680, Train Acc: 86.93%, Test Loss: 0.4996, Test Acc: 83.21%\n","Epoch 19/100 - Train Loss: 0.3486, Train Acc: 87.77%, Test Loss: 0.5422, Test Acc: 82.00%\n","Epoch 20/100 - Train Loss: 0.3293, Train Acc: 88.18%, Test Loss: 0.5283, Test Acc: 82.44%\n","Epoch 21/100 - Train Loss: 0.3122, Train Acc: 88.86%, Test Loss: 0.5055, Test Acc: 83.08%\n","Epoch 22/100 - Train Loss: 0.2919, Train Acc: 89.50%, Test Loss: 0.5396, Test Acc: 82.63%\n","Epoch 23/100 - Train Loss: 0.2780, Train Acc: 90.13%, Test Loss: 0.4953, Test Acc: 83.56%\n","Epoch 24/100 - Train Loss: 0.2632, Train Acc: 90.57%, Test Loss: 0.4998, Test Acc: 83.83%\n","Epoch 25/100 - Train Loss: 0.2469, Train Acc: 91.13%, Test Loss: 0.5134, Test Acc: 83.82%\n","Epoch 26/100 - Train Loss: 0.2322, Train Acc: 91.66%, Test Loss: 0.5207, Test Acc: 83.37%\n","Epoch 27/100 - Train Loss: 0.2194, Train Acc: 91.98%, Test Loss: 0.5209, Test Acc: 84.10%\n","Epoch 28/100 - Train Loss: 0.2088, Train Acc: 92.60%, Test Loss: 0.5286, Test Acc: 83.45%\n","Epoch 29/100 - Train Loss: 0.1922, Train Acc: 93.09%, Test Loss: 0.5374, Test Acc: 83.56%\n","Epoch 30/100 - Train Loss: 0.1806, Train Acc: 93.49%, Test Loss: 0.5467, Test Acc: 83.65%\n","Epoch 31/100 - Train Loss: 0.1694, Train Acc: 93.92%, Test Loss: 0.5571, Test Acc: 83.83%\n","Epoch 32/100 - Train Loss: 0.1599, Train Acc: 94.38%, Test Loss: 0.5577, Test Acc: 83.48%\n","Epoch 33/100 - Train Loss: 0.1510, Train Acc: 94.62%, Test Loss: 0.5751, Test Acc: 83.82%\n","Epoch 34/100 - Train Loss: 0.1408, Train Acc: 94.99%, Test Loss: 0.5764, Test Acc: 83.88%\n","Epoch 35/100 - Train Loss: 0.1311, Train Acc: 95.28%, Test Loss: 0.5833, Test Acc: 83.70%\n","Epoch 36/100 - Train Loss: 0.1226, Train Acc: 95.50%, Test Loss: 0.5767, Test Acc: 84.48%\n","Epoch 37/100 - Train Loss: 0.1201, Train Acc: 95.69%, Test Loss: 0.5934, Test Acc: 84.34%\n","Epoch 38/100 - Train Loss: 0.1111, Train Acc: 96.00%, Test Loss: 0.5798, Test Acc: 84.75%\n","Epoch 39/100 - Train Loss: 0.1011, Train Acc: 96.38%, Test Loss: 0.5861, Test Acc: 84.32%\n","Epoch 40/100 - Train Loss: 0.0982, Train Acc: 96.46%, Test Loss: 0.6325, Test Acc: 83.89%\n","Epoch 41/100 - Train Loss: 0.0939, Train Acc: 96.70%, Test Loss: 0.6028, Test Acc: 84.92%\n","Epoch 42/100 - Train Loss: 0.0854, Train Acc: 96.97%, Test Loss: 0.6206, Test Acc: 85.02%\n","Epoch 43/100 - Train Loss: 0.0856, Train Acc: 96.94%, Test Loss: 0.6038, Test Acc: 85.03%\n","Epoch 44/100 - Train Loss: 0.0770, Train Acc: 97.30%, Test Loss: 0.6218, Test Acc: 84.67%\n","Epoch 45/100 - Train Loss: 0.0719, Train Acc: 97.42%, Test Loss: 0.6376, Test Acc: 84.84%\n","Epoch 46/100 - Train Loss: 0.0664, Train Acc: 97.61%, Test Loss: 0.5954, Test Acc: 85.34%\n","Epoch 47/100 - Train Loss: 0.0620, Train Acc: 97.78%, Test Loss: 0.6460, Test Acc: 84.80%\n","Epoch 48/100 - Train Loss: 0.0620, Train Acc: 97.79%, Test Loss: 0.6278, Test Acc: 84.69%\n","Epoch 49/100 - Train Loss: 0.0556, Train Acc: 98.07%, Test Loss: 0.6623, Test Acc: 84.97%\n","Epoch 50/100 - Train Loss: 0.0545, Train Acc: 98.11%, Test Loss: 0.6674, Test Acc: 84.72%\n","Epoch 51/100 - Train Loss: 0.0525, Train Acc: 98.11%, Test Loss: 0.6490, Test Acc: 85.30%\n","Epoch 52/100 - Train Loss: 0.0456, Train Acc: 98.45%, Test Loss: 0.6683, Test Acc: 84.91%\n","Epoch 53/100 - Train Loss: 0.0443, Train Acc: 98.45%, Test Loss: 0.6724, Test Acc: 85.31%\n","Epoch 54/100 - Train Loss: 0.0400, Train Acc: 98.63%, Test Loss: 0.6971, Test Acc: 85.16%\n","Epoch 55/100 - Train Loss: 0.0383, Train Acc: 98.66%, Test Loss: 0.7058, Test Acc: 85.07%\n","Epoch 56/100 - Train Loss: 0.0382, Train Acc: 98.62%, Test Loss: 0.6900, Test Acc: 85.28%\n","Epoch 57/100 - Train Loss: 0.0340, Train Acc: 98.80%, Test Loss: 0.7022, Test Acc: 85.27%\n","Epoch 58/100 - Train Loss: 0.0330, Train Acc: 98.87%, Test Loss: 0.6632, Test Acc: 85.67%\n","Epoch 59/100 - Train Loss: 0.0314, Train Acc: 98.89%, Test Loss: 0.7005, Test Acc: 85.42%\n","Epoch 60/100 - Train Loss: 0.0257, Train Acc: 99.13%, Test Loss: 0.7175, Test Acc: 85.27%\n","Epoch 61/100 - Train Loss: 0.0247, Train Acc: 99.17%, Test Loss: 0.7053, Test Acc: 85.31%\n","Epoch 62/100 - Train Loss: 0.0222, Train Acc: 99.27%, Test Loss: 0.7512, Test Acc: 84.88%\n","Epoch 63/100 - Train Loss: 0.0241, Train Acc: 99.18%, Test Loss: 0.7249, Test Acc: 85.61%\n","Epoch 64/100 - Train Loss: 0.0214, Train Acc: 99.26%, Test Loss: 0.7357, Test Acc: 85.10%\n","Epoch 65/100 - Train Loss: 0.0223, Train Acc: 99.21%, Test Loss: 0.7353, Test Acc: 85.40%\n","Epoch 66/100 - Train Loss: 0.0181, Train Acc: 99.38%, Test Loss: 0.7368, Test Acc: 85.35%\n","Epoch 67/100 - Train Loss: 0.0171, Train Acc: 99.43%, Test Loss: 0.7527, Test Acc: 85.76%\n","Epoch 68/100 - Train Loss: 0.0148, Train Acc: 99.48%, Test Loss: 0.7465, Test Acc: 85.92%\n","Epoch 69/100 - Train Loss: 0.0142, Train Acc: 99.52%, Test Loss: 0.7640, Test Acc: 85.79%\n","Epoch 70/100 - Train Loss: 0.0126, Train Acc: 99.56%, Test Loss: 0.7711, Test Acc: 85.47%\n","Epoch 71/100 - Train Loss: 0.0130, Train Acc: 99.55%, Test Loss: 0.7835, Test Acc: 85.87%\n","Epoch 72/100 - Train Loss: 0.0119, Train Acc: 99.59%, Test Loss: 0.7829, Test Acc: 86.21%\n","Epoch 73/100 - Train Loss: 0.0116, Train Acc: 99.61%, Test Loss: 0.7736, Test Acc: 85.97%\n","Epoch 74/100 - Train Loss: 0.0097, Train Acc: 99.66%, Test Loss: 0.7699, Test Acc: 86.25%\n","Epoch 75/100 - Train Loss: 0.0076, Train Acc: 99.76%, Test Loss: 0.7839, Test Acc: 86.10%\n","Epoch 76/100 - Train Loss: 0.0087, Train Acc: 99.72%, Test Loss: 0.7949, Test Acc: 85.90%\n","Epoch 77/100 - Train Loss: 0.0066, Train Acc: 99.78%, Test Loss: 0.8025, Test Acc: 86.34%\n","Epoch 78/100 - Train Loss: 0.0061, Train Acc: 99.81%, Test Loss: 0.7996, Test Acc: 86.42%\n","Epoch 79/100 - Train Loss: 0.0042, Train Acc: 99.87%, Test Loss: 0.8003, Test Acc: 86.38%\n","Epoch 80/100 - Train Loss: 0.0052, Train Acc: 99.84%, Test Loss: 0.7978, Test Acc: 86.41%\n","Epoch 81/100 - Train Loss: 0.0046, Train Acc: 99.86%, Test Loss: 0.8155, Test Acc: 86.30%\n","Epoch 82/100 - Train Loss: 0.0047, Train Acc: 99.85%, Test Loss: 0.8297, Test Acc: 86.49%\n","Epoch 83/100 - Train Loss: 0.0041, Train Acc: 99.87%, Test Loss: 0.8325, Test Acc: 86.39%\n","Epoch 84/100 - Train Loss: 0.0036, Train Acc: 99.90%, Test Loss: 0.8194, Test Acc: 86.61%\n","Epoch 85/100 - Train Loss: 0.0035, Train Acc: 99.91%, Test Loss: 0.8257, Test Acc: 86.75%\n","Epoch 86/100 - Train Loss: 0.0032, Train Acc: 99.88%, Test Loss: 0.8272, Test Acc: 86.70%\n","Epoch 87/100 - Train Loss: 0.0031, Train Acc: 99.90%, Test Loss: 0.8272, Test Acc: 86.89%\n","Epoch 88/100 - Train Loss: 0.0025, Train Acc: 99.92%, Test Loss: 0.8378, Test Acc: 86.73%\n","Epoch 89/100 - Train Loss: 0.0020, Train Acc: 99.95%, Test Loss: 0.8368, Test Acc: 86.99%\n","Epoch 90/100 - Train Loss: 0.0020, Train Acc: 99.95%, Test Loss: 0.8421, Test Acc: 86.78%\n","Epoch 91/100 - Train Loss: 0.0024, Train Acc: 99.94%, Test Loss: 0.8364, Test Acc: 86.98%\n","Epoch 92/100 - Train Loss: 0.0019, Train Acc: 99.96%, Test Loss: 0.8411, Test Acc: 86.95%\n","Epoch 93/100 - Train Loss: 0.0017, Train Acc: 99.97%, Test Loss: 0.8440, Test Acc: 87.01%\n","Epoch 94/100 - Train Loss: 0.0015, Train Acc: 99.97%, Test Loss: 0.8430, Test Acc: 86.95%\n","Epoch 95/100 - Train Loss: 0.0015, Train Acc: 99.98%, Test Loss: 0.8428, Test Acc: 86.96%\n","Epoch 96/100 - Train Loss: 0.0016, Train Acc: 99.95%, Test Loss: 0.8442, Test Acc: 86.98%\n","Epoch 97/100 - Train Loss: 0.0016, Train Acc: 99.95%, Test Loss: 0.8429, Test Acc: 86.99%\n","Epoch 98/100 - Train Loss: 0.0016, Train Acc: 99.95%, Test Loss: 0.8438, Test Acc: 86.99%\n","Epoch 99/100 - Train Loss: 0.0016, Train Acc: 99.97%, Test Loss: 0.8434, Test Acc: 87.02%\n","Epoch 100/100 - Train Loss: 0.0015, Train Acc: 99.97%, Test Loss: 0.8434, Test Acc: 87.01%\n","Training completed. Best Test Accuracy: 87.02%\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <style>\n","        .wandb-row {\n","            display: flex;\n","            flex-direction: row;\n","            flex-wrap: wrap;\n","            justify-content: flex-start;\n","            width: 100%;\n","        }\n","        .wandb-col {\n","            display: flex;\n","            flex-direction: column;\n","            flex-basis: 100%;\n","            flex: 1;\n","            padding: 10px;\n","        }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>████████████▇▇▇▇▆▆▆▆▆▅▄▄▄▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁▄▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇█████████████████████</td></tr><tr><td>test_loss</td><td>█▆▃▂▁▁▁▁▁▁▂▂▃▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅</td></tr><tr><td>train_acc</td><td>▁▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇████████████████████████</td></tr><tr><td>train_loss</td><td>█▇▆▅▄▃▂▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>test_acc</td><td>87.01</td></tr><tr><td>test_loss</td><td>0.84339</td></tr><tr><td>train_acc</td><td>99.96885</td></tr><tr><td>train_loss</td><td>0.00151</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">snowy-water-5</strong> at: <a href='https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar10/runs/i300qqlx' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar10/runs/i300qqlx</a><br/> View project at: <a href='https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar10' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar10</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20241127_190920-i300qqlx/logs</code>"]},"metadata":{}}],"source":["def main():\n","    # Initialize wandb\n","    wandb.init(project='nvit-cifar10', config={\n","        'model': 'nViT',\n","        'dataset': 'CIFAR-10',\n","        'epochs': 100,\n","        'batch_size': 128,\n","        'learning_rate': 3e-4,\n","        'weight_decay': 1e-4,\n","        'image_size': 32,\n","        'patch_size': 4,\n","        'dim': 384,\n","        'depth': 6,\n","        'heads': 6,\n","        'mlp_dim': 384 * 4,\n","        'dropout': 0.1,\n","        'num_classes': 10,\n","        'dim_head': 64\n","    })\n","    config = wandb.config\n","\n","    # Set device\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    # Data transforms\n","    transform_train = transforms.Compose([\n","        transforms.RandomCrop(config.image_size, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465),\n","                             (0.2470, 0.2435, 0.2616)),\n","    ])\n","\n","    transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465),\n","                             (0.2470, 0.2435, 0.2616)),\n","    ])\n","\n","    # Load CIFAR-10 dataset\n","    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n","    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n","    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n","\n","    # Initialize model\n","    model = nViT(\n","        image_size=config.image_size,\n","        patch_size=config.patch_size,\n","        num_classes=config.num_classes,\n","        dim=config.dim,\n","        depth=config.depth,\n","        heads=config.heads,\n","        mlp_dim=config.mlp_dim,\n","        dropout=config.dropout,\n","        dim_head=config.dim_head\n","    ).to(device)\n","\n","    # Loss function and optimizer\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n","    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs)\n","\n","    # Training loop\n","    best_acc = 0.0\n","    for epoch in range(config.epochs):\n","        model.train()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        for batch_idx, (inputs, targets) in enumerate(train_loader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","            if batch_idx % 100 == 0:\n","                wandb.log({\n","                    'train_loss': running_loss / (batch_idx + 1),\n","                    'train_acc': 100. * correct / total,\n","                    'learning_rate': optimizer.param_groups[0]['lr']\n","                })\n","\n","\n","        train_loss = running_loss / len(train_loader)\n","        train_acc = 100. * correct / total\n","\n","        # Validation\n","        model.eval()\n","        test_loss = 0.0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, targets in test_loader:\n","                inputs, targets = inputs.to(device), targets.to(device)\n","                outputs = model(inputs)\n","                loss = criterion(outputs, targets)\n","\n","                test_loss += loss.item()\n","                _, predicted = outputs.max(1)\n","                total += targets.size(0)\n","                correct += predicted.eq(targets).sum().item()\n","\n","        acc = 100. * correct / total\n","        avg_test_loss = test_loss / len(test_loader)\n","        wandb.log({\n","            'test_loss': avg_test_loss,\n","            'test_acc': acc,\n","            'epoch': epoch\n","        })\n","\n","        print(f\"Epoch {epoch + 1}/{config.epochs} - \"\n","              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n","              f\"Test Loss: {avg_test_loss:.4f}, Test Acc: {acc:.2f}%\")\n","\n","        # Save best model\n","        if acc > best_acc:\n","            best_acc = acc\n","            torch.save(model.state_dict(), os.path.join(wandb.run.dir, 'best_model.pth'))\n","\n","        scheduler.step()\n","\n","    print(f\"Training completed. Best Test Accuracy: {best_acc:.2f}%\")\n","    wandb.finish()\n","\n","if __name__ == '__main__':\n","    main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"86omwO281d7p"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyO8zVs96eMMxIB+lEgwL6oq"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}