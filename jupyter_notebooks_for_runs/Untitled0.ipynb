{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":649},"id":"ASEdaW1w1cqL","executionInfo":{"status":"error","timestamp":1733638096036,"user_tz":300,"elapsed":98462,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"}},"outputId":"a119f7b8-f972-4bbd-c36d-6fa7d20e27a7"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":[" ··········\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.18.7"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20241208_060700-ne65iyka</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100-ablation/runs/ne65iyka' target=\"_blank\">nViT_CIFAR100_convstem=False_mixup=False_aug=advanced</a></strong> to <a href='https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100-ablation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100-ablation' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100-ablation</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100-ablation/runs/ne65iyka' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100-ablation/runs/ne65iyka</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Running Experiment: nViT_CIFAR100_convstem=False_mixup=False_aug=advanced on device: cuda\n","\n","Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 169M/169M [00:13<00:00, 12.2MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-100-python.tar.gz to ./data\n","Files already downloaded and verified\n","Epoch 1/100 - Train Loss: 4.1983, Train Acc: 5.37%, Test Loss: 4.1039, Test Acc: 7.06%\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-69faecba58ff>\u001b[0m in \u001b[0;36m<cell line: 310>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m     \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-1-69faecba58ff>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","from torch import nn, optim\n","from torch.utils.data import DataLoader\n","import torchvision\n","from torchvision import datasets, transforms\n","import wandb\n","import numpy as np\n","import torch.nn.functional as F\n","from einops.layers.torch import Rearrange\n","import math\n","import random\n","import copy\n","\n","####################\n","# Helper Functions #\n","####################\n","\n","def set_seed(seed=42):\n","    \"\"\"\n","    Set random seeds for reproducibility.\n","    \"\"\"\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","def l2norm(t, dim=-1):\n","    return F.normalize(t, dim=dim, p=2)\n","\n","####################\n","# Model Components #\n","####################\n","\n","class NormLinear(nn.Module):\n","    def __init__(self, dim, dim_out):\n","        super().__init__()\n","        self.linear = nn.Linear(dim, dim_out, bias=False)\n","        self.norm = nn.LayerNorm(dim_out)\n","\n","    def forward(self, x):\n","        return self.norm(self.linear(x))\n","\n","class Attention(nn.Module):\n","    def __init__(self, dim, dim_head=64, heads=8, dropout=0.):\n","        super().__init__()\n","        dim_inner = dim_head * heads\n","        self.to_q = NormLinear(dim, dim_inner)\n","        self.to_k = NormLinear(dim, dim_inner)\n","        self.to_v = NormLinear(dim, dim_inner)\n","        self.dropout = nn.Dropout(dropout)\n","        self.scale = dim_head ** -0.5\n","        self.heads = heads\n","\n","        self.to_out = nn.Sequential(\n","            nn.Linear(dim_inner, dim),\n","            nn.LayerNorm(dim)\n","        )\n","\n","    def forward(self, x):\n","        b, n, _, h = *x.shape, self.heads\n","        q = self.to_q(x).reshape(b, n, h, -1).permute(0, 2, 1, 3) * self.scale\n","        k = self.to_k(x).reshape(b, n, h, -1).permute(0, 2, 1, 3)\n","        v = self.to_v(x).reshape(b, n, h, -1).permute(0, 2, 1, 3)\n","\n","        attn = (q @ k.transpose(-2, -1)).softmax(dim=-1)\n","        attn = self.dropout(attn)\n","\n","        out = (attn @ v).transpose(1, 2).reshape(b, n, -1)\n","        return self.to_out(out)\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, dim, mlp_dim, dropout=0.):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(dim, mlp_dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(mlp_dim, dim),\n","            nn.Dropout(dropout)\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class nViT(nn.Module):\n","    def __init__(self, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, dropout=0.):\n","        super().__init__()\n","        assert image_size % patch_size == 0, \"Image size must be divisible by patch size.\"\n","        num_patches = (image_size // patch_size) ** 2\n","        self.to_patch_embedding = nn.Sequential(\n","            Rearrange('b c (h p1) (w p2) -> b (h w) (c p1 p2)', p1=patch_size, p2=patch_size),\n","            NormLinear(patch_size * patch_size * 3, dim)\n","        )\n","        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, dim))\n","        self.layers = nn.ModuleList([])\n","        for _ in range(depth):\n","            self.layers.append(nn.ModuleList([\n","                Attention(dim, dim_head=64, heads=heads, dropout=dropout),\n","                FeedForward(dim, mlp_dim, dropout=dropout)\n","            ]))\n","        self.mlp_head = nn.Sequential(\n","            nn.LayerNorm(dim),\n","            nn.Linear(dim, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        x = self.to_patch_embedding(x) + self.pos_embedding\n","        for attn, ff in self.layers:\n","            x = attn(x) + ff(x)\n","        x = x.mean(dim=1)\n","        return self.mlp_head(x)\n","\n","#####################\n","# Data Augmentations#\n","#####################\n","\n","def get_transforms(image_size, augment_level):\n","    if augment_level == 'advanced':\n","        train_transform = transforms.Compose([\n","            transforms.RandomCrop(image_size, padding=4),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","            transforms.RandomRotation(15),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5071, 0.4867, 0.4408),\n","                                 (0.2675, 0.2565, 0.2761)),\n","        ])\n","    else:  # baseline\n","        train_transform = transforms.Compose([\n","            transforms.RandomCrop(image_size, padding=4),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5071, 0.4867, 0.4408),\n","                                 (0.2675, 0.2565, 0.2761)),\n","        ])\n","\n","    test_transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5071, 0.4867, 0.4408),\n","                             (0.2675, 0.2565, 0.2761)),\n","    ])\n","    return train_transform, test_transform\n","\n","###############################\n","# Experiment Runner Function  #\n","###############################\n","\n","def run_experiment():\n","    \"\"\"\n","    Runs the experiment with Advanced Augmentations without ConvStem and no MixUp.\n","    \"\"\"\n","    # Initialize wandb\n","    config = {\n","        'epochs': 100,\n","        'batch_size': 128,\n","        'learning_rate': 3e-4,\n","        'weight_decay': 1e-4,\n","        'image_size': 32,\n","        'patch_size': 4,\n","        'dim': 384,\n","        'depth': 6,\n","        'heads': 6,\n","        'mlp_dim': 384*4,\n","        'dropout': 0.1,\n","        'num_classes': 100,\n","        'patience': 20,\n","        'augment_level': 'advanced',\n","        'use_convstem': False,\n","        'use_mixup': False\n","    }\n","\n","    run_name = f\"nViT_CIFAR100_convstem={config['use_convstem']}_mixup={config['use_mixup']}_aug={config['augment_level']}\"\n","    wandb.init(project='nvit-cifar100-ablation', config=config, name=run_name)\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"\\nRunning Experiment: {run_name} on device: {device}\\n\")\n","\n","    # Set seeds for reproducibility\n","    set_seed(42)\n","\n","    # Data augmentation\n","    train_transform, test_transform = get_transforms(config['image_size'], config['augment_level'])\n","\n","    # Load CIFAR-100 dataset\n","    train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n","    test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True,\n","                              num_workers=4, pin_memory=True)\n","    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False,\n","                             num_workers=4, pin_memory=True)\n","\n","    # Initialize model\n","    model = nViT(\n","        image_size=config['image_size'],\n","        patch_size=config['patch_size'],\n","        num_classes=config['num_classes'],\n","        dim=config['dim'],\n","        depth=config['depth'],\n","        heads=config['heads'],\n","        mlp_dim=config['mlp_dim'],\n","        dropout=config['dropout']\n","    ).to(device)\n","\n","    # Define Loss Function and Optimizer\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'],\n","                            weight_decay=config['weight_decay'])\n","    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['epochs'])\n","\n","    # Early Stopping Parameters\n","    best_acc = 0.0\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    patience = config['patience']\n","    trigger_times = 0\n","\n","    # Training Loop\n","    for epoch in range(config['epochs']):\n","        model.train()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        for batch_idx, (inputs, targets) in enumerate(train_loader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient Clipping\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","            # Logging intermediate batch metrics\n","            if batch_idx % 100 == 0:\n","                wandb.log({\n","                    'train_loss': running_loss / (batch_idx + 1),\n","                    'train_acc': 100. * correct / total,\n","                    'learning_rate': optimizer.param_groups[0]['lr']\n","                })\n","\n","        # Epoch-wise Training Metrics\n","        train_loss = running_loss / len(train_loader)\n","        train_acc = 100. * correct / total\n","\n","        # Validation Phase\n","        model.eval()\n","        test_loss = 0.0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, targets in test_loader:\n","                inputs, targets = inputs.to(device), targets.to(device)\n","                outputs = model(inputs)\n","                loss = criterion(outputs, targets)\n","\n","                test_loss += loss.item()\n","                _, predicted = outputs.max(1)\n","                total += targets.size(0)\n","                correct += predicted.eq(targets).sum().item()\n","\n","        # Epoch-wise Validation Metrics\n","        acc = 100. * correct / total\n","        avg_test_loss = test_loss / len(test_loader)\n","        wandb.log({\n","            'epoch': epoch,\n","            'test_loss': avg_test_loss,\n","            'test_acc': acc\n","        })\n","\n","        print(f\"Epoch {epoch + 1}/{config['epochs']} - \"\n","              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n","              f\"Test Loss: {avg_test_loss:.4f}, Test Acc: {acc:.2f}%\")\n","\n","        # Early Stopping Logic\n","        if acc > best_acc:\n","            best_acc = acc\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","            trigger_times = 0\n","            # Save the best model\n","            torch.save(model.state_dict(), f'best_nvit_{run_name}.pth')\n","        else:\n","            trigger_times += 1\n","            if trigger_times >= patience:\n","                print(\"Early stopping triggered!\")\n","                break\n","\n","        # Step the scheduler\n","        scheduler.step()\n","\n","        # Log best accuracy so far\n","        wandb.log({'best_test_acc': best_acc})\n","\n","    # Load Best Model Weights\n","    model.load_state_dict(best_model_wts)\n","    print(f\"Training completed. Best Test Accuracy: {best_acc:.2f}%\")\n","    wandb.finish()\n","\n","####################\n","# Main Function    #\n","####################\n","\n","if __name__ == '__main__':\n","    run_experiment()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"thv7FLam1gh6"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyOnbqc3XGuYAtTbMTwFuIQn"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}