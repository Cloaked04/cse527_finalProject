{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyOpRmFjO2NRQXb1LPIEK9Zy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":6,"metadata":{"id":"mZsECO2DYR4_","executionInfo":{"status":"ok","timestamp":1733629834950,"user_tz":300,"elapsed":193,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"}}},"outputs":[],"source":["import torch\n","from torch import nn, optim\n","from torch.utils.data import DataLoader\n","import torchvision\n","from torchvision import datasets, transforms\n","import wandb\n","import os\n","import random\n","import copy\n","import numpy as np\n","from einops import rearrange\n","from einops.layers.torch import Rearrange"]},{"cell_type":"code","source":["# Helper function\n","def pair(t):\n","    return t if isinstance(t, tuple) else (t, t)"],"metadata":{"id":"cx09FY9eYVmc","executionInfo":{"status":"ok","timestamp":1733629814153,"user_tz":300,"elapsed":2,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Attention and FeedForward classes without normalization\n","class Attention(nn.Module):\n","    def __init__(self, dim, *, dim_head=64, heads=8, dropout=0.0):\n","        super().__init__()\n","        inner_dim = dim_head * heads\n","        self.heads = heads\n","        self.dim_head = dim_head\n","        self.scale = dim_head ** -0.5\n","\n","        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n","        self.attn_drop = nn.Dropout(dropout)\n","        self.proj = nn.Linear(inner_dim, dim)\n","        self.proj_drop = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        B, N, C = x.shape\n","        qkv = self.to_qkv(x)  # (B, N, 3*inner_dim)\n","        qkv = qkv.reshape(B, N, 3, self.heads, self.dim_head)  # (B, N, 3, H, D)\n","        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, H, N, D)\n","        q, k, v = qkv[0], qkv[1], qkv[2]  # Each is (B, H, N, D)\n","\n","        q = q * self.scale\n","        attn = torch.matmul(q, k.transpose(-2, -1))  # (B, H, N, N)\n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        out = torch.matmul(attn, v)  # (B, H, N, D)\n","        out = out.transpose(1, 2).reshape(B, N, -1)  # (B, N, H*D)\n","        out = self.proj(out)  # (B, N, dim)\n","        out = self.proj_drop(out)\n","        return out\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, dim, hidden_dim, dropout=0.0):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_dim, dim),\n","            nn.Dropout(dropout)\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)"],"metadata":{"id":"lb9Q1kkpYX1D","executionInfo":{"status":"ok","timestamp":1733629814153,"user_tz":300,"elapsed":2,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Modified ViT for CIFAR-10 without normalization in transformer\n","class ViT(nn.Module):\n","    def __init__(\n","        self,\n","        *,\n","        image_size,\n","        patch_size,\n","        num_classes,\n","        dim,\n","        depth,\n","        heads,\n","        mlp_dim,\n","        dropout=0.0,\n","        emb_dropout=0.0,\n","        channels=3,\n","        dim_head=64\n","    ):\n","        super().__init__()\n","        image_height, image_width = pair(image_size)\n","        patch_height, patch_width = pair(patch_size)\n","\n","        assert image_height % patch_height == 0 and image_width % patch_width == 0, \\\n","            'Image dimensions must be divisible by the patch size.'\n","\n","        num_patches = (image_height // patch_height) * (image_width // patch_width)\n","        patch_dim = channels * patch_height * patch_width\n","\n","        self.patch_size = patch_size\n","        self.dim = dim\n","\n","        self.to_patch_embedding = nn.Sequential(\n","            Rearrange('b c (h ph) (w pw) -> b (h w) (ph pw c)', ph=patch_height, pw=patch_width),\n","            nn.Linear(patch_dim, dim)\n","        )\n","\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n","        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n","        self.dropout = nn.Dropout(emb_dropout)\n","\n","        self.transformer = nn.ModuleList([])\n","        for _ in range(depth):\n","            self.transformer.append(nn.ModuleList([\n","                # Removed LayerNorm layers\n","                Attention(dim, dim_head=dim_head, heads=heads, dropout=dropout),\n","                FeedForward(dim, hidden_dim=mlp_dim, dropout=dropout)\n","            ]))\n","\n","        # Retained LayerNorm before MLP head for classification stability\n","        self.mlp_head = nn.Sequential(\n","            nn.LayerNorm(dim),  # Optional: Remove if you want no normalization at all\n","            nn.Linear(dim, num_classes)\n","        )\n","\n","    def forward(self, img):\n","        x = self.to_patch_embedding(img)  # (B, N, dim)\n","        B, N, _ = x.shape\n","\n","        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, dim)\n","        x = torch.cat((cls_tokens, x), dim=1)  # (B, N+1, dim)\n","        x = x + self.pos_embedding[:, :N + 1, :]\n","        x = self.dropout(x)\n","\n","        for attn, ff in self.transformer:\n","            x = x + attn(x)  # Residual connection\n","            x = x + ff(x)    # Residual connection\n","\n","        x = x[:, 0]  # (B, dim)\n","        x = self.mlp_head(x)  # (B, num_classes)\n","        return x"],"metadata":{"id":"DbWkr6iFYb2G","executionInfo":{"status":"ok","timestamp":1733629815559,"user_tz":300,"elapsed":116,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def main():\n","    # Set random seeds for reproducibility\n","    torch.manual_seed(42)\n","    torch.cuda.manual_seed_all(42)\n","    np.random.seed(42)\n","    random.seed(42)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","    # Initialize wandb\n","    wandb.init(project='vit-cifar10-noNorm', config={\n","        'model': 'ViT',\n","        'dataset': 'CIFAR-10',\n","        'epochs': 100,\n","        'batch_size': 128,  # Batch size for CIFAR-10\n","        'learning_rate': 3e-4,\n","        'weight_decay': 1e-4,\n","        'image_size': 32,   # CIFAR-10 image size\n","        'patch_size': 4,    # Patch size for CIFAR-10\n","        'dim': 384,         # Model dimension\n","        'depth': 6,         # Transformer depth\n","        'heads': 6,         # Number of heads\n","        'mlp_dim': 384 * 4, # MLP hidden dimension\n","        'dropout': 0.1,\n","        'emb_dropout': 0.1,\n","        'num_classes': 10,\n","        'patience': 10      # Patience for early stopping\n","    })\n","    config = wandb.config\n","\n","    # Set device\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    # Data transforms\n","    transform_train = transforms.Compose([\n","        transforms.RandomCrop(config.image_size, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465),\n","                             (0.2023, 0.1994, 0.2010)),\n","    ])\n","\n","    transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465),\n","                             (0.2023, 0.1994, 0.2010)),\n","    ])\n","\n","    # Load CIFAR-10 dataset\n","    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n","    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True,\n","                              num_workers=4, pin_memory=True)\n","    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False,\n","                             num_workers=4, pin_memory=True)\n","\n","    # Initialize model\n","    model = ViT(\n","        image_size=config.image_size,\n","        patch_size=config.patch_size,\n","        num_classes=config.num_classes,\n","        dim=config.dim,\n","        depth=config.depth,\n","        heads=config.heads,\n","        mlp_dim=config.mlp_dim,\n","        dropout=config.dropout,\n","        emb_dropout=config.emb_dropout,\n","        channels=3,\n","        dim_head=64\n","    ).to(device)\n","\n","    # Define Loss Function and Optimizer\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate,\n","                            weight_decay=config.weight_decay)\n","    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs)\n","\n","    # Early Stopping Parameters\n","    best_acc = 0.0\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    patience = config.patience\n","    trigger_times = 0\n","\n","    for epoch in range(config.epochs):\n","        model.train()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        for batch_idx, (inputs, targets) in enumerate(train_loader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","            if batch_idx % 100 == 0:\n","                wandb.log({\n","                    'train_loss': running_loss / (batch_idx + 1),\n","                    'train_acc': 100. * correct / total,\n","                    'learning_rate': optimizer.param_groups[0]['lr']\n","                })\n","\n","        # Calculate average training loss and accuracy\n","        train_loss = running_loss / len(train_loader)\n","        train_acc = 100. * correct / total\n","\n","        # Validation Phase\n","        model.eval()\n","        test_loss = 0.0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, targets in test_loader:\n","                inputs, targets = inputs.to(device), targets.to(device)\n","                outputs = model(inputs)\n","                loss = criterion(outputs, targets)\n","\n","                test_loss += loss.item()\n","                _, predicted = outputs.max(1)\n","                total += targets.size(0)\n","                correct += predicted.eq(targets).sum().item()\n","\n","        # Calculate average test loss and accuracy\n","        acc = 100. * correct / total\n","        avg_test_loss = test_loss / len(test_loader)\n","        wandb.log({\n","            'test_loss': avg_test_loss,\n","            'test_acc': acc,\n","            'epoch': epoch\n","        })\n","\n","        print(f\"Epoch {epoch + 1}/{config.epochs} - \"\n","              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n","              f\"Test Loss: {avg_test_loss:.4f}, Test Acc: {acc:.2f}%\")\n","\n","        # Early Stopping Check\n","        if acc > best_acc:\n","            best_acc = acc\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","            trigger_times = 0\n","            # Save the best model\n","            torch.save(model.state_dict(), f'best_cifar10_vit_epoch{epoch+1}.pth')\n","        else:\n","            trigger_times += 1\n","            if trigger_times >= patience:\n","                print(\"Early stopping triggered!\")\n","                break\n","\n","        # Step the scheduler\n","        scheduler.step()\n","\n","        # Log best accuracy so far\n","        wandb.log({'best_test_acc': best_acc})\n","\n","    # Load best model weights after training\n","    model.load_state_dict(best_model_wts)\n","    print(f\"Training completed. Best Test Accuracy: {best_acc:.2f}%\")\n","    wandb.finish()\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":924},"id":"rrFmr94WYfIj","executionInfo":{"status":"ok","timestamp":1733630496395,"user_tz":300,"elapsed":658177,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"}},"outputId":"ee3d47ec-ac6d-4d20-d4ca-f26e9c3ca5bf"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":[" ··········\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.18.7"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20241208_035057-9izjkw27</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar10-noNorm/runs/9izjkw27' target=\"_blank\">hopeful-wildflower-1</a></strong> to <a href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar10-noNorm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar10-noNorm' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/vit-cifar10-noNorm</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar10-noNorm/runs/9izjkw27' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/vit-cifar10-noNorm/runs/9izjkw27</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170M/170M [00:05<00:00, 33.3MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","Epoch 1/100 - Train Loss: 2.0013, Train Acc: 25.10%, Test Loss: 2.3016, Test Acc: 11.76%\n","Epoch 2/100 - Train Loss: 2.3171, Train Acc: 10.49%, Test Loss: 2.3108, Test Acc: 10.52%\n","Epoch 3/100 - Train Loss: 2.3131, Train Acc: 10.33%, Test Loss: 2.3150, Test Acc: 10.00%\n","Epoch 4/100 - Train Loss: 2.3127, Train Acc: 10.11%, Test Loss: 2.3049, Test Acc: 10.56%\n","Epoch 5/100 - Train Loss: 2.3098, Train Acc: 10.02%, Test Loss: 2.2887, Test Acc: 11.92%\n","Epoch 6/100 - Train Loss: 2.3100, Train Acc: 9.86%, Test Loss: 2.3008, Test Acc: 12.24%\n","Epoch 7/100 - Train Loss: 2.3076, Train Acc: 10.02%, Test Loss: 2.3053, Test Acc: 10.00%\n","Epoch 8/100 - Train Loss: 2.3066, Train Acc: 10.06%, Test Loss: 2.3056, Test Acc: 10.43%\n","Epoch 9/100 - Train Loss: 2.3055, Train Acc: 10.13%, Test Loss: 2.3044, Test Acc: 9.87%\n","Epoch 10/100 - Train Loss: 2.3053, Train Acc: 10.19%, Test Loss: 2.3035, Test Acc: 10.00%\n","Epoch 11/100 - Train Loss: 2.3056, Train Acc: 9.95%, Test Loss: 2.3080, Test Acc: 10.36%\n","Epoch 12/100 - Train Loss: 2.3052, Train Acc: 9.79%, Test Loss: 2.3050, Test Acc: 10.00%\n","Epoch 13/100 - Train Loss: 2.3041, Train Acc: 10.20%, Test Loss: 2.3043, Test Acc: 10.00%\n","Epoch 14/100 - Train Loss: 2.3037, Train Acc: 10.39%, Test Loss: 2.3038, Test Acc: 10.00%\n","Epoch 15/100 - Train Loss: 2.3038, Train Acc: 10.25%, Test Loss: 2.3046, Test Acc: 10.00%\n","Epoch 16/100 - Train Loss: 2.3027, Train Acc: 10.43%, Test Loss: 2.3242, Test Acc: 10.00%\n","Early stopping triggered!\n","Training completed. Best Test Accuracy: 12.24%\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <style>\n","        .wandb-row {\n","            display: flex;\n","            flex-direction: row;\n","            flex-wrap: wrap;\n","            justify-content: flex-start;\n","            width: 100%;\n","        }\n","        .wandb-col {\n","            display: flex;\n","            flex-direction: column;\n","            flex-basis: 100%;\n","            flex: 1;\n","            padding: 10px;\n","        }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_test_acc</td><td>▁▁▁▁▃██████████</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>learning_rate</td><td>███████████▇▇▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▃▃▃▃▂▂▂▁▁</td></tr><tr><td>test_acc</td><td>▇▃▁▃▇█▁▃▁▁▂▁▁▁▁▁</td></tr><tr><td>test_loss</td><td>▄▅▆▄▁▃▄▄▄▄▅▄▄▄▄█</td></tr><tr><td>train_acc</td><td>▃▇█▃▃▂▃▃▃▂▂▂▂▃▂▂▄▃▂▃▃▃▂▃▃▂▂▃▂▂▁▂▃▂▂▂▃▃▃▃</td></tr><tr><td>train_loss</td><td>█▁▁▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_test_acc</td><td>12.24</td></tr><tr><td>epoch</td><td>15</td></tr><tr><td>learning_rate</td><td>0.00028</td></tr><tr><td>test_acc</td><td>10</td></tr><tr><td>test_loss</td><td>2.32423</td></tr><tr><td>train_acc</td><td>10.43137</td></tr><tr><td>train_loss</td><td>2.303</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">hopeful-wildflower-1</strong> at: <a href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar10-noNorm/runs/9izjkw27' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/vit-cifar10-noNorm/runs/9izjkw27</a><br/> View project at: <a href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar10-noNorm' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/vit-cifar10-noNorm</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20241208_035057-9izjkw27/logs</code>"]},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"7KoZRGAJY7mz"},"execution_count":null,"outputs":[]}]}