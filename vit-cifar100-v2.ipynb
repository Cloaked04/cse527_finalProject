{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":9367,"status":"ok","timestamp":1733033691352,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"},"user_tz":300},"id":"KTR3M0Rm4E-c"},"outputs":[],"source":["import torch\n","from torch import nn, optim\n","from torch.utils.data import DataLoader\n","import torchvision\n","from torchvision import datasets, transforms\n","import wandb\n","import os\n","import numpy as np\n","from einops import rearrange\n","from einops.layers.torch import Rearrange\n","import math\n","import random\n","import copy"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1733033691353,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"},"user_tz":300},"id":"M2O3CR366mru"},"outputs":[],"source":["# Define MixUp Function\n","def mixup_data(x, y, alpha=0.2):\n","    '''Returns mixed inputs, pairs of targets, and lambda'''\n","    if alpha \u003e 0:\n","        lam = np.random.beta(alpha, alpha)\n","    else:\n","        lam = 1\n","    batch_size = x.size()[0]\n","    index = torch.randperm(batch_size).to(x.device)\n","\n","    mixed_x = lam * x + (1 - lam) * x[index, :]\n","    y_a, y_b = y, y[index]\n","    return mixed_x, y_a, y_b, lam\n","\n","# Define MixUp Criterion\n","def mixup_criterion(criterion, pred, y_a, y_b, lam):\n","    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1733033691353,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"},"user_tz":300},"id":"mRbBAYEb6rxb"},"outputs":[],"source":["class Attention(nn.Module):\n","    def __init__(self, dim, *, dim_head=64, heads=8, dropout=0.0):\n","        super().__init__()\n","        inner_dim = dim_head * heads\n","        self.heads = heads\n","        self.dim_head = dim_head\n","        self.scale = dim_head ** -0.5\n","\n","        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n","        self.attn_drop = nn.Dropout(dropout)\n","        self.proj = nn.Linear(inner_dim, dim)\n","        self.proj_drop = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        B, N, C = x.shape\n","        qkv = self.to_qkv(x)\n","        qkv = qkv.reshape(B, N, 3, self.heads, self.dim_head)\n","        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, heads, N, dim_head)\n","        q, k, v = qkv[0], qkv[1], qkv[2]\n","        q = q * self.scale\n","        attn = torch.matmul(q, k.transpose(-2, -1))\n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","        out = torch.matmul(attn, v)\n","        out = out.transpose(1, 2).reshape(B, N, -1)\n","        out = self.proj(out)\n","        out = self.proj_drop(out)\n","        return out\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, dim, dim_inner, dropout=0.0):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(dim, dim_inner),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(dim_inner, dim),\n","            nn.Dropout(dropout)\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","def pair(t):\n","    return t if isinstance(t, tuple) else (t, t)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1733033691353,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"},"user_tz":300},"id":"95msOqLpgt8u"},"outputs":[],"source":["class ConvStem(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels // 2, kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm2d(out_channels // 2),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels // 2, out_channels, kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.conv(x)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1733033691353,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"},"user_tz":300},"id":"B2_x7SO6kA2G"},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1733033691353,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"},"user_tz":300},"id":"5iouVTpx6uqW"},"outputs":[],"source":["# Modified ViT for CIFAR-100\n","class ViT(nn.Module):\n","    def __init__(\n","        self,\n","        *,\n","        image_size,\n","        patch_size,\n","        num_classes,\n","        dim,\n","        depth,\n","        heads,\n","        mlp_dim,\n","        dropout=0.0,\n","        emb_dropout=0.0,\n","        channels=3,\n","        dim_head=64\n","    ):\n","        super().__init__()\n","        self.conv_stem = ConvStem(channels, dim)\n","        image_height, image_width = pair(image_size)\n","        patch_height, patch_width = pair(patch_size)\n","\n","        assert image_height % patch_height == 0 and image_width % patch_width == 0, \\\n","            'Image dimensions must be divisible by the patch size.'\n","\n","        num_patches = (image_height // patch_height) * (image_width // patch_width)\n","        #To use without convstream\n","        patch_dim = channels * patch_height * patch_width\n","\n","        #Updated patch_dim to use with convstream\n","        patch_dim = dim*patch_height*patch_width\n","\n","        self.patch_size = patch_size\n","        self.dim = dim\n","\n","        self.to_patch_embedding = nn.Sequential(\n","            Rearrange('b c (h ph) (w pw) -\u003e b (h w) (ph pw c)', ph=patch_height, pw=patch_width),\n","            nn.Linear(patch_dim, dim)\n","        )\n","\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n","        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n","        self.dropout = nn.Dropout(emb_dropout)\n","\n","        self.transformer = nn.ModuleList([])\n","        for _ in range(depth):\n","            self.transformer.append(nn.ModuleList([\n","                nn.LayerNorm(dim),\n","                Attention(dim, dim_head=dim_head, heads=heads, dropout=dropout),\n","                nn.LayerNorm(dim),\n","                FeedForward(dim, dim_inner=mlp_dim, dropout=dropout)\n","            ]))\n","\n","        self.mlp_head = nn.Sequential(\n","            nn.LayerNorm(dim),\n","            nn.Linear(dim, num_classes)\n","        )\n","\n","    def forward(self, img):\n","        x = self.conv_stem(img)\n","        x = self.to_patch_embedding(x)\n","        B, N, _ = x.shape\n","\n","        cls_tokens = self.cls_token.expand(B, -1, -1)\n","        x = torch.cat((cls_tokens, x), dim=1)\n","        x = x + self.pos_embedding[:, :N + 1, :]\n","        x = self.dropout(x)\n","\n","        for norm1, attn, norm2, ff in self.transformer:\n","            x = x + attn(norm1(x))\n","            x = x + ff(norm2(x))\n","\n","        x = x[:, 0]\n","        x = self.mlp_head(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"cJUWfciv6xel"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"]},{"data":{"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) =\u003e {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() =\u003e {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() =\u003e reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data =\u003e {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "],"text/plain":["\u003cIPython.core.display.Javascript object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W\u0026B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["Tracking run with wandb version 0.18.7"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in \u003ccode\u003e/content/wandb/run-20241201_061525-mvbnt6es\u003c/code\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run \u003cstrong\u003e\u003ca href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100/runs/mvbnt6es' target=\"_blank\"\u003egiddy-snowflake-12\u003c/a\u003e\u003c/strong\u003e to \u003ca href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100' target=\"_blank\"\u003eWeights \u0026 Biases\u003c/a\u003e (\u003ca href='https://wandb.me/developer-guide' target=\"_blank\"\u003edocs\u003c/a\u003e)\u003cbr/\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at \u003ca href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100' target=\"_blank\"\u003ehttps://wandb.ai/pratkumar-stony-brook-university/vit-cifar100\u003c/a\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at \u003ca href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100/runs/mvbnt6es' target=\"_blank\"\u003ehttps://wandb.ai/pratkumar-stony-brook-university/vit-cifar100/runs/mvbnt6es\u003c/a\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 169M/169M [00:03\u003c00:00, 43.1MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/cifar-100-python.tar.gz to ./data\n","Files already downloaded and verified\n","Epoch 1/200 - Train Loss: 4.5593, Train Acc: 2.37%, Test Loss: 4.2830, Test Acc: 5.09%\n","Epoch 2/200 - Train Loss: 4.2852, Train Acc: 5.71%, Test Loss: 4.0092, Test Acc: 9.92%\n","Epoch 3/200 - Train Loss: 4.0677, Train Acc: 9.98%, Test Loss: 3.7008, Test Acc: 16.48%\n","Epoch 4/200 - Train Loss: 3.8902, Train Acc: 13.82%, Test Loss: 3.4680, Test Acc: 21.80%\n","Epoch 5/200 - Train Loss: 3.7276, Train Acc: 17.73%, Test Loss: 3.2952, Test Acc: 25.92%\n","Epoch 6/200 - Train Loss: 3.6112, Train Acc: 20.80%, Test Loss: 3.1159, Test Acc: 30.33%\n","Epoch 7/200 - Train Loss: 3.4776, Train Acc: 24.11%, Test Loss: 2.9606, Test Acc: 34.34%\n","Epoch 8/200 - Train Loss: 3.3490, Train Acc: 27.06%, Test Loss: 2.8758, Test Acc: 37.13%\n","Epoch 9/200 - Train Loss: 3.2457, Train Acc: 29.90%, Test Loss: 2.7649, Test Acc: 39.96%\n","Epoch 10/200 - Train Loss: 3.1772, Train Acc: 31.47%, Test Loss: 2.6686, Test Acc: 42.21%\n","Epoch 11/200 - Train Loss: 3.1122, Train Acc: 33.64%, Test Loss: 2.6378, Test Acc: 43.11%\n","Epoch 12/200 - Train Loss: 3.0530, Train Acc: 35.32%, Test Loss: 2.5568, Test Acc: 45.17%\n","Epoch 13/200 - Train Loss: 3.0477, Train Acc: 35.83%, Test Loss: 2.5159, Test Acc: 46.63%\n","Epoch 14/200 - Train Loss: 2.8809, Train Acc: 39.53%, Test Loss: 2.4505, Test Acc: 48.01%\n","Epoch 15/200 - Train Loss: 2.8715, Train Acc: 39.95%, Test Loss: 2.4826, Test Acc: 47.25%\n","Epoch 16/200 - Train Loss: 2.8624, Train Acc: 41.01%, Test Loss: 2.4239, Test Acc: 48.77%\n","Epoch 17/200 - Train Loss: 2.7781, Train Acc: 42.41%, Test Loss: 2.3480, Test Acc: 51.27%\n","Epoch 18/200 - Train Loss: 2.7868, Train Acc: 42.49%, Test Loss: 2.3212, Test Acc: 51.73%\n","Epoch 19/200 - Train Loss: 2.7365, Train Acc: 44.10%, Test Loss: 2.3012, Test Acc: 52.81%\n","Epoch 20/200 - Train Loss: 2.6825, Train Acc: 45.60%, Test Loss: 2.2935, Test Acc: 52.58%\n","Epoch 21/200 - Train Loss: 2.7007, Train Acc: 45.59%, Test Loss: 2.3447, Test Acc: 51.07%\n","Epoch 22/200 - Train Loss: 2.6574, Train Acc: 46.78%, Test Loss: 2.2766, Test Acc: 52.83%\n","Epoch 23/200 - Train Loss: 2.6043, Train Acc: 48.11%, Test Loss: 2.2352, Test Acc: 54.69%\n","Epoch 24/200 - Train Loss: 2.5404, Train Acc: 49.96%, Test Loss: 2.1676, Test Acc: 56.43%\n","Epoch 25/200 - Train Loss: 2.4538, Train Acc: 52.26%, Test Loss: 2.1777, Test Acc: 56.33%\n","Epoch 26/200 - Train Loss: 2.4426, Train Acc: 53.28%, Test Loss: 2.1846, Test Acc: 56.89%\n","Epoch 27/200 - Train Loss: 2.3686, Train Acc: 55.14%, Test Loss: 2.1457, Test Acc: 57.46%\n","Epoch 28/200 - Train Loss: 2.3802, Train Acc: 54.99%, Test Loss: 2.1681, Test Acc: 57.39%\n","Epoch 29/200 - Train Loss: 2.2596, Train Acc: 58.28%, Test Loss: 2.1475, Test Acc: 57.94%\n","Epoch 30/200 - Train Loss: 2.2278, Train Acc: 59.49%, Test Loss: 2.1740, Test Acc: 57.08%\n","Epoch 31/200 - Train Loss: 2.2579, Train Acc: 59.24%, Test Loss: 2.0851, Test Acc: 59.75%\n","Epoch 32/200 - Train Loss: 2.2186, Train Acc: 60.52%, Test Loss: 2.1514, Test Acc: 58.56%\n","Epoch 33/200 - Train Loss: 2.1471, Train Acc: 62.67%, Test Loss: 2.1166, Test Acc: 59.03%\n","Epoch 34/200 - Train Loss: 2.0661, Train Acc: 65.02%, Test Loss: 2.0978, Test Acc: 59.89%\n","Epoch 35/200 - Train Loss: 1.9990, Train Acc: 66.98%, Test Loss: 2.0849, Test Acc: 60.64%\n","Epoch 36/200 - Train Loss: 2.0435, Train Acc: 66.37%, Test Loss: 2.1262, Test Acc: 59.67%\n","Epoch 37/200 - Train Loss: 2.0088, Train Acc: 67.80%, Test Loss: 2.1785, Test Acc: 58.48%\n","Epoch 38/200 - Train Loss: 1.9353, Train Acc: 69.59%, Test Loss: 2.1203, Test Acc: 60.23%\n","Epoch 39/200 - Train Loss: 1.8023, Train Acc: 73.22%, Test Loss: 2.0926, Test Acc: 61.12%\n","Epoch 40/200 - Train Loss: 1.9381, Train Acc: 70.40%, Test Loss: 2.1243, Test Acc: 60.60%\n","Epoch 41/200 - Train Loss: 1.8724, Train Acc: 71.99%, Test Loss: 2.1097, Test Acc: 60.90%\n","Epoch 42/200 - Train Loss: 1.8801, Train Acc: 72.08%, Test Loss: 2.1119, Test Acc: 60.46%\n","Epoch 43/200 - Train Loss: 1.9061, Train Acc: 71.82%, Test Loss: 2.1098, Test Acc: 60.83%\n","Epoch 44/200 - Train Loss: 1.7953, Train Acc: 74.83%, Test Loss: 2.1363, Test Acc: 60.24%\n","Epoch 45/200 - Train Loss: 1.7445, Train Acc: 76.56%, Test Loss: 2.1293, Test Acc: 61.14%\n","Epoch 46/200 - Train Loss: 1.8754, Train Acc: 73.26%, Test Loss: 2.0988, Test Acc: 61.30%\n","Epoch 47/200 - Train Loss: 1.7181, Train Acc: 77.13%, Test Loss: 2.0753, Test Acc: 62.21%\n","Epoch 48/200 - Train Loss: 1.7780, Train Acc: 76.03%, Test Loss: 2.0892, Test Acc: 61.92%\n","Epoch 49/200 - Train Loss: 1.6182, Train Acc: 80.26%, Test Loss: 2.1212, Test Acc: 60.89%\n","Epoch 50/200 - Train Loss: 1.6886, Train Acc: 78.55%, Test Loss: 2.1020, Test Acc: 61.26%\n","Epoch 51/200 - Train Loss: 1.7568, Train Acc: 76.57%, Test Loss: 2.1290, Test Acc: 60.81%\n","Epoch 52/200 - Train Loss: 1.7719, Train Acc: 76.40%, Test Loss: 2.1014, Test Acc: 61.82%\n","Epoch 53/200 - Train Loss: 1.7132, Train Acc: 77.98%, Test Loss: 2.1096, Test Acc: 61.79%\n","Epoch 54/200 - Train Loss: 1.6502, Train Acc: 79.54%, Test Loss: 2.1258, Test Acc: 61.30%\n","Epoch 55/200 - Train Loss: 1.7017, Train Acc: 78.41%, Test Loss: 2.1043, Test Acc: 62.09%\n","Epoch 56/200 - Train Loss: 1.6651, Train Acc: 79.59%, Test Loss: 2.0763, Test Acc: 62.36%\n","Epoch 57/200 - Train Loss: 1.6599, Train Acc: 79.62%, Test Loss: 2.0924, Test Acc: 62.55%\n","Epoch 58/200 - Train Loss: 1.5798, Train Acc: 81.43%, Test Loss: 2.1001, Test Acc: 61.93%\n","Epoch 59/200 - Train Loss: 1.5445, Train Acc: 82.72%, Test Loss: 2.1151, Test Acc: 61.90%\n","Epoch 60/200 - Train Loss: 1.6468, Train Acc: 80.16%, Test Loss: 2.0927, Test Acc: 61.95%\n","Epoch 61/200 - Train Loss: 1.5902, Train Acc: 81.75%, Test Loss: 2.0974, Test Acc: 62.45%\n","Epoch 62/200 - Train Loss: 1.6275, Train Acc: 80.51%, Test Loss: 2.1091, Test Acc: 61.99%\n","Epoch 63/200 - Train Loss: 1.5819, Train Acc: 81.72%, Test Loss: 2.0846, Test Acc: 62.51%\n","Epoch 64/200 - Train Loss: 1.5570, Train Acc: 82.34%, Test Loss: 2.1283, Test Acc: 62.04%\n","Epoch 65/200 - Train Loss: 1.5735, Train Acc: 81.71%, Test Loss: 2.1100, Test Acc: 62.21%\n","Epoch 66/200 - Train Loss: 1.6160, Train Acc: 80.77%, Test Loss: 2.0776, Test Acc: 62.88%\n","Epoch 67/200 - Train Loss: 1.5742, Train Acc: 81.76%, Test Loss: 2.0683, Test Acc: 63.15%\n","Epoch 68/200 - Train Loss: 1.5724, Train Acc: 82.15%, Test Loss: 2.0962, Test Acc: 62.72%\n","Epoch 69/200 - Train Loss: 1.5218, Train Acc: 83.14%, Test Loss: 2.1268, Test Acc: 62.05%\n","Epoch 70/200 - Train Loss: 1.4839, Train Acc: 84.14%, Test Loss: 2.1133, Test Acc: 62.74%\n","Epoch 71/200 - Train Loss: 1.5531, Train Acc: 82.31%, Test Loss: 2.1100, Test Acc: 62.65%\n","Epoch 72/200 - Train Loss: 1.5461, Train Acc: 82.39%, Test Loss: 2.1255, Test Acc: 62.72%\n","Epoch 73/200 - Train Loss: 1.5403, Train Acc: 82.54%, Test Loss: 2.1162, Test Acc: 62.51%\n","Epoch 74/200 - Train Loss: 1.5114, Train Acc: 83.44%, Test Loss: 2.1491, Test Acc: 61.63%\n","Epoch 75/200 - Train Loss: 1.5750, Train Acc: 81.63%, Test Loss: 2.0972, Test Acc: 63.10%\n","Epoch 76/200 - Train Loss: 1.5339, Train Acc: 82.44%, Test Loss: 2.1146, Test Acc: 62.22%\n","Epoch 77/200 - Train Loss: 1.5345, Train Acc: 82.81%, Test Loss: 2.0951, Test Acc: 62.66%\n","Epoch 78/200 - Train Loss: 1.5078, Train Acc: 83.31%, Test Loss: 2.1261, Test Acc: 62.64%\n","Epoch 79/200 - Train Loss: 1.4943, Train Acc: 83.79%, Test Loss: 2.0864, Test Acc: 62.59%\n","Epoch 80/200 - Train Loss: 1.5015, Train Acc: 83.51%, Test Loss: 2.1308, Test Acc: 62.44%\n","Epoch 81/200 - Train Loss: 1.5127, Train Acc: 82.99%, Test Loss: 2.0903, Test Acc: 63.25%\n","Epoch 82/200 - Train Loss: 1.5049, Train Acc: 83.24%, Test Loss: 2.1253, Test Acc: 62.61%\n","Epoch 83/200 - Train Loss: 1.5151, Train Acc: 83.08%, Test Loss: 2.1241, Test Acc: 62.56%\n","Epoch 84/200 - Train Loss: 1.4858, Train Acc: 83.86%, Test Loss: 2.1328, Test Acc: 62.16%\n","Epoch 85/200 - Train Loss: 1.5425, Train Acc: 82.48%, Test Loss: 2.1124, Test Acc: 63.23%\n","Epoch 86/200 - Train Loss: 1.4834, Train Acc: 83.85%, Test Loss: 2.1230, Test Acc: 62.92%\n","Epoch 87/200 - Train Loss: 1.4882, Train Acc: 84.01%, Test Loss: 2.1151, Test Acc: 63.26%\n","Epoch 88/200 - Train Loss: 1.5332, Train Acc: 82.78%, Test Loss: 2.1120, Test Acc: 63.22%\n","Epoch 89/200 - Train Loss: 1.5404, Train Acc: 82.44%, Test Loss: 2.1047, Test Acc: 63.62%\n","Epoch 90/200 - Train Loss: 1.4601, Train Acc: 84.46%, Test Loss: 2.1299, Test Acc: 62.71%\n","Epoch 91/200 - Train Loss: 1.3954, Train Acc: 85.88%, Test Loss: 2.1365, Test Acc: 62.71%\n","Epoch 92/200 - Train Loss: 1.5453, Train Acc: 82.26%, Test Loss: 2.1243, Test Acc: 62.74%\n","Epoch 93/200 - Train Loss: 1.4817, Train Acc: 83.91%, Test Loss: 2.1362, Test Acc: 62.95%\n","Epoch 94/200 - Train Loss: 1.5053, Train Acc: 83.45%, Test Loss: 2.1432, Test Acc: 63.24%\n","Epoch 95/200 - Train Loss: 1.5063, Train Acc: 83.41%, Test Loss: 2.1452, Test Acc: 63.34%\n","Epoch 96/200 - Train Loss: 1.5158, Train Acc: 83.18%, Test Loss: 2.1364, Test Acc: 62.83%\n","Epoch 97/200 - Train Loss: 1.5106, Train Acc: 83.22%, Test Loss: 2.1309, Test Acc: 63.42%\n","Epoch 98/200 - Train Loss: 1.5188, Train Acc: 83.03%, Test Loss: 2.1275, Test Acc: 63.15%\n","Epoch 99/200 - Train Loss: 1.5244, Train Acc: 82.89%, Test Loss: 2.1296, Test Acc: 63.31%\n","Epoch 100/200 - Train Loss: 1.4233, Train Acc: 85.51%, Test Loss: 2.1570, Test Acc: 63.33%\n","Epoch 101/200 - Train Loss: 1.3577, Train Acc: 87.01%, Test Loss: 2.1565, Test Acc: 63.54%\n","Epoch 102/200 - Train Loss: 1.4257, Train Acc: 85.51%, Test Loss: 2.1130, Test Acc: 63.66%\n","Epoch 103/200 - Train Loss: 1.4405, Train Acc: 85.04%, Test Loss: 2.1464, Test Acc: 63.41%\n","Epoch 104/200 - Train Loss: 1.4395, Train Acc: 84.99%, Test Loss: 2.1364, Test Acc: 63.72%\n","Epoch 105/200 - Train Loss: 1.3493, Train Acc: 87.41%, Test Loss: 2.1504, Test Acc: 63.45%\n","Epoch 106/200 - Train Loss: 1.4334, Train Acc: 85.00%, Test Loss: 2.1962, Test Acc: 63.34%\n","Epoch 107/200 - Train Loss: 1.4442, Train Acc: 84.89%, Test Loss: 2.1837, Test Acc: 62.75%\n","Epoch 108/200 - Train Loss: 1.4270, Train Acc: 85.33%, Test Loss: 2.1414, Test Acc: 63.71%\n","Epoch 109/200 - Train Loss: 1.4264, Train Acc: 85.30%, Test Loss: 2.1341, Test Acc: 63.55%\n","Epoch 110/200 - Train Loss: 1.4120, Train Acc: 85.86%, Test Loss: 2.1645, Test Acc: 63.54%\n","Epoch 111/200 - Train Loss: 1.3840, Train Acc: 86.64%, Test Loss: 2.1385, Test Acc: 63.66%\n","Epoch 112/200 - Train Loss: 1.4180, Train Acc: 85.34%, Test Loss: 2.1337, Test Acc: 63.18%\n","Epoch 113/200 - Train Loss: 1.4977, Train Acc: 83.60%, Test Loss: 2.1423, Test Acc: 63.48%\n","Epoch 114/200 - Train Loss: 1.3893, Train Acc: 86.26%, Test Loss: 2.1204, Test Acc: 64.10%\n","Epoch 115/200 - Train Loss: 1.3394, Train Acc: 87.58%, Test Loss: 2.1117, Test Acc: 64.03%\n","Epoch 116/200 - Train Loss: 1.3215, Train Acc: 88.03%, Test Loss: 2.1404, Test Acc: 63.91%\n","Epoch 117/200 - Train Loss: 1.3366, Train Acc: 87.56%, Test Loss: 2.1310, Test Acc: 64.02%\n","Epoch 118/200 - Train Loss: 1.4903, Train Acc: 83.72%, Test Loss: 2.1317, Test Acc: 64.18%\n","Epoch 119/200 - Train Loss: 1.4412, Train Acc: 85.02%, Test Loss: 2.1646, Test Acc: 63.40%\n","Epoch 120/200 - Train Loss: 1.3584, Train Acc: 87.29%, Test Loss: 2.1689, Test Acc: 63.58%\n","Epoch 121/200 - Train Loss: 1.4109, Train Acc: 85.74%, Test Loss: 2.1453, Test Acc: 63.73%\n","Epoch 122/200 - Train Loss: 1.4529, Train Acc: 84.39%, Test Loss: 2.1515, Test Acc: 64.29%\n","Epoch 123/200 - Train Loss: 1.3646, Train Acc: 86.97%, Test Loss: 2.1477, Test Acc: 63.66%\n","Epoch 124/200 - Train Loss: 1.4259, Train Acc: 85.64%, Test Loss: 2.1445, Test Acc: 64.02%\n","Epoch 125/200 - Train Loss: 1.4251, Train Acc: 85.50%, Test Loss: 2.1482, Test Acc: 63.26%\n","Epoch 126/200 - Train Loss: 1.5044, Train Acc: 83.42%, Test Loss: 2.1487, Test Acc: 64.36%\n","Epoch 127/200 - Train Loss: 1.3909, Train Acc: 86.49%, Test Loss: 2.1544, Test Acc: 63.38%\n","Epoch 128/200 - Train Loss: 1.4203, Train Acc: 85.63%, Test Loss: 2.1406, Test Acc: 64.26%\n","Epoch 129/200 - Train Loss: 1.3946, Train Acc: 86.42%, Test Loss: 2.1370, Test Acc: 64.03%\n","Epoch 130/200 - Train Loss: 1.3699, Train Acc: 87.12%, Test Loss: 2.1240, Test Acc: 64.66%\n","Epoch 131/200 - Train Loss: 1.3567, Train Acc: 87.13%, Test Loss: 2.1702, Test Acc: 64.28%\n","Epoch 132/200 - Train Loss: 1.4540, Train Acc: 84.69%, Test Loss: 2.1064, Test Acc: 64.94%\n","Epoch 133/200 - Train Loss: 1.3574, Train Acc: 87.00%, Test Loss: 2.1477, Test Acc: 64.41%\n","Epoch 134/200 - Train Loss: 1.3781, Train Acc: 86.63%, Test Loss: 2.1341, Test Acc: 64.63%\n","Epoch 135/200 - Train Loss: 1.4611, Train Acc: 84.47%, Test Loss: 2.1501, Test Acc: 64.62%\n","Epoch 136/200 - Train Loss: 1.3623, Train Acc: 87.18%, Test Loss: 2.1494, Test Acc: 64.43%\n","Epoch 137/200 - Train Loss: 1.3053, Train Acc: 88.64%, Test Loss: 2.1403, Test Acc: 64.59%\n","Epoch 138/200 - Train Loss: 1.4043, Train Acc: 85.94%, Test Loss: 2.1449, Test Acc: 64.73%\n","Epoch 139/200 - Train Loss: 1.3614, Train Acc: 86.95%, Test Loss: 2.1364, Test Acc: 64.60%\n","Epoch 140/200 - Train Loss: 1.3396, Train Acc: 87.41%, Test Loss: 2.1494, Test Acc: 64.92%\n","Epoch 141/200 - Train Loss: 1.3549, Train Acc: 87.29%, Test Loss: 2.1432, Test Acc: 64.69%\n","Epoch 142/200 - Train Loss: 1.3431, Train Acc: 87.28%, Test Loss: 2.1083, Test Acc: 65.24%\n","Epoch 143/200 - Train Loss: 1.4183, Train Acc: 85.42%, Test Loss: 2.1168, Test Acc: 65.35%\n","Epoch 144/200 - Train Loss: 1.4112, Train Acc: 86.05%, Test Loss: 2.1144, Test Acc: 65.22%\n","Epoch 145/200 - Train Loss: 1.4323, Train Acc: 85.29%, Test Loss: 2.1083, Test Acc: 65.07%\n","Epoch 146/200 - Train Loss: 1.3789, Train Acc: 86.61%, Test Loss: 2.1207, Test Acc: 64.73%\n","Epoch 147/200 - Train Loss: 1.3800, Train Acc: 86.63%, Test Loss: 2.1155, Test Acc: 65.42%\n","Epoch 148/200 - Train Loss: 1.3836, Train Acc: 86.38%, Test Loss: 2.1226, Test Acc: 65.25%\n","Epoch 149/200 - Train Loss: 1.3355, Train Acc: 87.68%, Test Loss: 2.1312, Test Acc: 65.03%\n","Epoch 150/200 - Train Loss: 1.3674, Train Acc: 86.98%, Test Loss: 2.1159, Test Acc: 65.28%\n","Epoch 151/200 - Train Loss: 1.3526, Train Acc: 87.17%, Test Loss: 2.1054, Test Acc: 65.32%\n","Epoch 152/200 - Train Loss: 1.4356, Train Acc: 85.18%, Test Loss: 2.1237, Test Acc: 65.22%\n","Epoch 153/200 - Train Loss: 1.3849, Train Acc: 86.80%, Test Loss: 2.1272, Test Acc: 65.23%\n","Epoch 154/200 - Train Loss: 1.3728, Train Acc: 86.94%, Test Loss: 2.1114, Test Acc: 65.44%\n","Epoch 155/200 - Train Loss: 1.3144, Train Acc: 88.18%, Test Loss: 2.1135, Test Acc: 65.27%\n","Epoch 156/200 - Train Loss: 1.3448, Train Acc: 87.57%, Test Loss: 2.1207, Test Acc: 65.21%\n","Epoch 157/200 - Train Loss: 1.3583, Train Acc: 87.21%, Test Loss: 2.1192, Test Acc: 65.56%\n","Epoch 158/200 - Train Loss: 1.3582, Train Acc: 87.16%, Test Loss: 2.1052, Test Acc: 65.48%\n","Epoch 159/200 - Train Loss: 1.3667, Train Acc: 86.97%, Test Loss: 2.0752, Test Acc: 65.66%\n","Epoch 160/200 - Train Loss: 1.4081, Train Acc: 86.11%, Test Loss: 2.0858, Test Acc: 65.40%\n","Epoch 161/200 - Train Loss: 1.3814, Train Acc: 86.60%, Test Loss: 2.1127, Test Acc: 65.59%\n","Epoch 162/200 - Train Loss: 1.3509, Train Acc: 87.48%, Test Loss: 2.0918, Test Acc: 65.81%\n","Epoch 163/200 - Train Loss: 1.3752, Train Acc: 86.76%, Test Loss: 2.1165, Test Acc: 65.23%\n","Epoch 164/200 - Train Loss: 1.3836, Train Acc: 86.62%, Test Loss: 2.1216, Test Acc: 65.81%\n","Epoch 165/200 - Train Loss: 1.3958, Train Acc: 86.26%, Test Loss: 2.0931, Test Acc: 65.94%\n","Epoch 166/200 - Train Loss: 1.3966, Train Acc: 86.45%, Test Loss: 2.0985, Test Acc: 65.63%\n","Epoch 167/200 - Train Loss: 1.3460, Train Acc: 87.46%, Test Loss: 2.1037, Test Acc: 65.81%\n","Epoch 168/200 - Train Loss: 1.3555, Train Acc: 87.41%, Test Loss: 2.1012, Test Acc: 65.87%\n","Epoch 169/200 - Train Loss: 1.3769, Train Acc: 86.58%, Test Loss: 2.1021, Test Acc: 65.80%\n","Epoch 170/200 - Train Loss: 1.3968, Train Acc: 86.01%, Test Loss: 2.0871, Test Acc: 66.14%\n","Epoch 171/200 - Train Loss: 1.3944, Train Acc: 86.26%, Test Loss: 2.0965, Test Acc: 65.82%\n","Epoch 172/200 - Train Loss: 1.4116, Train Acc: 85.85%, Test Loss: 2.0952, Test Acc: 66.03%\n","Epoch 173/200 - Train Loss: 1.3845, Train Acc: 86.77%, Test Loss: 2.0973, Test Acc: 66.02%\n","Epoch 174/200 - Train Loss: 1.4094, Train Acc: 86.09%, Test Loss: 2.0840, Test Acc: 65.93%\n","Epoch 175/200 - Train Loss: 1.3450, Train Acc: 87.56%, Test Loss: 2.0875, Test Acc: 65.86%\n","Epoch 176/200 - Train Loss: 1.3766, Train Acc: 86.53%, Test Loss: 2.0917, Test Acc: 66.16%\n","Epoch 177/200 - Train Loss: 1.3773, Train Acc: 86.94%, Test Loss: 2.0847, Test Acc: 66.21%\n","Epoch 178/200 - Train Loss: 1.3749, Train Acc: 86.93%, Test Loss: 2.0957, Test Acc: 66.08%\n","Epoch 179/200 - Train Loss: 1.4069, Train Acc: 85.82%, Test Loss: 2.0803, Test Acc: 66.01%\n","Epoch 180/200 - Train Loss: 1.3361, Train Acc: 87.88%, Test Loss: 2.0896, Test Acc: 66.09%\n","Epoch 181/200 - Train Loss: 1.3934, Train Acc: 86.21%, Test Loss: 2.0825, Test Acc: 66.22%\n","Epoch 182/200 - Train Loss: 1.4382, Train Acc: 85.36%, Test Loss: 2.0743, Test Acc: 66.12%\n","Epoch 183/200 - Train Loss: 1.4016, Train Acc: 85.93%, Test Loss: 2.0901, Test Acc: 66.11%\n","Epoch 184/200 - Train Loss: 1.4047, Train Acc: 86.07%, Test Loss: 2.0713, Test Acc: 65.82%\n","Epoch 185/200 - Train Loss: 1.3491, Train Acc: 87.32%, Test Loss: 2.0752, Test Acc: 66.28%\n","Epoch 186/200 - Train Loss: 1.3700, Train Acc: 87.02%, Test Loss: 2.0865, Test Acc: 66.35%\n","Epoch 187/200 - Train Loss: 1.4251, Train Acc: 85.40%, Test Loss: 2.0857, Test Acc: 66.16%\n","Epoch 188/200 - Train Loss: 1.3471, Train Acc: 87.48%, Test Loss: 2.0916, Test Acc: 66.43%\n","Epoch 189/200 - Train Loss: 1.3178, Train Acc: 88.18%, Test Loss: 2.0884, Test Acc: 66.31%\n","Epoch 190/200 - Train Loss: 1.4421, Train Acc: 85.21%, Test Loss: 2.0865, Test Acc: 66.18%\n","Epoch 191/200 - Train Loss: 1.3348, Train Acc: 87.79%, Test Loss: 2.0893, Test Acc: 66.19%\n","Epoch 192/200 - Train Loss: 1.3246, Train Acc: 88.22%, Test Loss: 2.0795, Test Acc: 65.97%\n","Epoch 193/200 - Train Loss: 1.3169, Train Acc: 88.24%, Test Loss: 2.0818, Test Acc: 66.03%\n","Epoch 194/200 - Train Loss: 1.3707, Train Acc: 86.96%, Test Loss: 2.0837, Test Acc: 66.06%\n","Epoch 195/200 - Train Loss: 1.3446, Train Acc: 87.63%, Test Loss: 2.0850, Test Acc: 66.11%\n","Epoch 196/200 - Train Loss: 1.3056, Train Acc: 88.61%, Test Loss: 2.0945, Test Acc: 66.28%\n","Epoch 197/200 - Train Loss: 1.3453, Train Acc: 87.63%, Test Loss: 2.0884, Test Acc: 66.29%\n","Epoch 198/200 - Train Loss: 1.3737, Train Acc: 86.67%, Test Loss: 2.0872, Test Acc: 66.14%\n","Epoch 199/200 - Train Loss: 1.3431, Train Acc: 87.86%, Test Loss: 2.0840, Test Acc: 66.21%\n","Epoch 200/200 - Train Loss: 1.3276, Train Acc: 87.86%, Test Loss: 2.0916, Test Acc: 66.22%\n","Training completed. Best Test Accuracy: 66.43%\n"]},{"data":{"text/html":["\n","    \u003cstyle\u003e\n","        .wandb-row {\n","            display: flex;\n","            flex-direction: row;\n","            flex-wrap: wrap;\n","            justify-content: flex-start;\n","            width: 100%;\n","        }\n","        .wandb-col {\n","            display: flex;\n","            flex-direction: column;\n","            flex-basis: 100%;\n","            flex: 1;\n","            padding: 10px;\n","        }\n","    \u003c/style\u003e\n","\u003cdiv class=\"wandb-row\"\u003e\u003cdiv class=\"wandb-col\"\u003e\u003ch3\u003eRun history:\u003c/h3\u003e\u003cbr/\u003e\u003ctable class=\"wandb\"\u003e\u003ctr\u003e\u003ctd\u003ebest_test_acc\u003c/td\u003e\u003ctd\u003e▁▂▂▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇██████████████████████\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eepoch\u003c/td\u003e\u003ctd\u003e▁▁▁▁▂▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇████\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003elearning_rate\u003c/td\u003e\u003ctd\u003e▂████▇▇▇▇▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etest_acc\u003c/td\u003e\u003ctd\u003e▁▂▄▅▅▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇████████████████\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etest_loss\u003c/td\u003e\u003ctd\u003e█▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain_acc\u003c/td\u003e\u003ctd\u003e▁▂▂▁▃▄▄▅▅▅▆▆█▇▆▆▇▆▇▇▅▇█▇▇▇▆▇▂█▇▇▇▇▇▇▇█▇▇\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain_loss\u003c/td\u003e\u003ctd\u003e▇▇▆▆▆▆▄▄█▃▃▃▃▃▃▃▃▃▂▃▃▁▃▂▅▃▂▃▂▂▃▂▂▁▂▇▃▂▂▇\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\u003cbr/\u003e\u003c/div\u003e\u003cdiv class=\"wandb-col\"\u003e\u003ch3\u003eRun summary:\u003c/h3\u003e\u003cbr/\u003e\u003ctable class=\"wandb\"\u003e\u003ctr\u003e\u003ctd\u003ebest_test_acc\u003c/td\u003e\u003ctd\u003e66.43\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eepoch\u003c/td\u003e\u003ctd\u003e199\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003elearning_rate\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etest_acc\u003c/td\u003e\u003ctd\u003e66.22\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etest_loss\u003c/td\u003e\u003ctd\u003e2.09159\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain_acc\u003c/td\u003e\u003ctd\u003e88.59522\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain_loss\u003c/td\u003e\u003ctd\u003e1.30057\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\u003cbr/\u003e\u003c/div\u003e\u003c/div\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run \u003cstrong style=\"color:#cdcd00\"\u003egiddy-snowflake-12\u003c/strong\u003e at: \u003ca href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100/runs/mvbnt6es' target=\"_blank\"\u003ehttps://wandb.ai/pratkumar-stony-brook-university/vit-cifar100/runs/mvbnt6es\u003c/a\u003e\u003cbr/\u003e View project at: \u003ca href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100' target=\"_blank\"\u003ehttps://wandb.ai/pratkumar-stony-brook-university/vit-cifar100\u003c/a\u003e\u003cbr/\u003eSynced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: \u003ccode\u003e./wandb/run-20241201_061525-mvbnt6es/logs\u003c/code\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["def main():\n","    # Set random seeds for reproducibility\n","    torch.manual_seed(42)\n","    torch.cuda.manual_seed_all(42)\n","    np.random.seed(42)\n","    random.seed(42)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","    # Initialize wandb\n","    wandb.init(project='vit-cifar100', config={\n","        'model': 'ViT',\n","        'dataset': 'CIFAR-100',\n","        'epochs': 200,\n","        'batch_size': 128,\n","        'learning_rate': 3e-4,\n","        #'weight_decay': 5e-4,\n","        'weight_decay': 0.01,\n","        'image_size': 32,\n","        'patch_size': 1,\n","        #'dim': 256,\n","        'dim':512,\n","        'depth': 8,\n","        'heads': 8,\n","        'mlp_dim': 512 * 4,     # Adjusted to match dim\n","        'dropout': 0.1,\n","        'emb_dropout': 0.1,\n","        'num_classes': 100,\n","        'mixup_alpha': 0.2,     # Added MixUp alpha\n","        'label_smoothing': 0.1  # Added label smoothing\n","    })\n","    config = wandb.config\n","\n","    # Set device\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    # Data transforms for CIFAR-100 with additional augmentations\n","    transform_train = transforms.Compose([\n","        transforms.RandomCrop(32, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","        transforms.RandomRotation(15),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n","    ])\n","\n","    transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n","    ])\n","\n","    # Load CIFAR-100 dataset\n","    train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n","    test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2, pin_memory=True)\n","    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n","\n","    # Initialize model\n","    model = ViT(\n","        image_size=config.image_size,\n","        patch_size=config.patch_size,\n","        num_classes=config.num_classes,\n","        dim=config.dim,\n","        depth=config.depth,\n","        heads=config.heads,\n","        mlp_dim=config.mlp_dim,\n","        dropout=config.dropout,\n","        emb_dropout=config.emb_dropout,\n","        channels=3,\n","        dim_head=64\n","    ).to(device)\n","\n","    # Define Loss Function with Label Smoothing\n","    criterion = nn.CrossEntropyLoss(label_smoothing=config.label_smoothing)\n","\n","    # Optimizer with parameter-wise weight decay (no decay for bias and norm parameters)\n","    no_decay = ['bias', 'LayerNorm.weight']\n","    optimizer_grouped_parameters = [\n","        {\n","            'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            'weight_decay': config.weight_decay\n","        },\n","        {\n","            'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","            'weight_decay': 0.0\n","        }\n","    ]\n","    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=config.learning_rate)\n","\n","    # Learning rate scheduler with cosine annealing and warmup\n","    total_steps = config.epochs * len(train_loader)\n","    warmup_steps = int(0.1 * total_steps)  # 10% of total steps for warmup\n","\n","    def lr_lambda(current_step):\n","        if current_step \u003c warmup_steps:\n","            return float(current_step) / float(max(1, warmup_steps))\n","        return 0.5 * (1. + math.cos(math.pi * (current_step - warmup_steps) / (total_steps - warmup_steps)))\n","\n","    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n","\n","    # Training loop with Early Stopping\n","    best_acc = 0.0\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    patience = 20  # Number of epochs to wait for improvement\n","    trigger_times = 0\n","\n","    for epoch in range(config.epochs):\n","        model.train()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        for batch_idx, (inputs, targets) in enumerate(train_loader):\n","            # Apply MixUp\n","            inputs, targets_a, targets_b, lam = mixup_data(inputs, targets, alpha=config.mixup_alpha)\n","            inputs, targets_a, targets_b = map(lambda x: x.to(device), (inputs, targets_a, targets_b))\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n","            loss.backward()\n","\n","            # Gradient clipping\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","            optimizer.step()\n","            scheduler.step()\n","\n","            running_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            # Approximate correct predictions with MixUp\n","            correct += (lam * predicted.eq(targets_a).sum().item() + (1 - lam) * predicted.eq(targets_b).sum().item())\n","\n","            if batch_idx % 100 == 0:\n","                wandb.log({\n","                    'train_loss': running_loss / (batch_idx + 1),\n","                    'train_acc': 100. * correct / total,\n","                    'learning_rate': optimizer.param_groups[0]['lr']\n","                })\n","\n","        train_loss = running_loss / len(train_loader)\n","        train_acc = 100. * correct / total\n","\n","        # Validation Phase\n","        model.eval()\n","        test_loss = 0.0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, targets in test_loader:\n","                inputs, targets = inputs.to(device), targets.to(device)\n","                outputs = model(inputs)\n","                loss = criterion(outputs, targets)\n","\n","                test_loss += loss.item()\n","                _, predicted = outputs.max(1)\n","                total += targets.size(0)\n","                correct += predicted.eq(targets).sum().item()\n","\n","        acc = 100. * correct / total\n","        avg_test_loss = test_loss / len(test_loader)\n","        wandb.log({\n","            'test_loss': avg_test_loss,\n","            'test_acc': acc,\n","            'epoch': epoch\n","        })\n","\n","        # Early Stopping Check\n","        if acc \u003e best_acc:\n","            best_acc = acc\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","            trigger_times = 0\n","            # Save the best model\n","            torch.save(model.state_dict(), 'best_cifar100_vit.pth')\n","        else:\n","            trigger_times += 1\n","            if trigger_times \u003e= patience:\n","                print(\"Early stopping triggered!\")\n","                break\n","\n","        print(f\"Epoch {epoch + 1}/{config.epochs} - \"\n","              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n","              f\"Test Loss: {avg_test_loss:.4f}, Test Acc: {acc:.2f}%\")\n","\n","        # Log additional hyperparameters and metrics at the end of each epoch\n","        wandb.log({\n","            'epoch': epoch,\n","            'best_test_acc': best_acc\n","        })\n","\n","    # Load best model weights\n","    model.load_state_dict(best_model_wts)\n","    print(f\"Training completed. Best Test Accuracy: {best_acc:.2f}%\")\n","    wandb.finish()\n","\n","if __name__ == '__main__':\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EZfDa_oQ8plO"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyO5eVt7sc8SVqM3wsfEtLtp","gpuType":"L4","machine_shape":"hm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}