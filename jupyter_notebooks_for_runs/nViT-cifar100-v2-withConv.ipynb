{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyMlfimIcQjvkjl8sk4j1Fm/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0RfqWo9MXpW_"},"outputs":[],"source":["import torch\n","from torch import nn, optim\n","from torch.utils.data import DataLoader\n","import torchvision\n","from torchvision import datasets, transforms\n","import wandb\n","import os\n","import numpy as np\n","import torch.nn.functional as F\n","from einops import rearrange\n","from einops.layers.torch import Rearrange\n","import torch.nn.utils.parametrize as parametrize\n","import math\n","import random\n","import copy"]},{"cell_type":"code","source":["# Set random seeds for reproducibility\n","torch.manual_seed(42)\n","torch.cuda.manual_seed_all(42)\n","np.random.seed(42)\n","random.seed(42)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n"],"metadata":{"id":"X10B0EtNXrZI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Helper functions\n","def exists(v):\n","    return v is not None\n","\n","def default(v, d):\n","    return v if exists(v) else d\n","\n","def pair(t):\n","    return t if isinstance(t, tuple) else (t, t)\n","\n","def divisible_by(numer, denom):\n","    return (numer % denom) == 0\n","\n","def l2norm(t, dim=-1):\n","    return F.normalize(t, dim=dim, p=2)\n","\n","# For use with parametrize\n","class L2Norm(nn.Module):\n","    def __init__(self, dim=-1):\n","        super().__init__()\n","        self.dim = dim\n","\n","    def forward(self, t):\n","        return l2norm(t, dim=self.dim)\n","\n","class NormLinear(nn.Module):\n","    def __init__(self, dim, dim_out, norm_dim_in=True):\n","        super().__init__()\n","        self.linear = nn.Linear(dim, dim_out, bias=False)\n","\n","        parametrize.register_parametrization(\n","            self.linear,\n","            'weight',\n","            L2Norm(dim=-1 if norm_dim_in else 0)\n","        )\n","\n","    @property\n","    def weight(self):\n","        return self.linear.weight\n","\n","    def forward(self, x):\n","        return self.linear(x)"],"metadata":{"id":"eQDgwRNhRxK_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define MixUp Function\n","def mixup_data(x, y, alpha=0.2):\n","    '''Returns mixed inputs, pairs of targets, and lambda'''\n","    if alpha > 0:\n","        lam = np.random.beta(alpha, alpha)\n","    else:\n","        lam = 1\n","    batch_size = x.size()[0]\n","    index = torch.randperm(batch_size).to(x.device)\n","\n","    mixed_x = lam * x + (1 - lam) * x[index, :]\n","    y_a, y_b = y, y[index]\n","    return mixed_x, y_a, y_b, lam\n","\n","# Define MixUp Criterion\n","def mixup_criterion(criterion, pred, y_a, y_b, lam):\n","    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n","\n","# Scaled dot product attention function\n","def scaled_dot_product_attention(q, k, v, dropout_p=0., training=True):\n","    d_k = q.size(-1)\n","    attn_weights = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n","    attn_weights = F.softmax(attn_weights, dim=-1)\n","    if training and dropout_p > 0.0:\n","        attn_weights = F.dropout(attn_weights, p=dropout_p)\n","    output = torch.matmul(attn_weights, v)\n","    return output"],"metadata":{"id":"YcSQuI0ZRxwp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Attention and FeedForward classes\n","class Attention(nn.Module):\n","    def __init__(self, dim, *, dim_head=64, heads=8, dropout=0.):\n","        super().__init__()\n","        dim_inner = dim_head * heads\n","        self.to_q = NormLinear(dim, dim_inner)\n","        self.to_k = NormLinear(dim, dim_inner)\n","        self.to_v = NormLinear(dim, dim_inner)\n","\n","        self.dropout = dropout\n","\n","        self.q_scale = nn.Parameter(torch.ones(heads, 1, dim_head) * (dim_head ** 0.25))\n","        self.k_scale = nn.Parameter(torch.ones(heads, 1, dim_head) * (dim_head ** 0.25))\n","\n","        self.split_heads = Rearrange('b n (h d) -> b h n d', h=heads)\n","        self.merge_heads = Rearrange('b h n d -> b n (h d)')\n","\n","        self.to_out = NormLinear(dim_inner, dim, norm_dim_in=False)\n","\n","    def forward(self, x):\n","        q, k, v = self.to_q(x), self.to_k(x), self.to_v(x)\n","\n","        q, k, v = map(self.split_heads, (q, k, v))\n","\n","        # Query key rmsnorm\n","        q, k = map(l2norm, (q, k))\n","\n","        q = q * self.q_scale\n","        k = k * self.k_scale\n","\n","        out = scaled_dot_product_attention(\n","            q, k, v,\n","            dropout_p=self.dropout,\n","            training=self.training\n","        )\n","\n","        out = self.merge_heads(out)\n","        return self.to_out(out)"],"metadata":{"id":"9GG96KQHR0h_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FeedForward(nn.Module):\n","    def __init__(self, dim, *, dim_inner, dropout=0.):\n","        super().__init__()\n","        dim_inner = int(dim_inner * 2 / 3)\n","\n","        self.dim = dim\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.to_hidden = NormLinear(dim, dim_inner)\n","        self.to_gate = NormLinear(dim, dim_inner)\n","\n","        self.hidden_scale = nn.Parameter(torch.ones(dim_inner))\n","        self.gate_scale = nn.Parameter(torch.ones(dim_inner))\n","\n","        self.to_out = NormLinear(dim_inner, dim, norm_dim_in=False)\n","\n","    def forward(self, x):\n","        hidden, gate = self.to_hidden(x), self.to_gate(x)\n","\n","        hidden = hidden * self.hidden_scale\n","        gate = gate * self.gate_scale * (self.dim ** 0.5)\n","\n","        hidden = F.silu(gate) * hidden\n","\n","        hidden = self.dropout(hidden)\n","        return self.to_out(hidden)"],"metadata":{"id":"MfnlPHy9R3KF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ConvStem Module\n","class ConvStem(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels // 2, kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm2d(out_channels // 2),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels // 2, out_channels, kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.conv(x)"],"metadata":{"id":"4saUy8nLR5dw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Updated nViT with ConvStem and ViT-like configurations\n","class nViT(nn.Module):\n","    def __init__(\n","        self,\n","        *,\n","        image_size,\n","        patch_size,\n","        num_classes,\n","        dim,\n","        depth,\n","        heads,\n","        mlp_dim,\n","        dropout=0.,\n","        emb_dropout=0.,\n","        channels=3,\n","        dim_head=64,\n","        residual_lerp_scale_init=None\n","    ):\n","        super().__init__()\n","        image_height, image_width = pair(image_size)\n","\n","        assert divisible_by(image_height, patch_size) and divisible_by(image_width, patch_size), 'Image dimensions must be divisible by the patch size.'\n","\n","        patch_height_dim, patch_width_dim = (image_height // patch_size), (image_width // patch_size)\n","        patch_dim = channels * (patch_size ** 2)\n","        num_patches = patch_height_dim * patch_width_dim\n","\n","        self.channels = channels\n","        self.patch_size = patch_size\n","        self.num_patches = num_patches\n","        self.image_size = image_size\n","\n","        # ConvStem integration\n","        self.conv_stem = ConvStem(channels, dim)\n","\n","        self.to_patch_embedding = nn.Sequential(\n","            Rearrange('b c h w -> b (h w) c'),\n","            NormLinear(dim, dim, norm_dim_in=False),\n","        )\n","\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n","        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n","        self.emb_dropout = nn.Dropout(emb_dropout)\n","\n","        residual_lerp_scale_init = default(residual_lerp_scale_init, 1. / depth)\n","\n","        self.dim = dim\n","        self.scale = dim ** 0.5\n","\n","        self.layers = nn.ModuleList([])\n","        self.residual_lerp_scales = nn.ModuleList([])\n","\n","        for _ in range(depth):\n","            self.layers.append(nn.ModuleList([\n","                Attention(dim, dim_head=dim_head, heads=heads, dropout=dropout),\n","                FeedForward(dim, dim_inner=mlp_dim, dropout=dropout),\n","            ]))\n","\n","            self.residual_lerp_scales.append(nn.ParameterList([\n","                nn.Parameter(torch.ones(dim) * residual_lerp_scale_init / self.scale),\n","                nn.Parameter(torch.ones(dim) * residual_lerp_scale_init / self.scale),\n","            ]))\n","\n","        # Classification head\n","        self.mlp_head = nn.Sequential(\n","            nn.LayerNorm(dim),\n","            nn.Linear(dim, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        device = x.device\n","\n","        x = self.conv_stem(x)\n","        x = self.to_patch_embedding(x)\n","\n","        B, N, C = x.shape\n","\n","        cls_tokens = self.cls_token.expand(B, -1, -1)\n","        x = torch.cat((cls_tokens, x), dim=1)\n","        x = x + self.pos_embedding[:, :N + 1, :]\n","        x = self.emb_dropout(x)\n","\n","        for (attn, ff), residual_scales in zip(self.layers, self.residual_lerp_scales):\n","            attn_alpha, ff_alpha = residual_scales\n","\n","            attn_out = l2norm(attn(x))\n","            x = l2norm(x.lerp(attn_out, attn_alpha * self.scale))\n","\n","            ff_out = l2norm(ff(x))\n","            x = l2norm(x.lerp(ff_out, ff_alpha * self.scale))\n","\n","        # Classification token\n","        tokens = x[:, 0]\n","        logits = self.mlp_head(tokens)\n","        return logits"],"metadata":{"id":"SvYhdYipR8sQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main():\n","    # Initialize wandb\n","    wandb.init(project='nvit-cifar100-v2-withConv', config={\n","        'model': 'nViT',\n","        'dataset': 'CIFAR-100',\n","        'epochs': 200,  # Increased to match ViT\n","        'batch_size': 128,\n","        'learning_rate': 3e-4,\n","        'weight_decay': 0.01,  # Updated to match ViT\n","        'image_size': 32,\n","        'patch_size': 1,  # Updated to match ViT\n","        'dim': 512,        # Updated to match ViT\n","        'depth': 8,\n","        'heads': 8,\n","        'mlp_dim': 512 * 4,  # Updated to match ViT\n","        'dropout': 0.1,\n","        'emb_dropout': 0.1,\n","        'num_classes': 100,\n","        'dim_head': 64,\n","        'mixup_alpha': 0.2,     # Added MixUp alpha\n","        'label_smoothing': 0.1  # Added label smoothing\n","    })\n","    config = wandb.config\n","\n","    # Set device\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    # Data transforms with advanced augmentations\n","    transform_train = transforms.Compose([\n","        transforms.RandomCrop(config.image_size, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","        transforms.RandomRotation(15),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n","    ])\n","\n","    transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n","    ])\n","\n","    # Load CIFAR-100 dataset\n","    train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n","    test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n","    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n","\n","    # Initialize model\n","    model = nViT(\n","        image_size=config.image_size,\n","        patch_size=config.patch_size,\n","        num_classes=config.num_classes,\n","        dim=config.dim,\n","        depth=config.depth,\n","        heads=config.heads,\n","        mlp_dim=config.mlp_dim,\n","        dropout=config.dropout,\n","        emb_dropout=config.emb_dropout,\n","        channels=3,\n","        dim_head=config.dim_head\n","    ).to(device)\n","\n","    # Define Loss Function with Label Smoothing\n","    criterion = nn.CrossEntropyLoss(label_smoothing=config.label_smoothing)\n","\n","    # Optimizer with parameter-wise weight decay (no decay for bias and norm parameters)\n","    no_decay = ['bias', 'LayerNorm.weight']\n","    optimizer_grouped_parameters = [\n","        {\n","            'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            'weight_decay': config.weight_decay\n","        },\n","        {\n","            'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","            'weight_decay': 0.0\n","        }\n","    ]\n","    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=config.learning_rate)\n","\n","    # Learning rate scheduler with cosine annealing and warmup\n","    total_steps = config.epochs * len(train_loader)\n","    warmup_steps = int(0.1 * total_steps)  # 10% of total steps for warmup\n","\n","    def lr_lambda(current_step):\n","        if current_step < warmup_steps:\n","            return float(current_step) / float(max(1, warmup_steps))\n","        return 0.5 * (1. + math.cos(math.pi * (current_step - warmup_steps) / (total_steps - warmup_steps)))\n","\n","    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n","\n","    # Training loop with Early Stopping\n","    best_acc = 0.0\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    patience = 20  # Number of epochs to wait for improvement\n","    trigger_times = 0\n","\n","    for epoch in range(config.epochs):\n","        model.train()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        for batch_idx, (inputs, targets) in enumerate(train_loader):\n","            # Apply MixUp\n","            inputs, targets_a, targets_b, lam = mixup_data(inputs, targets, alpha=config.mixup_alpha)\n","            inputs, targets_a, targets_b = map(lambda x: x.to(device), (inputs, targets_a, targets_b))\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n","            loss.backward()\n","\n","            # Gradient clipping\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","            optimizer.step()\n","            scheduler.step()\n","\n","            running_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            # Approximate correct predictions with MixUp\n","            correct += (lam * predicted.eq(targets_a).sum().item() + (1 - lam) * predicted.eq(targets_b).sum().item())\n","\n","            if batch_idx % 100 == 0:\n","                wandb.log({\n","                    'train_loss': running_loss / (batch_idx + 1),\n","                    'train_acc': 100. * correct / total,\n","                    'learning_rate': optimizer.param_groups[0]['lr']\n","                })\n","\n","        train_loss = running_loss / len(train_loader)\n","        train_acc = 100. * correct / total\n","\n","        # Validation Phase\n","        model.eval()\n","        test_loss = 0.0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, targets in test_loader:\n","                inputs, targets = inputs.to(device), targets.to(device)\n","                outputs = model(inputs)\n","                loss = criterion(outputs, targets)\n","\n","                test_loss += loss.item()\n","                _, predicted = outputs.max(1)\n","                total += targets.size(0)\n","                correct += predicted.eq(targets).sum().item()\n","\n","        acc = 100. * correct / total\n","        avg_test_loss = test_loss / len(test_loader)\n","        wandb.log({\n","            'test_loss': avg_test_loss,\n","            'test_acc': acc,\n","            'epoch': epoch\n","        })\n","\n","        # Early Stopping Check\n","        if acc > best_acc:\n","            best_acc = acc\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","            trigger_times = 0\n","            # Save the best model\n","            torch.save(model.state_dict(), 'best_nvit_cifar100.pth')\n","        else:\n","            trigger_times += 1\n","            if trigger_times >= patience:\n","                print(\"Early stopping triggered!\")\n","                break\n","\n","        print(f\"Epoch {epoch + 1}/{config.epochs} - \"\n","              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n","              f\"Test Loss: {avg_test_loss:.4f}, Test Acc: {acc:.2f}%\")\n","\n","        # Log additional hyperparameters and metrics at the end of each epoch\n","        wandb.log({\n","            'epoch': epoch,\n","            'best_test_acc': best_acc\n","        })\n","\n","    # Load best model weights\n","    model.load_state_dict(best_model_wts)\n","    print(f\"Training completed. Best Test Accuracy: {best_acc:.2f}%\")\n","    wandb.finish()\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"jojIy-uSSAEc","executionInfo":{"status":"ok","timestamp":1733469458867,"user_tz":300,"elapsed":9163057,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"}},"outputId":"e9cbc257-a433-4848-9579-9ad18391fff7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":[" ··········\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.18.7"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20241206_044505-xfbkrp9e</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100/runs/xfbkrp9e' target=\"_blank\">lilac-sun-8</a></strong> to <a href='https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100/runs/xfbkrp9e' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100/runs/xfbkrp9e</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 169M/169M [00:03<00:00, 42.9MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-100-python.tar.gz to ./data\n","Files already downloaded and verified\n","Epoch 1/200 - Train Loss: 4.5049, Train Acc: 3.02%, Test Loss: 4.1999, Test Acc: 7.30%\n","Epoch 2/200 - Train Loss: 4.1677, Train Acc: 8.03%, Test Loss: 3.8610, Test Acc: 13.30%\n","Epoch 3/200 - Train Loss: 3.9792, Train Acc: 11.77%, Test Loss: 3.6667, Test Acc: 17.00%\n","Epoch 4/200 - Train Loss: 3.8451, Train Acc: 14.54%, Test Loss: 3.4901, Test Acc: 20.69%\n","Epoch 5/200 - Train Loss: 3.7037, Train Acc: 17.67%, Test Loss: 3.3235, Test Acc: 24.80%\n","Epoch 6/200 - Train Loss: 3.6161, Train Acc: 19.91%, Test Loss: 3.1284, Test Acc: 29.16%\n","Epoch 7/200 - Train Loss: 3.4760, Train Acc: 23.23%, Test Loss: 3.0202, Test Acc: 32.62%\n","Epoch 8/200 - Train Loss: 3.3566, Train Acc: 25.96%, Test Loss: 2.9546, Test Acc: 33.42%\n","Epoch 9/200 - Train Loss: 3.2472, Train Acc: 28.80%, Test Loss: 2.8372, Test Acc: 37.15%\n","Epoch 10/200 - Train Loss: 3.1709, Train Acc: 31.15%, Test Loss: 2.6789, Test Acc: 40.89%\n","Epoch 11/200 - Train Loss: 3.1045, Train Acc: 32.70%, Test Loss: 2.6605, Test Acc: 41.51%\n","Epoch 12/200 - Train Loss: 3.0306, Train Acc: 35.06%, Test Loss: 2.6063, Test Acc: 43.17%\n","Epoch 13/200 - Train Loss: 3.0221, Train Acc: 35.63%, Test Loss: 2.5526, Test Acc: 45.10%\n","Epoch 14/200 - Train Loss: 2.8365, Train Acc: 40.24%, Test Loss: 2.4470, Test Acc: 47.20%\n","Epoch 15/200 - Train Loss: 2.8188, Train Acc: 40.91%, Test Loss: 2.4351, Test Acc: 48.07%\n","Epoch 16/200 - Train Loss: 2.7953, Train Acc: 41.65%, Test Loss: 2.3919, Test Acc: 49.52%\n","Epoch 17/200 - Train Loss: 2.6966, Train Acc: 44.24%, Test Loss: 2.3510, Test Acc: 50.77%\n","Epoch 18/200 - Train Loss: 2.6893, Train Acc: 45.03%, Test Loss: 2.3300, Test Acc: 51.76%\n","Epoch 19/200 - Train Loss: 2.6265, Train Acc: 46.47%, Test Loss: 2.2889, Test Acc: 53.16%\n","Epoch 20/200 - Train Loss: 2.5549, Train Acc: 48.73%, Test Loss: 2.3027, Test Acc: 52.07%\n","Epoch 21/200 - Train Loss: 2.5684, Train Acc: 48.85%, Test Loss: 2.3184, Test Acc: 51.90%\n","Epoch 22/200 - Train Loss: 2.4924, Train Acc: 51.32%, Test Loss: 2.2918, Test Acc: 53.62%\n","Epoch 23/200 - Train Loss: 2.4212, Train Acc: 53.45%, Test Loss: 2.2458, Test Acc: 54.62%\n","Epoch 24/200 - Train Loss: 2.3330, Train Acc: 56.04%, Test Loss: 2.2852, Test Acc: 54.02%\n","Epoch 25/200 - Train Loss: 2.2290, Train Acc: 59.15%, Test Loss: 2.2366, Test Acc: 56.13%\n","Epoch 26/200 - Train Loss: 2.2104, Train Acc: 60.15%, Test Loss: 2.2639, Test Acc: 55.31%\n","Epoch 27/200 - Train Loss: 2.1367, Train Acc: 62.60%, Test Loss: 2.2812, Test Acc: 55.84%\n","Epoch 28/200 - Train Loss: 2.1299, Train Acc: 63.17%, Test Loss: 2.2967, Test Acc: 55.48%\n","Epoch 29/200 - Train Loss: 1.9997, Train Acc: 67.25%, Test Loss: 2.2851, Test Acc: 55.81%\n","Epoch 30/200 - Train Loss: 1.9804, Train Acc: 67.85%, Test Loss: 2.2710, Test Acc: 56.84%\n","Epoch 31/200 - Train Loss: 2.0138, Train Acc: 67.67%, Test Loss: 2.2908, Test Acc: 56.92%\n","Epoch 32/200 - Train Loss: 1.9851, Train Acc: 69.06%, Test Loss: 2.3036, Test Acc: 56.94%\n","Epoch 33/200 - Train Loss: 1.9136, Train Acc: 70.85%, Test Loss: 2.2947, Test Acc: 56.39%\n","Epoch 34/200 - Train Loss: 1.8449, Train Acc: 72.86%, Test Loss: 2.3020, Test Acc: 56.72%\n","Epoch 35/200 - Train Loss: 1.7823, Train Acc: 74.90%, Test Loss: 2.3052, Test Acc: 56.76%\n","Epoch 36/200 - Train Loss: 1.8453, Train Acc: 73.61%, Test Loss: 2.3117, Test Acc: 56.58%\n","Epoch 37/200 - Train Loss: 1.8172, Train Acc: 74.76%, Test Loss: 2.3183, Test Acc: 56.58%\n","Epoch 38/200 - Train Loss: 1.7487, Train Acc: 76.27%, Test Loss: 2.3172, Test Acc: 57.24%\n","Epoch 39/200 - Train Loss: 1.6183, Train Acc: 79.81%, Test Loss: 2.2968, Test Acc: 58.25%\n","Epoch 40/200 - Train Loss: 1.7794, Train Acc: 76.01%, Test Loss: 2.2945, Test Acc: 57.83%\n","Epoch 41/200 - Train Loss: 1.7213, Train Acc: 77.48%, Test Loss: 2.3106, Test Acc: 57.39%\n","Epoch 42/200 - Train Loss: 1.7320, Train Acc: 77.34%, Test Loss: 2.3259, Test Acc: 56.69%\n","Epoch 43/200 - Train Loss: 1.7623, Train Acc: 76.70%, Test Loss: 2.3148, Test Acc: 57.68%\n","Epoch 44/200 - Train Loss: 1.6620, Train Acc: 79.26%, Test Loss: 2.3003, Test Acc: 58.34%\n","Epoch 45/200 - Train Loss: 1.6235, Train Acc: 80.58%, Test Loss: 2.2762, Test Acc: 58.28%\n","Epoch 46/200 - Train Loss: 1.7617, Train Acc: 76.94%, Test Loss: 2.2769, Test Acc: 58.79%\n","Epoch 47/200 - Train Loss: 1.6041, Train Acc: 80.80%, Test Loss: 2.3118, Test Acc: 57.82%\n","Epoch 48/200 - Train Loss: 1.6646, Train Acc: 79.67%, Test Loss: 2.2843, Test Acc: 58.22%\n","Epoch 49/200 - Train Loss: 1.5067, Train Acc: 83.57%, Test Loss: 2.3107, Test Acc: 58.92%\n","Epoch 50/200 - Train Loss: 1.5826, Train Acc: 81.76%, Test Loss: 2.3166, Test Acc: 58.06%\n","Epoch 51/200 - Train Loss: 1.6520, Train Acc: 79.81%, Test Loss: 2.2907, Test Acc: 58.42%\n","Epoch 52/200 - Train Loss: 1.6830, Train Acc: 78.93%, Test Loss: 2.3290, Test Acc: 57.71%\n","Epoch 53/200 - Train Loss: 1.6188, Train Acc: 80.86%, Test Loss: 2.3155, Test Acc: 58.61%\n","Epoch 54/200 - Train Loss: 1.5608, Train Acc: 82.03%, Test Loss: 2.3079, Test Acc: 58.63%\n","Epoch 55/200 - Train Loss: 1.6136, Train Acc: 80.74%, Test Loss: 2.3098, Test Acc: 58.32%\n","Epoch 56/200 - Train Loss: 1.5729, Train Acc: 81.85%, Test Loss: 2.3503, Test Acc: 57.82%\n","Epoch 57/200 - Train Loss: 1.5745, Train Acc: 81.61%, Test Loss: 2.2908, Test Acc: 58.43%\n","Epoch 58/200 - Train Loss: 1.4902, Train Acc: 83.58%, Test Loss: 2.3522, Test Acc: 58.68%\n","Epoch 59/200 - Train Loss: 1.4625, Train Acc: 84.45%, Test Loss: 2.4164, Test Acc: 57.23%\n","Epoch 60/200 - Train Loss: 1.5670, Train Acc: 81.80%, Test Loss: 2.3756, Test Acc: 57.90%\n","Epoch 61/200 - Train Loss: 1.5177, Train Acc: 83.08%, Test Loss: 2.3937, Test Acc: 58.12%\n","Epoch 62/200 - Train Loss: 1.5559, Train Acc: 82.02%, Test Loss: 2.3851, Test Acc: 57.92%\n","Epoch 63/200 - Train Loss: 1.5177, Train Acc: 82.98%, Test Loss: 2.4001, Test Acc: 58.51%\n","Epoch 64/200 - Train Loss: 1.4952, Train Acc: 83.45%, Test Loss: 2.4175, Test Acc: 58.21%\n","Epoch 65/200 - Train Loss: 1.5164, Train Acc: 82.84%, Test Loss: 2.3855, Test Acc: 58.51%\n","Epoch 66/200 - Train Loss: 1.5508, Train Acc: 81.94%, Test Loss: 2.3581, Test Acc: 58.66%\n","Epoch 67/200 - Train Loss: 1.5169, Train Acc: 82.80%, Test Loss: 2.4690, Test Acc: 57.18%\n","Epoch 68/200 - Train Loss: 1.5169, Train Acc: 83.16%, Test Loss: 2.4186, Test Acc: 57.79%\n","Early stopping triggered!\n","Training completed. Best Test Accuracy: 58.92%\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <style>\n","        .wandb-row {\n","            display: flex;\n","            flex-direction: row;\n","            flex-wrap: wrap;\n","            justify-content: flex-start;\n","            width: 100%;\n","        }\n","        .wandb-col {\n","            display: flex;\n","            flex-direction: column;\n","            flex-basis: 100%;\n","            flex: 1;\n","            padding: 10px;\n","        }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_test_acc</td><td>▁▂▄▄▅▆▆▆▆▇▇▇▇▇██████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>learning_rate</td><td>▁▂▂▃▃▄▄▆▆▆████████████████████▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>test_acc</td><td>▁▂▂▃▃▄▅▅▆▆▇▇▇▇▇█████████████████████████</td></tr><tr><td>test_loss</td><td>█▇▆▅▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂</td></tr><tr><td>train_acc</td><td>▁▁▂▂▂▃▃▃▂▃▃▄▄▃▄▄▅▂▅▅▅▆▆▇▆▇▇▇█▇▇▇▇█▇▃▆▇▇▇</td></tr><tr><td>train_loss</td><td>█▇▇▆▆▆▆▆▅▅▄▃▄▄▄▂▂▃▆▃▃▆▃▁▂▃▂▂▂▁▂▂▃▂▂▂▆▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_test_acc</td><td>58.92</td></tr><tr><td>epoch</td><td>68</td></tr><tr><td>learning_rate</td><td>0.00025</td></tr><tr><td>test_acc</td><td>58.02</td></tr><tr><td>test_loss</td><td>2.43563</td></tr><tr><td>train_acc</td><td>83.91947</td></tr><tr><td>train_loss</td><td>1.47264</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">lilac-sun-8</strong> at: <a href='https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100/runs/xfbkrp9e' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100/runs/xfbkrp9e</a><br/> View project at: <a href='https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20241206_044505-xfbkrp9e/logs</code>"]},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"48GBETvESP8I"},"execution_count":null,"outputs":[]}]}