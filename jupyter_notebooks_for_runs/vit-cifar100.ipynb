{"cells":[{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":395,"status":"ok","timestamp":1732765009715,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"},"user_tz":300},"id":"1v5TrhFJVcfN"},"outputs":[],"source":["import torch\n","from torch import nn, optim\n","from torch.utils.data import DataLoader\n","import torchvision\n","from torchvision import datasets, transforms\n","import wandb\n","import os\n","import numpy as np\n","from einops import rearrange\n","from einops.layers.torch import Rearrange\n","import math\n","import random\n"]},{"cell_type":"code","source":["# Define MixUp Function\n","def mixup_data(x, y, alpha=0.2):\n","    '''Returns mixed inputs, pairs of targets, and lambda'''\n","    if alpha > 0:\n","        lam = np.random.beta(alpha, alpha)\n","    else:\n","        lam = 1\n","    batch_size = x.size()[0]\n","    index = torch.randperm(batch_size).to(x.device)\n","\n","    mixed_x = lam * x + (1 - lam) * x[index, :]\n","    y_a, y_b = y, y[index]\n","    return mixed_x, y_a, y_b, lam\n","\n","# Define MixUp Criterion\n","def mixup_criterion(criterion, pred, y_a, y_b, lam):\n","    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"],"metadata":{"id":"MajMeXFLyYrw","executionInfo":{"status":"ok","timestamp":1732765109991,"user_tz":300,"elapsed":1049,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":1126,"status":"ok","timestamp":1732765050314,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"},"user_tz":300},"id":"cC4-sG8NVikG"},"outputs":[],"source":["class Attention(nn.Module):\n","    def __init__(self, dim, *, dim_head=64, heads=8, dropout=0.0):\n","        super().__init__()\n","        inner_dim = dim_head * heads\n","        self.heads = heads\n","        self.dim_head = dim_head\n","        self.scale = dim_head ** -0.5\n","\n","        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n","        self.attn_drop = nn.Dropout(dropout)\n","        self.proj = nn.Linear(inner_dim, dim)\n","        self.proj_drop = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        B, N, C = x.shape\n","        qkv = self.to_qkv(x)\n","        qkv = qkv.reshape(B, N, 3, self.heads, self.dim_head)\n","        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, heads, N, dim_head)\n","        q, k, v = qkv[0], qkv[1], qkv[2]\n","        q = q * self.scale\n","        attn = torch.matmul(q, k.transpose(-2, -1))\n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","        out = torch.matmul(attn, v)\n","        out = out.transpose(1, 2).reshape(B, N, -1)\n","        out = self.proj(out)\n","        out = self.proj_drop(out)\n","        return out\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, dim, dim_inner, dropout=0.0):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(dim, dim_inner),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(dim_inner, dim),\n","            nn.Dropout(dropout)\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","def pair(t):\n","    return t if isinstance(t, tuple) else (t, t)"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":901,"status":"ok","timestamp":1732765067058,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"},"user_tz":300},"id":"nPDTdlewVtE7"},"outputs":[],"source":["# Modified ViT for CIFAR-100\n","class ViT(nn.Module):\n","    def __init__(\n","        self,\n","        *,\n","        image_size,\n","        patch_size,\n","        num_classes,\n","        dim,\n","        depth,\n","        heads,\n","        mlp_dim,\n","        dropout=0.0,\n","        emb_dropout=0.0,\n","        channels=3,\n","        dim_head=64\n","    ):\n","        super().__init__()\n","        image_height, image_width = pair(image_size)\n","        patch_height, patch_width = pair(patch_size)\n","\n","        assert image_height % patch_height == 0 and image_width % patch_width == 0, \\\n","            'Image dimensions must be divisible by the patch size.'\n","\n","        num_patches = (image_height // patch_height) * (image_width // patch_width)\n","        patch_dim = channels * patch_height * patch_width\n","\n","        self.patch_size = patch_size\n","        self.dim = dim\n","\n","        self.to_patch_embedding = nn.Sequential(\n","            Rearrange('b c (h ph) (w pw) -> b (h w) (ph pw c)', ph=patch_height, pw=patch_width),\n","            nn.Linear(patch_dim, dim)\n","        )\n","\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n","        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n","        self.dropout = nn.Dropout(emb_dropout)\n","\n","        self.transformer = nn.ModuleList([])\n","        for _ in range(depth):\n","            self.transformer.append(nn.ModuleList([\n","                nn.LayerNorm(dim),\n","                Attention(dim, dim_head=dim_head, heads=heads, dropout=dropout),\n","                nn.LayerNorm(dim),\n","                FeedForward(dim, dim_inner=mlp_dim, dropout=dropout)\n","            ]))\n","\n","        self.mlp_head = nn.Sequential(\n","            nn.LayerNorm(dim),\n","            nn.Linear(dim, num_classes)\n","        )\n","\n","    def forward(self, img):\n","        x = self.to_patch_embedding(img)\n","        B, N, _ = x.shape\n","\n","        cls_tokens = self.cls_token.expand(B, -1, -1)\n","        x = torch.cat((cls_tokens, x), dim=1)\n","        x = x + self.pos_embedding[:, :N + 1, :]\n","        x = self.dropout(x)\n","\n","        for norm1, attn, norm2, ff in self.transformer:\n","            x = x + attn(norm1(x))\n","            x = x + ff(norm2(x))\n","\n","        x = x[:, 0]\n","        x = self.mlp_head(x)\n","        return x\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["5c57ec2b37d64aec96ab8226e34e2524","6a08d3ad18d94592a39732a83bd0a46b","d43f17b3ca504b54970eccb65e8998af","2d57ef6cde9842df94d397f67766ba51","a597ce0c135a4c009a4e0910679f563c","af8bec3047bb42feb57f176df9f9ae39","57c983f76c1a4a248a680fd5cb9110dc","7fa5fe8beef54a9dbef970c6c67c0797","0e3cabd188ac49419e3c63af7c77ab67","5cb8f53d82584626998db5e9ff2f3353","27ee69020a2540b1812cb1613c96e111","da5f21de226241e08661ce06c6eae16a","4df776a5411540ceba6b2fbddfff3d40","258014548ce04bcf8658b006d389965f","f8195f41b1a241d0aca6d7f2fd13276b","aca1e81530c04e29b761fd48b7426bdb"]},"id":"Ryfwg-gOVwUQ","executionInfo":{"status":"ok","timestamp":1732783437784,"user_tz":300,"elapsed":18323512,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"}},"outputId":"1a4d194a-db1c-40fc-93c6-8231b4782682"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Finishing last run (ID:5x34hkw3) before initializing another..."]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.012 MB of 0.012 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c57ec2b37d64aec96ab8226e34e2524"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">jolly-serenity-5</strong> at: <a href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100/runs/5x34hkw3' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100/runs/5x34hkw3</a><br/> View project at: <a href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20241128_033811-5x34hkw3/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Successfully finished last run (ID:5x34hkw3). Initializing new run:<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.18.7"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20241128_033834-1wqjsom7</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100/runs/1wqjsom7' target=\"_blank\">stellar-field-6</a></strong> to <a href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100/runs/1wqjsom7' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100/runs/1wqjsom7</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Epoch 1/200 - Train Loss: 4.5775, Train Acc: 2.06%, Test Loss: 4.4002, Test Acc: 3.77%\n","Epoch 2/200 - Train Loss: 4.3902, Train Acc: 4.15%, Test Loss: 4.2550, Test Acc: 5.68%\n","Epoch 3/200 - Train Loss: 4.2412, Train Acc: 6.80%, Test Loss: 3.9857, Test Acc: 10.45%\n","Epoch 4/200 - Train Loss: 4.0786, Train Acc: 10.12%, Test Loss: 3.7612, Test Acc: 15.64%\n","Epoch 5/200 - Train Loss: 3.9211, Train Acc: 13.38%, Test Loss: 3.5925, Test Acc: 18.95%\n","Epoch 6/200 - Train Loss: 3.8285, Train Acc: 15.45%, Test Loss: 3.4326, Test Acc: 23.00%\n","Epoch 7/200 - Train Loss: 3.7285, Train Acc: 18.00%, Test Loss: 3.3353, Test Acc: 25.46%\n","Epoch 8/200 - Train Loss: 3.6466, Train Acc: 19.87%, Test Loss: 3.2844, Test Acc: 26.91%\n","Epoch 9/200 - Train Loss: 3.5785, Train Acc: 21.33%, Test Loss: 3.2138, Test Acc: 28.58%\n","Epoch 10/200 - Train Loss: 3.5342, Train Acc: 22.57%, Test Loss: 3.1294, Test Acc: 30.78%\n","Epoch 11/200 - Train Loss: 3.4854, Train Acc: 23.88%, Test Loss: 3.1086, Test Acc: 31.28%\n","Epoch 12/200 - Train Loss: 3.4428, Train Acc: 24.97%, Test Loss: 3.0370, Test Acc: 32.67%\n","Epoch 13/200 - Train Loss: 3.4407, Train Acc: 25.11%, Test Loss: 3.0435, Test Acc: 32.82%\n","Epoch 14/200 - Train Loss: 3.3184, Train Acc: 27.59%, Test Loss: 2.9118, Test Acc: 36.82%\n","Epoch 15/200 - Train Loss: 3.3018, Train Acc: 28.19%, Test Loss: 2.8953, Test Acc: 36.48%\n","Epoch 16/200 - Train Loss: 3.2810, Train Acc: 28.86%, Test Loss: 2.8801, Test Acc: 37.32%\n","Epoch 17/200 - Train Loss: 3.2036, Train Acc: 30.71%, Test Loss: 2.7796, Test Acc: 39.25%\n","Epoch 18/200 - Train Loss: 3.1992, Train Acc: 31.18%, Test Loss: 2.7921, Test Acc: 39.65%\n","Epoch 19/200 - Train Loss: 3.1474, Train Acc: 32.49%, Test Loss: 2.7391, Test Acc: 40.98%\n","Epoch 20/200 - Train Loss: 3.0979, Train Acc: 33.66%, Test Loss: 2.6981, Test Acc: 41.51%\n","Epoch 21/200 - Train Loss: 3.1100, Train Acc: 33.52%, Test Loss: 2.7559, Test Acc: 40.24%\n","Epoch 22/200 - Train Loss: 3.0624, Train Acc: 35.08%, Test Loss: 2.6134, Test Acc: 44.11%\n","Epoch 23/200 - Train Loss: 3.0139, Train Acc: 36.08%, Test Loss: 2.5977, Test Acc: 44.67%\n","Epoch 24/200 - Train Loss: 2.9480, Train Acc: 37.65%, Test Loss: 2.5870, Test Acc: 45.30%\n","Epoch 25/200 - Train Loss: 2.8713, Train Acc: 39.76%, Test Loss: 2.5347, Test Acc: 46.29%\n","Epoch 26/200 - Train Loss: 2.8585, Train Acc: 40.57%, Test Loss: 2.5606, Test Acc: 45.77%\n","Epoch 27/200 - Train Loss: 2.7954, Train Acc: 42.21%, Test Loss: 2.4803, Test Acc: 48.37%\n","Epoch 28/200 - Train Loss: 2.7929, Train Acc: 42.24%, Test Loss: 2.4389, Test Acc: 49.02%\n","Epoch 29/200 - Train Loss: 2.6800, Train Acc: 45.07%, Test Loss: 2.4470, Test Acc: 49.11%\n","Epoch 30/200 - Train Loss: 2.6592, Train Acc: 46.18%, Test Loss: 2.4161, Test Acc: 49.94%\n","Epoch 31/200 - Train Loss: 2.6728, Train Acc: 45.97%, Test Loss: 2.4037, Test Acc: 50.57%\n","Epoch 32/200 - Train Loss: 2.6319, Train Acc: 47.32%, Test Loss: 2.3862, Test Acc: 50.96%\n","Epoch 33/200 - Train Loss: 2.5620, Train Acc: 48.85%, Test Loss: 2.3896, Test Acc: 50.63%\n","Epoch 34/200 - Train Loss: 2.4885, Train Acc: 51.09%, Test Loss: 2.3921, Test Acc: 51.29%\n","Epoch 35/200 - Train Loss: 2.4116, Train Acc: 53.06%, Test Loss: 2.3701, Test Acc: 51.83%\n","Epoch 36/200 - Train Loss: 2.4462, Train Acc: 52.91%, Test Loss: 2.3713, Test Acc: 52.03%\n","Epoch 37/200 - Train Loss: 2.4017, Train Acc: 54.34%, Test Loss: 2.3883, Test Acc: 51.73%\n","Epoch 38/200 - Train Loss: 2.3173, Train Acc: 56.53%, Test Loss: 2.3726, Test Acc: 52.00%\n","Epoch 39/200 - Train Loss: 2.1939, Train Acc: 59.55%, Test Loss: 2.3444, Test Acc: 53.52%\n","Epoch 40/200 - Train Loss: 2.3171, Train Acc: 57.24%, Test Loss: 2.3213, Test Acc: 54.08%\n","Epoch 41/200 - Train Loss: 2.2324, Train Acc: 59.74%, Test Loss: 2.3770, Test Acc: 52.85%\n","Epoch 42/200 - Train Loss: 2.2241, Train Acc: 60.29%, Test Loss: 2.3804, Test Acc: 53.01%\n","Epoch 43/200 - Train Loss: 2.2330, Train Acc: 60.45%, Test Loss: 2.3628, Test Acc: 53.29%\n","Epoch 44/200 - Train Loss: 2.1162, Train Acc: 63.72%, Test Loss: 2.3808, Test Acc: 53.60%\n","Epoch 45/200 - Train Loss: 2.0569, Train Acc: 65.48%, Test Loss: 2.4045, Test Acc: 53.12%\n","Epoch 46/200 - Train Loss: 2.1701, Train Acc: 63.12%, Test Loss: 2.4042, Test Acc: 52.91%\n","Epoch 47/200 - Train Loss: 2.0059, Train Acc: 67.27%, Test Loss: 2.3950, Test Acc: 52.87%\n","Epoch 48/200 - Train Loss: 2.0454, Train Acc: 66.93%, Test Loss: 2.3794, Test Acc: 53.50%\n","Epoch 49/200 - Train Loss: 1.8795, Train Acc: 71.15%, Test Loss: 2.3974, Test Acc: 53.62%\n","Epoch 50/200 - Train Loss: 1.9339, Train Acc: 70.50%, Test Loss: 2.4181, Test Acc: 53.78%\n","Epoch 51/200 - Train Loss: 1.9874, Train Acc: 69.07%, Test Loss: 2.3838, Test Acc: 53.87%\n","Epoch 52/200 - Train Loss: 2.0016, Train Acc: 68.81%, Test Loss: 2.3841, Test Acc: 53.51%\n","Epoch 53/200 - Train Loss: 1.9386, Train Acc: 70.87%, Test Loss: 2.3688, Test Acc: 54.84%\n","Epoch 54/200 - Train Loss: 1.8601, Train Acc: 72.69%, Test Loss: 2.3927, Test Acc: 54.03%\n","Epoch 55/200 - Train Loss: 1.9102, Train Acc: 71.73%, Test Loss: 2.4094, Test Acc: 53.85%\n","Epoch 56/200 - Train Loss: 1.8633, Train Acc: 73.12%, Test Loss: 2.3765, Test Acc: 54.86%\n","Epoch 57/200 - Train Loss: 1.8483, Train Acc: 73.74%, Test Loss: 2.4132, Test Acc: 54.39%\n","Epoch 58/200 - Train Loss: 1.7600, Train Acc: 75.79%, Test Loss: 2.3981, Test Acc: 54.29%\n","Epoch 59/200 - Train Loss: 1.7176, Train Acc: 77.25%, Test Loss: 2.3990, Test Acc: 54.43%\n","Epoch 60/200 - Train Loss: 1.8223, Train Acc: 74.67%, Test Loss: 2.3929, Test Acc: 54.72%\n","Epoch 61/200 - Train Loss: 1.7662, Train Acc: 76.18%, Test Loss: 2.3757, Test Acc: 54.63%\n","Epoch 62/200 - Train Loss: 1.7910, Train Acc: 75.58%, Test Loss: 2.3513, Test Acc: 55.22%\n","Epoch 63/200 - Train Loss: 1.7446, Train Acc: 76.97%, Test Loss: 2.3809, Test Acc: 54.87%\n","Epoch 64/200 - Train Loss: 1.7106, Train Acc: 77.88%, Test Loss: 2.3732, Test Acc: 54.94%\n","Epoch 65/200 - Train Loss: 1.7263, Train Acc: 77.29%, Test Loss: 2.4050, Test Acc: 55.11%\n","Epoch 66/200 - Train Loss: 1.7596, Train Acc: 76.64%, Test Loss: 2.3766, Test Acc: 54.62%\n","Epoch 67/200 - Train Loss: 1.7085, Train Acc: 77.92%, Test Loss: 2.3864, Test Acc: 55.22%\n","Epoch 68/200 - Train Loss: 1.7148, Train Acc: 78.01%, Test Loss: 2.4069, Test Acc: 54.35%\n","Epoch 69/200 - Train Loss: 1.6639, Train Acc: 79.02%, Test Loss: 2.3796, Test Acc: 54.54%\n","Epoch 70/200 - Train Loss: 1.6139, Train Acc: 80.35%, Test Loss: 2.3796, Test Acc: 55.32%\n","Epoch 71/200 - Train Loss: 1.6816, Train Acc: 78.79%, Test Loss: 2.3989, Test Acc: 54.41%\n","Epoch 72/200 - Train Loss: 1.6735, Train Acc: 78.97%, Test Loss: 2.3633, Test Acc: 55.39%\n","Epoch 73/200 - Train Loss: 1.6664, Train Acc: 79.21%, Test Loss: 2.3912, Test Acc: 55.04%\n","Epoch 74/200 - Train Loss: 1.6351, Train Acc: 80.13%, Test Loss: 2.3529, Test Acc: 55.38%\n","Epoch 75/200 - Train Loss: 1.6961, Train Acc: 78.54%, Test Loss: 2.4180, Test Acc: 54.47%\n","Epoch 76/200 - Train Loss: 1.6554, Train Acc: 79.31%, Test Loss: 2.3545, Test Acc: 55.48%\n","Epoch 77/200 - Train Loss: 1.6541, Train Acc: 79.49%, Test Loss: 2.3972, Test Acc: 55.05%\n","Epoch 78/200 - Train Loss: 1.6307, Train Acc: 79.95%, Test Loss: 2.3989, Test Acc: 54.83%\n","Epoch 79/200 - Train Loss: 1.6107, Train Acc: 80.56%, Test Loss: 2.3876, Test Acc: 55.51%\n","Epoch 80/200 - Train Loss: 1.6066, Train Acc: 80.76%, Test Loss: 2.3634, Test Acc: 56.05%\n","Epoch 81/200 - Train Loss: 1.6223, Train Acc: 80.07%, Test Loss: 2.3550, Test Acc: 55.63%\n","Epoch 82/200 - Train Loss: 1.6112, Train Acc: 80.34%, Test Loss: 2.3669, Test Acc: 55.99%\n","Epoch 83/200 - Train Loss: 1.6238, Train Acc: 80.22%, Test Loss: 2.3854, Test Acc: 55.53%\n","Epoch 84/200 - Train Loss: 1.5904, Train Acc: 81.12%, Test Loss: 2.3746, Test Acc: 55.36%\n","Epoch 85/200 - Train Loss: 1.6480, Train Acc: 79.61%, Test Loss: 2.4193, Test Acc: 55.27%\n","Epoch 86/200 - Train Loss: 1.5807, Train Acc: 81.40%, Test Loss: 2.3675, Test Acc: 56.08%\n","Epoch 87/200 - Train Loss: 1.5841, Train Acc: 81.53%, Test Loss: 2.3870, Test Acc: 55.76%\n","Epoch 88/200 - Train Loss: 1.6347, Train Acc: 80.03%, Test Loss: 2.3847, Test Acc: 55.51%\n","Epoch 89/200 - Train Loss: 1.6347, Train Acc: 79.94%, Test Loss: 2.3863, Test Acc: 55.55%\n","Epoch 90/200 - Train Loss: 1.5493, Train Acc: 82.24%, Test Loss: 2.3803, Test Acc: 55.94%\n","Epoch 91/200 - Train Loss: 1.4781, Train Acc: 83.76%, Test Loss: 2.3612, Test Acc: 56.48%\n","Epoch 92/200 - Train Loss: 1.6437, Train Acc: 79.55%, Test Loss: 2.3835, Test Acc: 55.90%\n","Epoch 93/200 - Train Loss: 1.5718, Train Acc: 81.61%, Test Loss: 2.4153, Test Acc: 55.89%\n","Epoch 94/200 - Train Loss: 1.5950, Train Acc: 81.01%, Test Loss: 2.4058, Test Acc: 55.79%\n","Epoch 95/200 - Train Loss: 1.5925, Train Acc: 81.22%, Test Loss: 2.4205, Test Acc: 55.92%\n","Epoch 96/200 - Train Loss: 1.6014, Train Acc: 80.84%, Test Loss: 2.3976, Test Acc: 55.69%\n","Epoch 97/200 - Train Loss: 1.5987, Train Acc: 80.96%, Test Loss: 2.4355, Test Acc: 55.43%\n","Epoch 98/200 - Train Loss: 1.6040, Train Acc: 80.90%, Test Loss: 2.4027, Test Acc: 56.03%\n","Epoch 99/200 - Train Loss: 1.6122, Train Acc: 80.65%, Test Loss: 2.3837, Test Acc: 56.16%\n","Epoch 100/200 - Train Loss: 1.5035, Train Acc: 83.44%, Test Loss: 2.4214, Test Acc: 55.96%\n","Epoch 101/200 - Train Loss: 1.4319, Train Acc: 84.93%, Test Loss: 2.4528, Test Acc: 55.57%\n","Epoch 102/200 - Train Loss: 1.5041, Train Acc: 83.38%, Test Loss: 2.4174, Test Acc: 56.26%\n","Epoch 103/200 - Train Loss: 1.5169, Train Acc: 82.94%, Test Loss: 2.3929, Test Acc: 56.52%\n","Epoch 104/200 - Train Loss: 1.5196, Train Acc: 82.86%, Test Loss: 2.4050, Test Acc: 55.83%\n","Epoch 105/200 - Train Loss: 1.4158, Train Acc: 85.67%, Test Loss: 2.4236, Test Acc: 56.14%\n","Epoch 106/200 - Train Loss: 1.5058, Train Acc: 83.17%, Test Loss: 2.4129, Test Acc: 56.23%\n","Epoch 107/200 - Train Loss: 1.5153, Train Acc: 83.11%, Test Loss: 2.4359, Test Acc: 56.22%\n","Epoch 108/200 - Train Loss: 1.5016, Train Acc: 83.31%, Test Loss: 2.4120, Test Acc: 56.31%\n","Epoch 109/200 - Train Loss: 1.4981, Train Acc: 83.35%, Test Loss: 2.4098, Test Acc: 56.08%\n","Epoch 110/200 - Train Loss: 1.4817, Train Acc: 83.96%, Test Loss: 2.4005, Test Acc: 57.00%\n","Epoch 111/200 - Train Loss: 1.4546, Train Acc: 84.68%, Test Loss: 2.4280, Test Acc: 56.22%\n","Epoch 112/200 - Train Loss: 1.4869, Train Acc: 83.54%, Test Loss: 2.4293, Test Acc: 55.68%\n","Epoch 113/200 - Train Loss: 1.5753, Train Acc: 81.57%, Test Loss: 2.3856, Test Acc: 56.40%\n","Epoch 114/200 - Train Loss: 1.4543, Train Acc: 84.59%, Test Loss: 2.4146, Test Acc: 56.07%\n","Epoch 115/200 - Train Loss: 1.3995, Train Acc: 86.08%, Test Loss: 2.3867, Test Acc: 56.63%\n","Epoch 116/200 - Train Loss: 1.3806, Train Acc: 86.51%, Test Loss: 2.4430, Test Acc: 56.26%\n","Epoch 117/200 - Train Loss: 1.3983, Train Acc: 85.97%, Test Loss: 2.4085, Test Acc: 56.31%\n","Epoch 118/200 - Train Loss: 1.5643, Train Acc: 81.83%, Test Loss: 2.4254, Test Acc: 56.30%\n","Epoch 119/200 - Train Loss: 1.5120, Train Acc: 83.06%, Test Loss: 2.4213, Test Acc: 56.69%\n","Epoch 120/200 - Train Loss: 1.4162, Train Acc: 85.81%, Test Loss: 2.4228, Test Acc: 56.17%\n","Epoch 121/200 - Train Loss: 1.4796, Train Acc: 83.87%, Test Loss: 2.4019, Test Acc: 56.66%\n","Epoch 122/200 - Train Loss: 1.5175, Train Acc: 82.77%, Test Loss: 2.4201, Test Acc: 57.11%\n","Epoch 123/200 - Train Loss: 1.4199, Train Acc: 85.64%, Test Loss: 2.4208, Test Acc: 57.02%\n","Epoch 124/200 - Train Loss: 1.4878, Train Acc: 84.00%, Test Loss: 2.4090, Test Acc: 56.72%\n","Epoch 125/200 - Train Loss: 1.4920, Train Acc: 83.66%, Test Loss: 2.4303, Test Acc: 56.59%\n","Epoch 126/200 - Train Loss: 1.5767, Train Acc: 81.52%, Test Loss: 2.4075, Test Acc: 57.15%\n","Epoch 127/200 - Train Loss: 1.4468, Train Acc: 85.04%, Test Loss: 2.3983, Test Acc: 57.18%\n","Epoch 128/200 - Train Loss: 1.4824, Train Acc: 83.97%, Test Loss: 2.4003, Test Acc: 56.97%\n","Epoch 129/200 - Train Loss: 1.4517, Train Acc: 85.03%, Test Loss: 2.3999, Test Acc: 56.96%\n","Epoch 130/200 - Train Loss: 1.4278, Train Acc: 85.54%, Test Loss: 2.4108, Test Acc: 56.97%\n","Epoch 131/200 - Train Loss: 1.4116, Train Acc: 85.65%, Test Loss: 2.4114, Test Acc: 57.33%\n","Epoch 132/200 - Train Loss: 1.5126, Train Acc: 83.14%, Test Loss: 2.3893, Test Acc: 57.33%\n","Epoch 133/200 - Train Loss: 1.4136, Train Acc: 85.42%, Test Loss: 2.4105, Test Acc: 56.98%\n","Epoch 134/200 - Train Loss: 1.4338, Train Acc: 85.18%, Test Loss: 2.3873, Test Acc: 57.57%\n","Epoch 135/200 - Train Loss: 1.5239, Train Acc: 82.76%, Test Loss: 2.3802, Test Acc: 57.73%\n","Epoch 136/200 - Train Loss: 1.4196, Train Acc: 85.74%, Test Loss: 2.4081, Test Acc: 57.12%\n","Epoch 137/200 - Train Loss: 1.3520, Train Acc: 87.42%, Test Loss: 2.4065, Test Acc: 57.25%\n","Epoch 138/200 - Train Loss: 1.4616, Train Acc: 84.54%, Test Loss: 2.4039, Test Acc: 56.76%\n","Epoch 139/200 - Train Loss: 1.4110, Train Acc: 85.61%, Test Loss: 2.3884, Test Acc: 57.42%\n","Epoch 140/200 - Train Loss: 1.3837, Train Acc: 86.35%, Test Loss: 2.4064, Test Acc: 57.32%\n","Epoch 141/200 - Train Loss: 1.4043, Train Acc: 86.13%, Test Loss: 2.4182, Test Acc: 57.10%\n","Epoch 142/200 - Train Loss: 1.3973, Train Acc: 85.96%, Test Loss: 2.4015, Test Acc: 57.28%\n","Epoch 143/200 - Train Loss: 1.4742, Train Acc: 83.98%, Test Loss: 2.3803, Test Acc: 57.83%\n","Epoch 144/200 - Train Loss: 1.4656, Train Acc: 84.60%, Test Loss: 2.4117, Test Acc: 57.03%\n","Epoch 145/200 - Train Loss: 1.4874, Train Acc: 83.84%, Test Loss: 2.3966, Test Acc: 57.41%\n","Epoch 146/200 - Train Loss: 1.4273, Train Acc: 85.42%, Test Loss: 2.4238, Test Acc: 57.15%\n","Epoch 147/200 - Train Loss: 1.4265, Train Acc: 85.41%, Test Loss: 2.4034, Test Acc: 56.96%\n","Epoch 148/200 - Train Loss: 1.4339, Train Acc: 85.11%, Test Loss: 2.4014, Test Acc: 57.56%\n","Epoch 149/200 - Train Loss: 1.3817, Train Acc: 86.55%, Test Loss: 2.4306, Test Acc: 57.22%\n","Epoch 150/200 - Train Loss: 1.4131, Train Acc: 85.88%, Test Loss: 2.4262, Test Acc: 57.08%\n","Epoch 151/200 - Train Loss: 1.4017, Train Acc: 85.94%, Test Loss: 2.3817, Test Acc: 58.03%\n","Epoch 152/200 - Train Loss: 1.4908, Train Acc: 83.80%, Test Loss: 2.3894, Test Acc: 57.41%\n","Epoch 153/200 - Train Loss: 1.4330, Train Acc: 85.63%, Test Loss: 2.3908, Test Acc: 57.69%\n","Epoch 154/200 - Train Loss: 1.4250, Train Acc: 85.61%, Test Loss: 2.3634, Test Acc: 57.89%\n","Epoch 155/200 - Train Loss: 1.3564, Train Acc: 87.19%, Test Loss: 2.3924, Test Acc: 57.44%\n","Epoch 156/200 - Train Loss: 1.3880, Train Acc: 86.43%, Test Loss: 2.3891, Test Acc: 57.90%\n","Epoch 157/200 - Train Loss: 1.4044, Train Acc: 86.11%, Test Loss: 2.3892, Test Acc: 57.71%\n","Epoch 158/200 - Train Loss: 1.4043, Train Acc: 85.89%, Test Loss: 2.3861, Test Acc: 57.81%\n","Epoch 159/200 - Train Loss: 1.4141, Train Acc: 85.90%, Test Loss: 2.3702, Test Acc: 57.99%\n","Epoch 160/200 - Train Loss: 1.4555, Train Acc: 85.05%, Test Loss: 2.3831, Test Acc: 57.98%\n","Epoch 161/200 - Train Loss: 1.4299, Train Acc: 85.41%, Test Loss: 2.3790, Test Acc: 58.21%\n","Epoch 162/200 - Train Loss: 1.3922, Train Acc: 86.48%, Test Loss: 2.3560, Test Acc: 58.27%\n","Epoch 163/200 - Train Loss: 1.4256, Train Acc: 85.48%, Test Loss: 2.3701, Test Acc: 58.11%\n","Epoch 164/200 - Train Loss: 1.4315, Train Acc: 85.48%, Test Loss: 2.3629, Test Acc: 58.09%\n","Epoch 165/200 - Train Loss: 1.4431, Train Acc: 85.05%, Test Loss: 2.3594, Test Acc: 58.47%\n","Epoch 166/200 - Train Loss: 1.4453, Train Acc: 85.30%, Test Loss: 2.3744, Test Acc: 57.87%\n","Epoch 167/200 - Train Loss: 1.3893, Train Acc: 86.42%, Test Loss: 2.3637, Test Acc: 58.30%\n","Epoch 168/200 - Train Loss: 1.4007, Train Acc: 86.27%, Test Loss: 2.3586, Test Acc: 58.35%\n","Epoch 169/200 - Train Loss: 1.4230, Train Acc: 85.51%, Test Loss: 2.3619, Test Acc: 58.23%\n","Epoch 170/200 - Train Loss: 1.4426, Train Acc: 84.95%, Test Loss: 2.3666, Test Acc: 58.25%\n","Epoch 171/200 - Train Loss: 1.4370, Train Acc: 85.29%, Test Loss: 2.3568, Test Acc: 58.47%\n","Epoch 172/200 - Train Loss: 1.4582, Train Acc: 84.67%, Test Loss: 2.3572, Test Acc: 58.38%\n","Epoch 173/200 - Train Loss: 1.4323, Train Acc: 85.56%, Test Loss: 2.3560, Test Acc: 58.34%\n","Epoch 174/200 - Train Loss: 1.4551, Train Acc: 85.01%, Test Loss: 2.3649, Test Acc: 58.29%\n","Epoch 175/200 - Train Loss: 1.3889, Train Acc: 86.56%, Test Loss: 2.3596, Test Acc: 58.45%\n","Epoch 176/200 - Train Loss: 1.4252, Train Acc: 85.36%, Test Loss: 2.3557, Test Acc: 58.38%\n","Epoch 177/200 - Train Loss: 1.4164, Train Acc: 85.97%, Test Loss: 2.3570, Test Acc: 58.24%\n","Epoch 178/200 - Train Loss: 1.4209, Train Acc: 85.79%, Test Loss: 2.3557, Test Acc: 58.30%\n","Epoch 179/200 - Train Loss: 1.4512, Train Acc: 84.78%, Test Loss: 2.3544, Test Acc: 58.19%\n","Epoch 180/200 - Train Loss: 1.3744, Train Acc: 87.14%, Test Loss: 2.3502, Test Acc: 58.21%\n","Epoch 181/200 - Train Loss: 1.4365, Train Acc: 85.23%, Test Loss: 2.3415, Test Acc: 58.41%\n","Epoch 182/200 - Train Loss: 1.4853, Train Acc: 84.18%, Test Loss: 2.3491, Test Acc: 58.50%\n","Epoch 183/200 - Train Loss: 1.4473, Train Acc: 84.82%, Test Loss: 2.3441, Test Acc: 58.48%\n","Epoch 184/200 - Train Loss: 1.4477, Train Acc: 84.97%, Test Loss: 2.3371, Test Acc: 58.77%\n","Epoch 185/200 - Train Loss: 1.3913, Train Acc: 86.36%, Test Loss: 2.3377, Test Acc: 58.66%\n","Epoch 186/200 - Train Loss: 1.4115, Train Acc: 86.00%, Test Loss: 2.3413, Test Acc: 58.51%\n","Epoch 187/200 - Train Loss: 1.4679, Train Acc: 84.29%, Test Loss: 2.3435, Test Acc: 58.58%\n","Epoch 188/200 - Train Loss: 1.3900, Train Acc: 86.45%, Test Loss: 2.3439, Test Acc: 58.65%\n","Epoch 189/200 - Train Loss: 1.3576, Train Acc: 87.26%, Test Loss: 2.3445, Test Acc: 58.67%\n","Epoch 190/200 - Train Loss: 1.4866, Train Acc: 84.23%, Test Loss: 2.3442, Test Acc: 58.58%\n","Epoch 191/200 - Train Loss: 1.3742, Train Acc: 86.85%, Test Loss: 2.3424, Test Acc: 58.56%\n","Epoch 192/200 - Train Loss: 1.3629, Train Acc: 87.30%, Test Loss: 2.3424, Test Acc: 58.69%\n","Epoch 193/200 - Train Loss: 1.3520, Train Acc: 87.45%, Test Loss: 2.3427, Test Acc: 58.60%\n","Epoch 194/200 - Train Loss: 1.4110, Train Acc: 85.87%, Test Loss: 2.3404, Test Acc: 58.65%\n","Epoch 195/200 - Train Loss: 1.3804, Train Acc: 86.83%, Test Loss: 2.3412, Test Acc: 58.66%\n","Epoch 196/200 - Train Loss: 1.3397, Train Acc: 87.83%, Test Loss: 2.3415, Test Acc: 58.62%\n","Epoch 197/200 - Train Loss: 1.3852, Train Acc: 86.73%, Test Loss: 2.3411, Test Acc: 58.60%\n","Epoch 198/200 - Train Loss: 1.4125, Train Acc: 85.75%, Test Loss: 2.3410, Test Acc: 58.60%\n","Epoch 199/200 - Train Loss: 1.3800, Train Acc: 87.00%, Test Loss: 2.3411, Test Acc: 58.62%\n","Epoch 200/200 - Train Loss: 1.3671, Train Acc: 86.93%, Test Loss: 2.3411, Test Acc: 58.62%\n","Training completed. Best Test Accuracy: 58.77%\n"]},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.030 MB of 0.030 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e3cabd188ac49419e3c63af7c77ab67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <style>\n","        .wandb-row {\n","            display: flex;\n","            flex-direction: row;\n","            flex-wrap: wrap;\n","            justify-content: flex-start;\n","            width: 100%;\n","        }\n","        .wandb-col {\n","            display: flex;\n","            flex-direction: column;\n","            flex-basis: 100%;\n","            flex: 1;\n","            padding: 10px;\n","        }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_test_acc</td><td>▁▂▂▂▃▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇███</td></tr><tr><td>learning_rate</td><td>▄▄▄▆███████▇▇▇▇▆▆▆▅▅▅▄▄▄▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁▂▃▄▄▆▆▆▆▇▇▇▇▇██████████████████████████</td></tr><tr><td>test_loss</td><td>█▇▆▅▅▂▂▂▂▁▁▁▂▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▃▄▄▅▅▅▆▆▆▆▇▇█▇▇▇▆█▆▃█▇▆▇▇▇▇█▇▃▇▇█▇▇▇▇</td></tr><tr><td>train_loss</td><td>██▇▇▅▅█▄▄▄▄▃▃▃▃▃▃▃▁▃▃▃▇▃▃▁▃▂▃▂▂▂▁▃▂▂▂▂▇▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_test_acc</td><td>58.77</td></tr><tr><td>epoch</td><td>199</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>test_acc</td><td>58.62</td></tr><tr><td>test_loss</td><td>2.34108</td></tr><tr><td>train_acc</td><td>87.77142</td></tr><tr><td>train_loss</td><td>1.33631</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">stellar-field-6</strong> at: <a href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100/runs/1wqjsom7' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100/runs/1wqjsom7</a><br/> View project at: <a href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/vit-cifar100</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20241128_033834-1wqjsom7/logs</code>"]},"metadata":{}}],"source":["def main():\n","    # Set random seeds for reproducibility\n","    torch.manual_seed(42)\n","    torch.cuda.manual_seed_all(42)\n","    np.random.seed(42)\n","    random.seed(42)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","    # Initialize wandb\n","    wandb.init(project='vit-cifar100', config={\n","        'model': 'ViT',\n","        'dataset': 'CIFAR-100',\n","        'epochs': 200,  # Increased epochs for better convergence\n","        'batch_size': 128,\n","        'learning_rate': 3e-4,\n","        'weight_decay': 5e-2,  # Increased weight decay\n","        'image_size': 32,\n","        'patch_size': 4,\n","        'dim': 384,\n","        'depth': 12,  # Increased depth for better representation\n","        'heads': 6,  # Adjusted number of heads\n","        'mlp_dim': 384 * 4,\n","        'dropout': 0.1,\n","        'emb_dropout': 0.1,\n","        'num_classes': 100,\n","        'mixup_alpha': 0.2,  # Added MixUp alpha\n","        'label_smoothing': 0.1  # Added label smoothing\n","    })\n","    config = wandb.config\n","\n","    # Set device\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    # Data transforms for CIFAR-100 with additional augmentations\n","    transform_train = transforms.Compose([\n","        transforms.RandomCrop(32, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","        transforms.RandomRotation(15),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n","    ])\n","\n","    transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n","    ])\n","\n","    # Load CIFAR-100 dataset\n","    train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n","    test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n","    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n","\n","    # Initialize model\n","    model = ViT(\n","        image_size=config.image_size,\n","        patch_size=config.patch_size,\n","        num_classes=config.num_classes,\n","        dim=config.dim,\n","        depth=config.depth,\n","        heads=config.heads,\n","        mlp_dim=config.mlp_dim,\n","        dropout=config.dropout,\n","        emb_dropout=config.emb_dropout\n","    ).to(device)\n","\n","    # Loss function with label smoothing\n","    criterion = nn.CrossEntropyLoss(label_smoothing=config.label_smoothing)\n","\n","    # Optimizer with parameter-wise weight decay (no decay for bias and norm parameters)\n","    no_decay = ['bias', 'LayerNorm.weight']\n","    optimizer_grouped_parameters = [\n","        {\n","            'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            'weight_decay': config.weight_decay\n","        },\n","        {\n","            'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","            'weight_decay': 0.0\n","        }\n","    ]\n","    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=config.learning_rate)\n","\n","    # Learning rate scheduler with warmup\n","    total_steps = config.epochs * len(train_loader)\n","    warmup_steps = int(0.1 * total_steps)  # 10% of total steps for warmup\n","\n","    def lr_lambda(current_step):\n","        if current_step < warmup_steps:\n","            return float(current_step) / float(max(1, warmup_steps))\n","        return 0.5 * (1. + math.cos(math.pi * (current_step - warmup_steps) / (total_steps - warmup_steps)))\n","\n","    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n","\n","    # Training loop\n","    best_acc = 0.0\n","    for epoch in range(config.epochs):\n","        model.train()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        for batch_idx, (inputs, targets) in enumerate(train_loader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","\n","            # Apply MixUp\n","            inputs, targets_a, targets_b, lam = mixup_data(inputs, targets, alpha=config.mixup_alpha)\n","            inputs, targets_a, targets_b = map(torch.autograd.Variable, (inputs, targets_a, targets_b))\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n","            loss.backward()\n","\n","            # Gradient clipping\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","            optimizer.step()\n","            scheduler.step()\n","\n","            running_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            # For MixUp, accuracy is not straightforward. Use separate metrics or skip accuracy during training.\n","            total += targets.size(0)\n","            # Approximate correct predictions\n","            correct += (lam * predicted.eq(targets_a).sum().item() + (1 - lam) * predicted.eq(targets_b).sum().item())\n","\n","            if batch_idx % 100 == 0:\n","                wandb.log({\n","                    'train_loss': running_loss / (batch_idx + 1),\n","                    'train_acc': 100. * correct / total,\n","                    'learning_rate': optimizer.param_groups[0]['lr']\n","                })\n","\n","        train_acc = 100. * correct / total\n","\n","        # Validation\n","        model.eval()\n","        test_loss = 0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, targets in test_loader:\n","                inputs, targets = inputs.to(device), targets.to(device)\n","                outputs = model(inputs)\n","                loss = criterion(outputs, targets)\n","\n","                test_loss += loss.item()\n","                _, predicted = outputs.max(1)\n","                total += targets.size(0)\n","                correct += predicted.eq(targets).sum().item()\n","\n","        acc = 100. * correct / total\n","        wandb.log({\n","            'test_loss': test_loss / len(test_loader),\n","            'test_acc': acc,\n","            'epoch': epoch\n","        })\n","\n","        # Save best model\n","        if acc > best_acc:\n","            best_acc = acc\n","            torch.save(model.state_dict(), 'best_cifar100_vit.pth')\n","\n","        print(f\"Epoch {epoch + 1}/{config.epochs} - \"\n","              f\"Train Loss: {running_loss / len(train_loader):.4f}, Train Acc: {train_acc:.2f}%, \"\n","              f\"Test Loss: {test_loss / len(test_loader):.4f}, Test Acc: {acc:.2f}%\")\n","\n","        # Log hyperparameters and metrics at the end of each epoch\n","        wandb.log({\n","            'epoch': epoch,\n","            'best_test_acc': best_acc\n","        })\n","\n","    print(f\"Training completed. Best Test Accuracy: {best_acc:.2f}%\")\n","    wandb.finish()\n","\n","if __name__ == '__main__':\n","    main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l8HEHETCWskK"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyOal+bMVWUXnLRfbC4QOgTs"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5c57ec2b37d64aec96ab8226e34e2524":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_6a08d3ad18d94592a39732a83bd0a46b","IPY_MODEL_d43f17b3ca504b54970eccb65e8998af"],"layout":"IPY_MODEL_2d57ef6cde9842df94d397f67766ba51"}},"6a08d3ad18d94592a39732a83bd0a46b":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a597ce0c135a4c009a4e0910679f563c","placeholder":"​","style":"IPY_MODEL_af8bec3047bb42feb57f176df9f9ae39","value":"0.012 MB of 0.012 MB uploaded\r"}},"d43f17b3ca504b54970eccb65e8998af":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_57c983f76c1a4a248a680fd5cb9110dc","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7fa5fe8beef54a9dbef970c6c67c0797","value":1}},"2d57ef6cde9842df94d397f67766ba51":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a597ce0c135a4c009a4e0910679f563c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af8bec3047bb42feb57f176df9f9ae39":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57c983f76c1a4a248a680fd5cb9110dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fa5fe8beef54a9dbef970c6c67c0797":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0e3cabd188ac49419e3c63af7c77ab67":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_5cb8f53d82584626998db5e9ff2f3353","IPY_MODEL_27ee69020a2540b1812cb1613c96e111"],"layout":"IPY_MODEL_da5f21de226241e08661ce06c6eae16a"}},"5cb8f53d82584626998db5e9ff2f3353":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4df776a5411540ceba6b2fbddfff3d40","placeholder":"​","style":"IPY_MODEL_258014548ce04bcf8658b006d389965f","value":"0.030 MB of 0.030 MB uploaded\r"}},"27ee69020a2540b1812cb1613c96e111":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8195f41b1a241d0aca6d7f2fd13276b","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aca1e81530c04e29b761fd48b7426bdb","value":1}},"da5f21de226241e08661ce06c6eae16a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4df776a5411540ceba6b2fbddfff3d40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"258014548ce04bcf8658b006d389965f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f8195f41b1a241d0aca6d7f2fd13276b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aca1e81530c04e29b761fd48b7426bdb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}