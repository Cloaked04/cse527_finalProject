{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyPATy3S4M4J/Zl0cSIbgeod"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qqfaR2OVvfvh"},"outputs":[],"source":["import torch\n","from torch import nn, optim\n","from torch.utils.data import DataLoader\n","import torchvision\n","from torchvision import datasets, transforms\n","import wandb\n","import os\n","import numpy as np\n","from einops import rearrange\n","from einops.layers.torch import Rearrange"]},{"cell_type":"code","source":["# Helper function\n","def pair(t):\n","    return t if isinstance(t, tuple) else (t, t)\n"],"metadata":{"id":"_aJbhSzev0Vg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Attention and FeedForward classes\n","class Attention(nn.Module):\n","    def __init__(self, dim, *, dim_head=64, heads=8, dropout=0.0):\n","        super().__init__()\n","        inner_dim = dim_head * heads\n","        self.heads = heads\n","        self.dim_head = dim_head\n","        self.scale = dim_head ** -0.5\n","\n","        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n","        self.attn_drop = nn.Dropout(dropout)\n","        self.proj = nn.Linear(inner_dim, dim)\n","        self.proj_drop = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        B, N, C = x.shape\n","        qkv = self.to_qkv(x)\n","        qkv = qkv.reshape(B, N, 3, self.heads, self.dim_head)\n","        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, heads, N, dim_head)\n","        q, k, v = qkv[0], qkv[1], qkv[2]\n","        q = q * self.scale\n","        attn = torch.matmul(q, k.transpose(-2, -1))\n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","        out = torch.matmul(attn, v)\n","        out = out.transpose(1, 2).reshape(B, N, -1)\n","        out = self.proj(out)\n","        out = self.proj_drop(out)\n","        return out\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, dim, hidden_dim, dropout=0.0):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_dim, dim),\n","            nn.Dropout(dropout)\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)"],"metadata":{"id":"JjfHtnrBv2ra"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Modified ViT for CIFAR-10\n","class ViT(nn.Module):\n","    def __init__(\n","        self,\n","        *,\n","        image_size,\n","        patch_size,\n","        num_classes,\n","        dim,\n","        depth,\n","        heads,\n","        mlp_dim,\n","        dropout=0.0,\n","        emb_dropout=0.0,\n","        channels=3,\n","        dim_head=64\n","    ):\n","        super().__init__()\n","        image_height, image_width = pair(image_size)\n","        patch_height, patch_width = pair(patch_size)\n","\n","        assert image_height % patch_height == 0 and image_width % patch_width == 0, \\\n","            'Image dimensions must be divisible by the patch size.'\n","\n","        num_patches = (image_height // patch_height) * (image_width // patch_width)\n","        patch_dim = channels * patch_height * patch_width\n","\n","        self.patch_size = patch_size\n","        self.dim = dim\n","\n","        self.to_patch_embedding = nn.Sequential(\n","            Rearrange('b c (h ph) (w pw) -> b (h w) (ph pw c)', ph=patch_height, pw=patch_width),\n","            nn.Linear(patch_dim, dim)\n","        )\n","\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n","        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n","        self.dropout = nn.Dropout(emb_dropout)\n","\n","        self.transformer = nn.ModuleList([])\n","        for _ in range(depth):\n","            self.transformer.append(nn.ModuleList([\n","                nn.LayerNorm(dim),\n","                Attention(dim, dim_head=dim_head, heads=heads, dropout=dropout),\n","                nn.LayerNorm(dim),\n","                FeedForward(dim, hidden_dim=mlp_dim, dropout=dropout)\n","            ]))\n","\n","        self.mlp_head = nn.Sequential(\n","            nn.LayerNorm(dim),\n","            nn.Linear(dim, num_classes)\n","        )\n","\n","    def forward(self, img):\n","        x = self.to_patch_embedding(img)\n","        B, N, _ = x.shape\n","\n","        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, dim)\n","        x = torch.cat((cls_tokens, x), dim=1)  # (B, N+1, dim)\n","        x = x + self.pos_embedding[:, :N + 1, :]\n","        x = self.dropout(x)\n","\n","        for norm1, attn, norm2, ff in self.transformer:\n","            x = x + attn(norm1(x))\n","            x = x + ff(norm2(x))\n","\n","        x = x[:, 0]\n","        x = self.mlp_head(x)\n","        return x"],"metadata":{"id":"bZ1AAFk4v6Vj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main():\n","    # Set random seeds\n","    torch.manual_seed(42)\n","    torch.cuda.manual_seed_all(42)\n","    np.random.seed(42)\n","\n","    # Initialize wandb\n","    wandb.init(project='vit-cifar10-withNorm', config={\n","        'model': 'ViT',\n","        'dataset': 'CIFAR-10',\n","        'epochs': 100,\n","        'batch_size': 128,  # Batch size for CIFAR-10\n","        'learning_rate': 3e-4,\n","        'weight_decay': 1e-4,\n","        'image_size': 32,   # CIFAR-10 image size\n","        'patch_size': 4,    # Patch size for CIFAR-10\n","        'dim': 384,        # Model dimension\n","        'depth': 6,        # Transformer depth\n","        'heads': 6,        # Number of heads\n","        'mlp_dim': 384 * 4,   # MLP hidden dimension\n","        'dropout': 0.1,\n","        'emb_dropout': 0.1,\n","        'num_classes': 10\n","    })\n","    config = wandb.config\n","\n","    # Set device\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    # Data transforms\n","    transform_train = transforms.Compose([\n","        transforms.RandomCrop(config.image_size, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","    ])\n","\n","    transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","    ])\n","\n","    # Load CIFAR-10 dataset\n","    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n","    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n","    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n","\n","    # Initialize model\n","    model = ViT(\n","        image_size=config.image_size,\n","        patch_size=config.patch_size,\n","        num_classes=config.num_classes,\n","        dim=config.dim,\n","        depth=config.depth,\n","        heads=config.heads,\n","        mlp_dim=config.mlp_dim,\n","        dropout=config.dropout,\n","        emb_dropout=config.emb_dropout\n","    ).to(device)\n","\n","    # Loss function and optimizer\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n","    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs)\n","\n","    # Training loop\n","    best_acc = 0.0\n","    for epoch in range(config.epochs):\n","        model.train()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        for batch_idx, (inputs, targets) in enumerate(train_loader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","            if batch_idx % 100 == 0:\n","                wandb.log({\n","                    'train_loss': running_loss / (batch_idx + 1),\n","                    'train_acc': 100. * correct / total,\n","                    'learning_rate': optimizer.param_groups[0]['lr']\n","                })\n","\n","        # Print training progress\n","        train_loss = running_loss / len(train_loader)\n","        train_acc = 100. * correct / total\n","        print(f\"Epoch {epoch + 1}/{config.epochs} - \"\n","              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n","\n","        # Validation\n","        model.eval()\n","        test_loss = 0.0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, targets in test_loader:\n","                inputs, targets = inputs.to(device), targets.to(device)\n","                outputs = model(inputs)\n","                loss = criterion(outputs, targets)\n","\n","                test_loss += loss.item()\n","                _, predicted = outputs.max(1)\n","                total += targets.size(0)\n","                correct += predicted.eq(targets).sum().item()\n","\n","        acc = 100. * correct / total\n","        avg_test_loss = test_loss / len(test_loader)\n","        wandb.log({\n","            'test_loss': avg_test_loss,\n","            'test_acc': acc,\n","            'epoch': epoch\n","        })\n","\n","        print(f\"Test Loss: {avg_test_loss:.4f}, Test Acc: {acc:.2f}%\")\n","\n","        # Save best model\n","        if acc > best_acc:\n","            best_acc = acc\n","            torch.save(model.state_dict(), 'best_cifar10_vit.pth')\n","\n","        scheduler.step()\n","\n","    print(f\"Training completed. Best Test Accuracy: {best_acc:.2f}%\")\n","    wandb.finish()\n","\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Bd5CimGJv-Qh","executionInfo":{"status":"ok","timestamp":1732688105429,"user_tz":300,"elapsed":4202203,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"}},"outputId":"8e2ddec9-3f6d-4a60-b5dc-f663c92cbbbd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpratkumar\u001b[0m (\u001b[33mpratkumar-stony-brook-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.18.7"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20241127_050503-5vg9dk3w</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar10/runs/5vg9dk3w' target=\"_blank\">visionary-lake-3</a></strong> to <a href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar10' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/vit-cifar10</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar10/runs/5vg9dk3w' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/vit-cifar10/runs/5vg9dk3w</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Epoch 1/100 - Train Loss: 1.7570, Train Acc: 35.41%\n","Test Loss: 1.4687, Test Acc: 47.46%\n","Epoch 2/100 - Train Loss: 1.4640, Train Acc: 46.48%\n","Test Loss: 1.3689, Test Acc: 50.27%\n","Epoch 3/100 - Train Loss: 1.3501, Train Acc: 50.79%\n","Test Loss: 1.2188, Test Acc: 56.19%\n","Epoch 4/100 - Train Loss: 1.2712, Train Acc: 53.84%\n","Test Loss: 1.1891, Test Acc: 57.49%\n","Epoch 5/100 - Train Loss: 1.2097, Train Acc: 56.35%\n","Test Loss: 1.1049, Test Acc: 60.29%\n","Epoch 6/100 - Train Loss: 1.1511, Train Acc: 58.66%\n","Test Loss: 1.0892, Test Acc: 61.38%\n","Epoch 7/100 - Train Loss: 1.0962, Train Acc: 60.50%\n","Test Loss: 1.0752, Test Acc: 61.49%\n","Epoch 8/100 - Train Loss: 1.0499, Train Acc: 62.47%\n","Test Loss: 1.0147, Test Acc: 64.07%\n","Epoch 9/100 - Train Loss: 0.9945, Train Acc: 64.54%\n","Test Loss: 0.9518, Test Acc: 66.27%\n","Epoch 10/100 - Train Loss: 0.9595, Train Acc: 65.78%\n","Test Loss: 0.9036, Test Acc: 68.20%\n","Epoch 11/100 - Train Loss: 0.9233, Train Acc: 67.04%\n","Test Loss: 0.9148, Test Acc: 68.01%\n","Epoch 12/100 - Train Loss: 0.8875, Train Acc: 68.55%\n","Test Loss: 0.8565, Test Acc: 69.71%\n","Epoch 13/100 - Train Loss: 0.8554, Train Acc: 69.56%\n","Test Loss: 0.8274, Test Acc: 70.83%\n","Epoch 14/100 - Train Loss: 0.8289, Train Acc: 70.49%\n","Test Loss: 0.8141, Test Acc: 71.46%\n","Epoch 15/100 - Train Loss: 0.8022, Train Acc: 71.63%\n","Test Loss: 0.7993, Test Acc: 72.03%\n","Epoch 16/100 - Train Loss: 0.7697, Train Acc: 72.67%\n","Test Loss: 0.7578, Test Acc: 73.30%\n","Epoch 17/100 - Train Loss: 0.7442, Train Acc: 73.62%\n","Test Loss: 0.7489, Test Acc: 73.35%\n","Epoch 18/100 - Train Loss: 0.7173, Train Acc: 74.52%\n","Test Loss: 0.7048, Test Acc: 75.47%\n","Epoch 19/100 - Train Loss: 0.6924, Train Acc: 75.43%\n","Test Loss: 0.7280, Test Acc: 74.72%\n","Epoch 20/100 - Train Loss: 0.6684, Train Acc: 76.34%\n","Test Loss: 0.6815, Test Acc: 76.27%\n","Epoch 21/100 - Train Loss: 0.6479, Train Acc: 76.97%\n","Test Loss: 0.6724, Test Acc: 76.12%\n","Epoch 22/100 - Train Loss: 0.6202, Train Acc: 77.95%\n","Test Loss: 0.6559, Test Acc: 77.36%\n","Epoch 23/100 - Train Loss: 0.6040, Train Acc: 78.50%\n","Test Loss: 0.6558, Test Acc: 77.61%\n","Epoch 24/100 - Train Loss: 0.5792, Train Acc: 79.27%\n","Test Loss: 0.6472, Test Acc: 78.03%\n","Epoch 25/100 - Train Loss: 0.5584, Train Acc: 80.12%\n","Test Loss: 0.6309, Test Acc: 78.41%\n","Epoch 26/100 - Train Loss: 0.5400, Train Acc: 80.70%\n","Test Loss: 0.6317, Test Acc: 78.69%\n","Epoch 27/100 - Train Loss: 0.5197, Train Acc: 81.37%\n","Test Loss: 0.6183, Test Acc: 79.25%\n","Epoch 28/100 - Train Loss: 0.4994, Train Acc: 82.20%\n","Test Loss: 0.6256, Test Acc: 78.91%\n","Epoch 29/100 - Train Loss: 0.4816, Train Acc: 82.68%\n","Test Loss: 0.5919, Test Acc: 80.01%\n","Epoch 30/100 - Train Loss: 0.4646, Train Acc: 83.33%\n","Test Loss: 0.6005, Test Acc: 80.33%\n","Epoch 31/100 - Train Loss: 0.4469, Train Acc: 84.00%\n","Test Loss: 0.6011, Test Acc: 79.94%\n","Epoch 32/100 - Train Loss: 0.4311, Train Acc: 84.60%\n","Test Loss: 0.5836, Test Acc: 81.06%\n","Epoch 33/100 - Train Loss: 0.4153, Train Acc: 85.02%\n","Test Loss: 0.6419, Test Acc: 79.60%\n","Epoch 34/100 - Train Loss: 0.3975, Train Acc: 85.74%\n","Test Loss: 0.6029, Test Acc: 80.78%\n","Epoch 35/100 - Train Loss: 0.3876, Train Acc: 86.06%\n","Test Loss: 0.6013, Test Acc: 80.77%\n","Epoch 36/100 - Train Loss: 0.3628, Train Acc: 86.88%\n","Test Loss: 0.6021, Test Acc: 81.24%\n","Epoch 37/100 - Train Loss: 0.3498, Train Acc: 87.28%\n","Test Loss: 0.5879, Test Acc: 81.41%\n","Epoch 38/100 - Train Loss: 0.3328, Train Acc: 87.96%\n","Test Loss: 0.6307, Test Acc: 80.32%\n","Epoch 39/100 - Train Loss: 0.3215, Train Acc: 88.50%\n","Test Loss: 0.6068, Test Acc: 81.61%\n","Epoch 40/100 - Train Loss: 0.3066, Train Acc: 89.02%\n","Test Loss: 0.6270, Test Acc: 80.37%\n","Epoch 41/100 - Train Loss: 0.2945, Train Acc: 89.34%\n","Test Loss: 0.6085, Test Acc: 81.56%\n","Epoch 42/100 - Train Loss: 0.2747, Train Acc: 90.06%\n","Test Loss: 0.6032, Test Acc: 81.52%\n","Epoch 43/100 - Train Loss: 0.2644, Train Acc: 90.30%\n","Test Loss: 0.6444, Test Acc: 81.02%\n","Epoch 44/100 - Train Loss: 0.2522, Train Acc: 90.85%\n","Test Loss: 0.6376, Test Acc: 81.21%\n","Epoch 45/100 - Train Loss: 0.2415, Train Acc: 91.31%\n","Test Loss: 0.6581, Test Acc: 81.72%\n","Epoch 46/100 - Train Loss: 0.2241, Train Acc: 91.99%\n","Test Loss: 0.6498, Test Acc: 81.53%\n","Epoch 47/100 - Train Loss: 0.2200, Train Acc: 91.98%\n","Test Loss: 0.6259, Test Acc: 82.09%\n","Epoch 48/100 - Train Loss: 0.2030, Train Acc: 92.71%\n","Test Loss: 0.6901, Test Acc: 80.95%\n","Epoch 49/100 - Train Loss: 0.1953, Train Acc: 93.00%\n","Test Loss: 0.6561, Test Acc: 82.00%\n","Epoch 50/100 - Train Loss: 0.1806, Train Acc: 93.56%\n","Test Loss: 0.6766, Test Acc: 82.02%\n","Epoch 51/100 - Train Loss: 0.1739, Train Acc: 93.66%\n","Test Loss: 0.7129, Test Acc: 81.39%\n","Epoch 52/100 - Train Loss: 0.1608, Train Acc: 94.12%\n","Test Loss: 0.6761, Test Acc: 82.36%\n","Epoch 53/100 - Train Loss: 0.1526, Train Acc: 94.43%\n","Test Loss: 0.6985, Test Acc: 82.46%\n","Epoch 54/100 - Train Loss: 0.1429, Train Acc: 94.77%\n","Test Loss: 0.7240, Test Acc: 81.79%\n","Epoch 55/100 - Train Loss: 0.1407, Train Acc: 94.96%\n","Test Loss: 0.7441, Test Acc: 82.08%\n","Epoch 56/100 - Train Loss: 0.1315, Train Acc: 95.35%\n","Test Loss: 0.7342, Test Acc: 81.81%\n","Epoch 57/100 - Train Loss: 0.1230, Train Acc: 95.61%\n","Test Loss: 0.7634, Test Acc: 82.22%\n","Epoch 58/100 - Train Loss: 0.1193, Train Acc: 95.72%\n","Test Loss: 0.7550, Test Acc: 82.25%\n","Epoch 59/100 - Train Loss: 0.1157, Train Acc: 95.94%\n","Test Loss: 0.7630, Test Acc: 81.74%\n","Epoch 60/100 - Train Loss: 0.1070, Train Acc: 96.15%\n","Test Loss: 0.7472, Test Acc: 82.60%\n","Epoch 61/100 - Train Loss: 0.1024, Train Acc: 96.32%\n","Test Loss: 0.7551, Test Acc: 82.35%\n","Epoch 62/100 - Train Loss: 0.0982, Train Acc: 96.54%\n","Test Loss: 0.7523, Test Acc: 82.85%\n","Epoch 63/100 - Train Loss: 0.0867, Train Acc: 97.00%\n","Test Loss: 0.7837, Test Acc: 82.34%\n","Epoch 64/100 - Train Loss: 0.0871, Train Acc: 96.96%\n","Test Loss: 0.7858, Test Acc: 82.88%\n","Epoch 65/100 - Train Loss: 0.0814, Train Acc: 97.14%\n","Test Loss: 0.8210, Test Acc: 82.42%\n","Epoch 66/100 - Train Loss: 0.0748, Train Acc: 97.34%\n","Test Loss: 0.7875, Test Acc: 83.01%\n","Epoch 67/100 - Train Loss: 0.0741, Train Acc: 97.39%\n","Test Loss: 0.8281, Test Acc: 82.26%\n","Epoch 68/100 - Train Loss: 0.0663, Train Acc: 97.64%\n","Test Loss: 0.8119, Test Acc: 83.14%\n","Epoch 69/100 - Train Loss: 0.0640, Train Acc: 97.73%\n","Test Loss: 0.8056, Test Acc: 83.34%\n","Epoch 70/100 - Train Loss: 0.0613, Train Acc: 97.87%\n","Test Loss: 0.8433, Test Acc: 82.57%\n","Epoch 71/100 - Train Loss: 0.0554, Train Acc: 98.10%\n","Test Loss: 0.8477, Test Acc: 82.83%\n","Epoch 72/100 - Train Loss: 0.0561, Train Acc: 98.09%\n","Test Loss: 0.8429, Test Acc: 83.05%\n","Epoch 73/100 - Train Loss: 0.0511, Train Acc: 98.24%\n","Test Loss: 0.8314, Test Acc: 82.96%\n","Epoch 74/100 - Train Loss: 0.0492, Train Acc: 98.31%\n","Test Loss: 0.8782, Test Acc: 82.64%\n","Epoch 75/100 - Train Loss: 0.0472, Train Acc: 98.39%\n","Test Loss: 0.8590, Test Acc: 82.74%\n","Epoch 76/100 - Train Loss: 0.0441, Train Acc: 98.50%\n","Test Loss: 0.8717, Test Acc: 82.63%\n","Epoch 77/100 - Train Loss: 0.0436, Train Acc: 98.50%\n","Test Loss: 0.8846, Test Acc: 82.91%\n","Epoch 78/100 - Train Loss: 0.0401, Train Acc: 98.59%\n","Test Loss: 0.8853, Test Acc: 83.00%\n","Epoch 79/100 - Train Loss: 0.0406, Train Acc: 98.58%\n","Test Loss: 0.8842, Test Acc: 82.80%\n","Epoch 80/100 - Train Loss: 0.0361, Train Acc: 98.77%\n","Test Loss: 0.8861, Test Acc: 83.15%\n","Epoch 81/100 - Train Loss: 0.0334, Train Acc: 98.86%\n","Test Loss: 0.8963, Test Acc: 83.06%\n","Epoch 82/100 - Train Loss: 0.0328, Train Acc: 98.89%\n","Test Loss: 0.9030, Test Acc: 83.21%\n","Epoch 83/100 - Train Loss: 0.0303, Train Acc: 99.00%\n","Test Loss: 0.8906, Test Acc: 83.09%\n","Epoch 84/100 - Train Loss: 0.0294, Train Acc: 99.01%\n","Test Loss: 0.9094, Test Acc: 83.07%\n","Epoch 85/100 - Train Loss: 0.0292, Train Acc: 99.04%\n","Test Loss: 0.8919, Test Acc: 83.40%\n","Epoch 86/100 - Train Loss: 0.0279, Train Acc: 99.03%\n","Test Loss: 0.8957, Test Acc: 83.27%\n","Epoch 87/100 - Train Loss: 0.0284, Train Acc: 99.06%\n","Test Loss: 0.9111, Test Acc: 83.27%\n","Epoch 88/100 - Train Loss: 0.0251, Train Acc: 99.15%\n","Test Loss: 0.9096, Test Acc: 83.14%\n","Epoch 89/100 - Train Loss: 0.0253, Train Acc: 99.16%\n","Test Loss: 0.9109, Test Acc: 83.33%\n","Epoch 90/100 - Train Loss: 0.0243, Train Acc: 99.23%\n","Test Loss: 0.9034, Test Acc: 83.58%\n","Epoch 91/100 - Train Loss: 0.0232, Train Acc: 99.26%\n","Test Loss: 0.9136, Test Acc: 83.42%\n","Epoch 92/100 - Train Loss: 0.0236, Train Acc: 99.28%\n","Test Loss: 0.9144, Test Acc: 83.37%\n","Epoch 93/100 - Train Loss: 0.0216, Train Acc: 99.26%\n","Test Loss: 0.9139, Test Acc: 83.42%\n","Epoch 94/100 - Train Loss: 0.0229, Train Acc: 99.26%\n","Test Loss: 0.9168, Test Acc: 83.33%\n","Epoch 95/100 - Train Loss: 0.0222, Train Acc: 99.24%\n","Test Loss: 0.9225, Test Acc: 83.34%\n","Epoch 96/100 - Train Loss: 0.0221, Train Acc: 99.26%\n","Test Loss: 0.9190, Test Acc: 83.26%\n","Epoch 97/100 - Train Loss: 0.0221, Train Acc: 99.26%\n","Test Loss: 0.9188, Test Acc: 83.38%\n","Epoch 98/100 - Train Loss: 0.0213, Train Acc: 99.32%\n","Test Loss: 0.9193, Test Acc: 83.36%\n","Epoch 99/100 - Train Loss: 0.0198, Train Acc: 99.36%\n","Test Loss: 0.9191, Test Acc: 83.41%\n","Epoch 100/100 - Train Loss: 0.0215, Train Acc: 99.31%\n","Test Loss: 0.9191, Test Acc: 83.42%\n","Training completed. Best Test Accuracy: 83.58%\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <style>\n","        .wandb-row {\n","            display: flex;\n","            flex-direction: row;\n","            flex-wrap: wrap;\n","            justify-content: flex-start;\n","            width: 100%;\n","        }\n","        .wandb-col {\n","            display: flex;\n","            flex-direction: column;\n","            flex-basis: 100%;\n","            flex: 1;\n","            padding: 10px;\n","        }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>███████▇▇▇▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁▂▃▃▅▅▆▆▆▆▇▇▇▇▇▇████████████████████████</td></tr><tr><td>test_loss</td><td>█▆▆▅▅▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄</td></tr><tr><td>train_acc</td><td>▁▄▄▄▄▅▇▆▆▆▇▇▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>test_acc</td><td>83.42</td></tr><tr><td>test_loss</td><td>0.91909</td></tr><tr><td>train_acc</td><td>99.3044</td></tr><tr><td>train_loss</td><td>0.02152</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">visionary-lake-3</strong> at: <a href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar10/runs/5vg9dk3w' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/vit-cifar10/runs/5vg9dk3w</a><br/> View project at: <a href='https://wandb.ai/pratkumar-stony-brook-university/vit-cifar10' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/vit-cifar10</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20241127_050503-5vg9dk3w/logs</code>"]},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"Y3I5hDyKzMSU"},"execution_count":null,"outputs":[]}]}