{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyMNMz0x7lIqS3O3Dub/ldsc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"bbLknTEWC7L0","executionInfo":{"status":"ok","timestamp":1733728781609,"user_tz":300,"elapsed":1291228,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"}},"outputId":"07e73a64-78c4-4d75-abeb-dd60f102f084"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"]},{"data":{"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["Tracking run with wandb version 0.18.7"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20241209_061317-s6596lgh</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/pratkumar-stony-brook-university/vit-mnist/runs/s6596lgh' target=\"_blank\">fragrant-jazz-1</a></strong> to <a href='https://wandb.ai/pratkumar-stony-brook-university/vit-mnist' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/pratkumar-stony-brook-university/vit-mnist' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/vit-mnist</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/pratkumar-stony-brook-university/vit-mnist/runs/s6596lgh' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/vit-mnist/runs/s6596lgh</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 9.91M/9.91M [00:00<00:00, 17.9MB/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 28.9k/28.9k [00:00<00:00, 491kB/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1.65M/1.65M [00:00<00:00, 4.52MB/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 4.54k/4.54k [00:00<00:00, 4.83MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Epoch 1/100 - Train Loss: 1.1510, Train Acc: 58.29%\n","Test Loss: 0.5082, Test Acc: 81.68%\n","Epoch 2/100 - Train Loss: 0.5113, Train Acc: 82.00%\n","Test Loss: 0.3265, Test Acc: 88.61%\n","Epoch 3/100 - Train Loss: 0.3811, Train Acc: 86.71%\n","Test Loss: 0.2259, Test Acc: 92.25%\n","Epoch 4/100 - Train Loss: 0.3088, Train Acc: 89.47%\n","Test Loss: 0.1972, Test Acc: 93.08%\n","Epoch 5/100 - Train Loss: 0.2551, Train Acc: 91.53%\n","Test Loss: 0.1441, Test Acc: 95.24%\n","Epoch 6/100 - Train Loss: 0.2170, Train Acc: 92.78%\n","Test Loss: 0.1300, Test Acc: 95.43%\n","Epoch 7/100 - Train Loss: 0.1859, Train Acc: 93.97%\n","Test Loss: 0.1040, Test Acc: 96.66%\n","Epoch 8/100 - Train Loss: 0.1642, Train Acc: 94.66%\n","Test Loss: 0.0795, Test Acc: 97.35%\n","Epoch 9/100 - Train Loss: 0.1414, Train Acc: 95.46%\n","Test Loss: 0.0835, Test Acc: 97.16%\n","Epoch 10/100 - Train Loss: 0.1333, Train Acc: 95.72%\n","Test Loss: 0.0680, Test Acc: 97.84%\n","Epoch 11/100 - Train Loss: 0.1220, Train Acc: 96.14%\n","Test Loss: 0.0684, Test Acc: 97.47%\n","Epoch 12/100 - Train Loss: 0.1122, Train Acc: 96.40%\n","Test Loss: 0.0621, Test Acc: 97.93%\n","Epoch 13/100 - Train Loss: 0.1110, Train Acc: 96.47%\n","Test Loss: 0.0666, Test Acc: 97.82%\n","Epoch 14/100 - Train Loss: 0.0980, Train Acc: 96.90%\n","Test Loss: 0.0535, Test Acc: 98.19%\n","Epoch 15/100 - Train Loss: 0.0932, Train Acc: 97.04%\n","Test Loss: 0.0649, Test Acc: 98.04%\n","Epoch 16/100 - Train Loss: 0.0896, Train Acc: 97.16%\n","Test Loss: 0.0533, Test Acc: 98.23%\n","Epoch 17/100 - Train Loss: 0.0870, Train Acc: 97.15%\n","Test Loss: 0.0437, Test Acc: 98.53%\n","Epoch 18/100 - Train Loss: 0.0834, Train Acc: 97.36%\n","Test Loss: 0.0483, Test Acc: 98.40%\n","Epoch 19/100 - Train Loss: 0.0804, Train Acc: 97.46%\n","Test Loss: 0.0590, Test Acc: 98.06%\n","Epoch 20/100 - Train Loss: 0.0822, Train Acc: 97.36%\n","Test Loss: 0.0509, Test Acc: 98.20%\n","Epoch 21/100 - Train Loss: 0.0729, Train Acc: 97.60%\n","Test Loss: 0.0429, Test Acc: 98.57%\n","Epoch 22/100 - Train Loss: 0.0730, Train Acc: 97.67%\n","Test Loss: 0.0516, Test Acc: 98.30%\n","Epoch 23/100 - Train Loss: 0.0696, Train Acc: 97.65%\n","Test Loss: 0.0550, Test Acc: 98.23%\n","Epoch 24/100 - Train Loss: 0.0693, Train Acc: 97.75%\n","Test Loss: 0.0456, Test Acc: 98.56%\n","Epoch 25/100 - Train Loss: 0.0670, Train Acc: 97.81%\n","Test Loss: 0.0460, Test Acc: 98.54%\n","Epoch 26/100 - Train Loss: 0.0646, Train Acc: 97.96%\n","Test Loss: 0.0352, Test Acc: 98.95%\n","Epoch 27/100 - Train Loss: 0.0604, Train Acc: 98.07%\n","Test Loss: 0.0457, Test Acc: 98.46%\n","Epoch 28/100 - Train Loss: 0.0616, Train Acc: 98.02%\n","Test Loss: 0.0403, Test Acc: 98.62%\n","Epoch 29/100 - Train Loss: 0.0597, Train Acc: 98.04%\n","Test Loss: 0.0450, Test Acc: 98.58%\n","Epoch 30/100 - Train Loss: 0.0598, Train Acc: 98.02%\n","Test Loss: 0.0464, Test Acc: 98.40%\n","Epoch 31/100 - Train Loss: 0.0556, Train Acc: 98.16%\n","Test Loss: 0.0400, Test Acc: 98.65%\n","Epoch 32/100 - Train Loss: 0.0524, Train Acc: 98.27%\n","Test Loss: 0.0438, Test Acc: 98.62%\n","Epoch 33/100 - Train Loss: 0.0544, Train Acc: 98.19%\n","Test Loss: 0.0478, Test Acc: 98.42%\n","Epoch 34/100 - Train Loss: 0.0530, Train Acc: 98.20%\n","Test Loss: 0.0372, Test Acc: 98.74%\n","Epoch 35/100 - Train Loss: 0.0497, Train Acc: 98.39%\n","Test Loss: 0.0432, Test Acc: 98.55%\n","Epoch 36/100 - Train Loss: 0.0496, Train Acc: 98.38%\n","Test Loss: 0.0413, Test Acc: 98.65%\n","Epoch 37/100 - Train Loss: 0.0484, Train Acc: 98.44%\n","Test Loss: 0.0370, Test Acc: 98.85%\n","Epoch 38/100 - Train Loss: 0.0465, Train Acc: 98.47%\n","Test Loss: 0.0399, Test Acc: 98.65%\n","Epoch 39/100 - Train Loss: 0.0440, Train Acc: 98.55%\n","Test Loss: 0.0411, Test Acc: 98.71%\n","Epoch 40/100 - Train Loss: 0.0458, Train Acc: 98.48%\n","Test Loss: 0.0368, Test Acc: 98.81%\n","Epoch 41/100 - Train Loss: 0.0436, Train Acc: 98.53%\n","Test Loss: 0.0558, Test Acc: 98.24%\n","Epoch 42/100 - Train Loss: 0.0424, Train Acc: 98.58%\n","Test Loss: 0.0351, Test Acc: 98.83%\n","Epoch 43/100 - Train Loss: 0.0400, Train Acc: 98.67%\n","Test Loss: 0.0340, Test Acc: 98.90%\n","Epoch 44/100 - Train Loss: 0.0389, Train Acc: 98.72%\n","Test Loss: 0.0326, Test Acc: 98.88%\n","Epoch 45/100 - Train Loss: 0.0380, Train Acc: 98.70%\n","Test Loss: 0.0275, Test Acc: 99.13%\n","Epoch 46/100 - Train Loss: 0.0378, Train Acc: 98.72%\n","Test Loss: 0.0322, Test Acc: 99.01%\n","Epoch 47/100 - Train Loss: 0.0378, Train Acc: 98.76%\n","Test Loss: 0.0312, Test Acc: 99.06%\n","Epoch 48/100 - Train Loss: 0.0359, Train Acc: 98.78%\n","Test Loss: 0.0336, Test Acc: 98.91%\n","Epoch 49/100 - Train Loss: 0.0368, Train Acc: 98.80%\n","Test Loss: 0.0319, Test Acc: 98.95%\n","Epoch 50/100 - Train Loss: 0.0344, Train Acc: 98.88%\n","Test Loss: 0.0320, Test Acc: 98.97%\n","Epoch 51/100 - Train Loss: 0.0335, Train Acc: 98.89%\n","Test Loss: 0.0345, Test Acc: 98.93%\n","Epoch 52/100 - Train Loss: 0.0322, Train Acc: 98.89%\n","Test Loss: 0.0343, Test Acc: 98.99%\n","Epoch 53/100 - Train Loss: 0.0311, Train Acc: 98.96%\n","Test Loss: 0.0342, Test Acc: 98.86%\n","Epoch 54/100 - Train Loss: 0.0319, Train Acc: 98.91%\n","Test Loss: 0.0303, Test Acc: 99.02%\n","Epoch 55/100 - Train Loss: 0.0288, Train Acc: 99.02%\n","Test Loss: 0.0311, Test Acc: 99.00%\n","Epoch 56/100 - Train Loss: 0.0290, Train Acc: 99.03%\n","Test Loss: 0.0380, Test Acc: 98.88%\n","Epoch 57/100 - Train Loss: 0.0288, Train Acc: 99.02%\n","Test Loss: 0.0293, Test Acc: 99.05%\n","Epoch 58/100 - Train Loss: 0.0290, Train Acc: 99.01%\n","Test Loss: 0.0291, Test Acc: 98.98%\n","Epoch 59/100 - Train Loss: 0.0283, Train Acc: 99.02%\n","Test Loss: 0.0259, Test Acc: 99.17%\n","Epoch 60/100 - Train Loss: 0.0255, Train Acc: 99.12%\n","Test Loss: 0.0317, Test Acc: 99.05%\n","Epoch 61/100 - Train Loss: 0.0247, Train Acc: 99.14%\n","Test Loss: 0.0281, Test Acc: 99.19%\n","Epoch 62/100 - Train Loss: 0.0245, Train Acc: 99.19%\n","Test Loss: 0.0245, Test Acc: 99.23%\n","Epoch 63/100 - Train Loss: 0.0235, Train Acc: 99.20%\n","Test Loss: 0.0297, Test Acc: 99.02%\n","Epoch 64/100 - Train Loss: 0.0228, Train Acc: 99.22%\n","Test Loss: 0.0288, Test Acc: 99.06%\n","Epoch 65/100 - Train Loss: 0.0217, Train Acc: 99.26%\n","Test Loss: 0.0247, Test Acc: 99.20%\n","Epoch 66/100 - Train Loss: 0.0223, Train Acc: 99.24%\n","Test Loss: 0.0286, Test Acc: 99.02%\n","Epoch 67/100 - Train Loss: 0.0210, Train Acc: 99.31%\n","Test Loss: 0.0276, Test Acc: 99.12%\n","Epoch 68/100 - Train Loss: 0.0202, Train Acc: 99.29%\n","Test Loss: 0.0236, Test Acc: 99.11%\n","Epoch 69/100 - Train Loss: 0.0196, Train Acc: 99.31%\n","Test Loss: 0.0309, Test Acc: 99.01%\n","Epoch 70/100 - Train Loss: 0.0197, Train Acc: 99.32%\n","Test Loss: 0.0245, Test Acc: 99.19%\n","Epoch 71/100 - Train Loss: 0.0182, Train Acc: 99.39%\n","Test Loss: 0.0247, Test Acc: 99.16%\n","Epoch 72/100 - Train Loss: 0.0184, Train Acc: 99.36%\n","Test Loss: 0.0314, Test Acc: 98.99%\n","Epoch 73/100 - Train Loss: 0.0203, Train Acc: 99.31%\n","Test Loss: 0.0238, Test Acc: 99.23%\n","Epoch 74/100 - Train Loss: 0.0169, Train Acc: 99.46%\n","Test Loss: 0.0228, Test Acc: 99.23%\n","Epoch 75/100 - Train Loss: 0.0174, Train Acc: 99.38%\n","Test Loss: 0.0274, Test Acc: 99.13%\n","Epoch 76/100 - Train Loss: 0.0165, Train Acc: 99.45%\n","Test Loss: 0.0259, Test Acc: 99.10%\n","Epoch 77/100 - Train Loss: 0.0156, Train Acc: 99.48%\n","Test Loss: 0.0283, Test Acc: 99.10%\n","Epoch 78/100 - Train Loss: 0.0147, Train Acc: 99.51%\n","Test Loss: 0.0254, Test Acc: 99.14%\n","Epoch 79/100 - Train Loss: 0.0145, Train Acc: 99.49%\n","Test Loss: 0.0257, Test Acc: 99.17%\n","Epoch 80/100 - Train Loss: 0.0142, Train Acc: 99.52%\n","Test Loss: 0.0259, Test Acc: 99.21%\n","Epoch 81/100 - Train Loss: 0.0134, Train Acc: 99.55%\n","Test Loss: 0.0282, Test Acc: 99.20%\n","Epoch 82/100 - Train Loss: 0.0138, Train Acc: 99.53%\n","Test Loss: 0.0247, Test Acc: 99.23%\n","Epoch 83/100 - Train Loss: 0.0139, Train Acc: 99.49%\n","Test Loss: 0.0282, Test Acc: 99.12%\n","Epoch 84/100 - Train Loss: 0.0134, Train Acc: 99.52%\n","Test Loss: 0.0273, Test Acc: 99.11%\n","Epoch 85/100 - Train Loss: 0.0132, Train Acc: 99.56%\n","Test Loss: 0.0257, Test Acc: 99.18%\n","Epoch 86/100 - Train Loss: 0.0130, Train Acc: 99.56%\n","Test Loss: 0.0264, Test Acc: 99.15%\n","Epoch 87/100 - Train Loss: 0.0134, Train Acc: 99.54%\n","Test Loss: 0.0280, Test Acc: 99.12%\n","Epoch 88/100 - Train Loss: 0.0129, Train Acc: 99.57%\n","Test Loss: 0.0270, Test Acc: 99.16%\n","Epoch 89/100 - Train Loss: 0.0130, Train Acc: 99.54%\n","Test Loss: 0.0261, Test Acc: 99.20%\n","Epoch 90/100 - Train Loss: 0.0119, Train Acc: 99.62%\n","Test Loss: 0.0273, Test Acc: 99.17%\n","Epoch 91/100 - Train Loss: 0.0117, Train Acc: 99.60%\n","Test Loss: 0.0256, Test Acc: 99.17%\n","Epoch 92/100 - Train Loss: 0.0128, Train Acc: 99.57%\n","Test Loss: 0.0258, Test Acc: 99.22%\n","Epoch 93/100 - Train Loss: 0.0118, Train Acc: 99.58%\n","Test Loss: 0.0261, Test Acc: 99.20%\n","Epoch 94/100 - Train Loss: 0.0121, Train Acc: 99.59%\n","Test Loss: 0.0267, Test Acc: 99.22%\n","Epoch 95/100 - Train Loss: 0.0120, Train Acc: 99.58%\n","Test Loss: 0.0266, Test Acc: 99.18%\n","Epoch 96/100 - Train Loss: 0.0122, Train Acc: 99.58%\n","Test Loss: 0.0267, Test Acc: 99.15%\n","Epoch 97/100 - Train Loss: 0.0112, Train Acc: 99.60%\n","Test Loss: 0.0273, Test Acc: 99.18%\n","Epoch 98/100 - Train Loss: 0.0115, Train Acc: 99.59%\n","Test Loss: 0.0273, Test Acc: 99.19%\n","Epoch 99/100 - Train Loss: 0.0122, Train Acc: 99.57%\n","Test Loss: 0.0272, Test Acc: 99.19%\n","Epoch 100/100 - Train Loss: 0.0120, Train Acc: 99.59%\n","Test Loss: 0.0272, Test Acc: 99.18%\n","Training completed. Best Test Accuracy: 99.23%\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <style>\n","        .wandb-row {\n","            display: flex;\n","            flex-direction: row;\n","            flex-wrap: wrap;\n","            justify-content: flex-start;\n","            width: 100%;\n","        }\n","        .wandb-col {\n","            display: flex;\n","            flex-direction: column;\n","            flex-basis: 100%;\n","            flex: 1;\n","            padding: 10px;\n","        }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>███████▇▇▆▆▆▆▆▅▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁▄▅▆▇███████████████████████████████████</td></tr><tr><td>test_loss</td><td>█▆▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇██▇████████████████████████████████</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>test_acc</td><td>99.18</td></tr><tr><td>test_loss</td><td>0.02725</td></tr><tr><td>train_acc</td><td>99.59866</td></tr><tr><td>train_loss</td><td>0.01211</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">fragrant-jazz-1</strong> at: <a href='https://wandb.ai/pratkumar-stony-brook-university/vit-mnist/runs/s6596lgh' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/vit-mnist/runs/s6596lgh</a><br/> View project at: <a href='https://wandb.ai/pratkumar-stony-brook-university/vit-mnist' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/vit-mnist</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20241209_061317-s6596lgh/logs</code>"]},"metadata":{}}],"source":["import torch\n","from torch import nn, optim\n","from torch.utils.data import DataLoader\n","import torchvision\n","from torchvision import datasets, transforms\n","import wandb\n","import os\n","import numpy as np\n","from einops import rearrange\n","from einops.layers.torch import Rearrange\n","\n","# Helper function\n","def pair(t):\n","    return t if isinstance(t, tuple) else (t, t)\n","\n","# Attention and FeedForward classes\n","class Attention(nn.Module):\n","    def __init__(self, dim, *, dim_head=64, heads=8, dropout=0.0):\n","        super().__init__()\n","        inner_dim = dim_head * heads\n","        self.heads = heads\n","        self.dim_head = dim_head\n","        self.scale = dim_head ** -0.5\n","\n","        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n","        self.attn_drop = nn.Dropout(dropout)\n","        self.proj = nn.Linear(inner_dim, dim)\n","        self.proj_drop = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        B, N, C = x.shape\n","        qkv = self.to_qkv(x)\n","        qkv = qkv.reshape(B, N, 3, self.heads, self.dim_head)\n","        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, heads, N, dim_head)\n","        q, k, v = qkv[0], qkv[1], qkv[2]\n","        q = q * self.scale\n","        attn = torch.matmul(q, k.transpose(-2, -1))\n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","        out = torch.matmul(attn, v)\n","        out = out.transpose(1, 2).reshape(B, N, -1)\n","        out = self.proj(out)\n","        out = self.proj_drop(out)\n","        return out\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, dim, hidden_dim, dropout=0.0):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_dim, dim),\n","            nn.Dropout(dropout)\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","# Modified ViT for MNIST\n","class ViT(nn.Module):\n","    def __init__(\n","        self,\n","        *,\n","        image_size,\n","        patch_size,\n","        num_classes,\n","        dim,\n","        depth,\n","        heads,\n","        mlp_dim,\n","        dropout=0.0,\n","        emb_dropout=0.0,\n","        channels=1,  # MNIST is grayscale\n","        dim_head=64\n","    ):\n","        super().__init__()\n","        image_height, image_width = pair(image_size)\n","        patch_height, patch_width = pair(patch_size)\n","\n","        assert image_height % patch_height == 0 and image_width % patch_width == 0, \\\n","            'Image dimensions must be divisible by the patch size.'\n","\n","        num_patches = (image_height // patch_height) * (image_width // patch_width)\n","        patch_dim = channels * patch_height * patch_width\n","\n","        self.patch_size = patch_size\n","        self.dim = dim\n","\n","        self.to_patch_embedding = nn.Sequential(\n","            Rearrange('b c (h ph) (w pw) -> b (h w) (ph pw c)', ph=patch_height, pw=patch_width),\n","            nn.Linear(patch_dim, dim)\n","        )\n","\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n","        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n","        self.dropout = nn.Dropout(emb_dropout)\n","\n","        self.transformer = nn.ModuleList([])\n","        for _ in range(depth):\n","            self.transformer.append(nn.ModuleList([\n","                nn.LayerNorm(dim),\n","                Attention(dim, dim_head=dim_head, heads=heads, dropout=dropout),\n","                nn.LayerNorm(dim),\n","                FeedForward(dim, hidden_dim=mlp_dim, dropout=dropout)\n","            ]))\n","\n","        self.mlp_head = nn.Sequential(\n","            nn.LayerNorm(dim),\n","            nn.Linear(dim, num_classes)\n","        )\n","\n","    def forward(self, img):\n","        x = self.to_patch_embedding(img)\n","        B, N, _ = x.shape\n","\n","        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, dim)\n","        x = torch.cat((cls_tokens, x), dim=1)  # (B, N+1, dim)\n","        x = x + self.pos_embedding[:, :N + 1, :]\n","        x = self.dropout(x)\n","\n","        for norm1, attn, norm2, ff in self.transformer:\n","            x = x + attn(norm1(x))\n","            x = x + ff(norm2(x))\n","\n","        x = x[:, 0]\n","        x = self.mlp_head(x)\n","        return x\n","\n","def main():\n","    # Set random seeds for reproducibility\n","    torch.manual_seed(42)\n","    torch.cuda.manual_seed_all(42)\n","    np.random.seed(42)\n","\n","    # Initialize wandb\n","    wandb.init(project='vit-mnist', config={\n","        'model': 'ViT',\n","        'dataset': 'MNIST',\n","        'epochs': 100,\n","        'batch_size': 128,  # Batch size for MNIST\n","        'learning_rate': 3e-4,\n","        'weight_decay': 1e-4,\n","        'image_size': 28,   # MNIST image size\n","        'patch_size': 4,    # Patch size for MNIST (28 / 4 = 7 patches per dimension)\n","        'dim': 384,         # Model dimension\n","        'depth': 6,         # Transformer depth\n","        'heads': 6,         # Number of heads\n","        'mlp_dim': 384 * 4, # MLP hidden dimension\n","        'dropout': 0.1,\n","        'emb_dropout': 0.1,\n","        'num_classes': 10\n","    })\n","    config = wandb.config\n","\n","    # Set device\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    # Data transforms\n","    transform_train = transforms.Compose([\n","        transforms.RandomCrop(config.image_size, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.1307,), (0.3081,)),  # MNIST mean and std\n","    ])\n","\n","    transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.1307,), (0.3081,)),\n","    ])\n","\n","    # Load MNIST dataset\n","    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform_train)\n","    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform_test)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n","    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n","\n","    # Initialize model\n","    model = ViT(\n","        image_size=config.image_size,\n","        patch_size=config.patch_size,\n","        num_classes=config.num_classes,\n","        dim=config.dim,\n","        depth=config.depth,\n","        heads=config.heads,\n","        mlp_dim=config.mlp_dim,\n","        dropout=config.dropout,\n","        emb_dropout=config.emb_dropout,\n","        channels=1  # MNIST is grayscale\n","    ).to(device)\n","\n","    # Loss function and optimizer\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n","    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs)\n","\n","    # Training loop\n","    best_acc = 0.0\n","    for epoch in range(config.epochs):\n","        model.train()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        for batch_idx, (inputs, targets) in enumerate(train_loader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","            if batch_idx % 100 == 0:\n","                wandb.log({\n","                    'train_loss': running_loss / (batch_idx + 1),\n","                    'train_acc': 100. * correct / total,\n","                    'learning_rate': optimizer.param_groups[0]['lr']\n","                })\n","\n","        # Print training progress\n","        train_loss = running_loss / len(train_loader)\n","        train_acc = 100. * correct / total\n","        print(f\"Epoch {epoch + 1}/{config.epochs} - \"\n","              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n","\n","        # Validation\n","        model.eval()\n","        test_loss = 0.0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, targets in test_loader:\n","                inputs, targets = inputs.to(device), targets.to(device)\n","                outputs = model(inputs)\n","                loss = criterion(outputs, targets)\n","\n","                test_loss += loss.item()\n","                _, predicted = outputs.max(1)\n","                total += targets.size(0)\n","                correct += predicted.eq(targets).sum().item()\n","\n","        acc = 100. * correct / total\n","        avg_test_loss = test_loss / len(test_loader)\n","        wandb.log({\n","            'test_loss': avg_test_loss,\n","            'test_acc': acc,\n","            'epoch': epoch + 1\n","        })\n","\n","        print(f\"Test Loss: {avg_test_loss:.4f}, Test Acc: {acc:.2f}%\")\n","\n","        # Save best model\n","        if acc > best_acc:\n","            best_acc = acc\n","            torch.save(model.state_dict(), 'best_mnist_vit.pth')\n","\n","        scheduler.step()\n","\n","    print(f\"Training completed. Best Test Accuracy: {best_acc:.2f}%\")\n","    wandb.finish()\n","\n","if __name__ == '__main__':\n","    main()\n"]},{"cell_type":"code","source":[],"metadata":{"id":"CjXv6F-QDMnt"},"execution_count":null,"outputs":[]}]}