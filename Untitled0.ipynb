{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyOGGQQlllX0y0hFG/15kyds"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"SIEoUF8laCFW","executionInfo":{"status":"ok","timestamp":1732792002138,"user_tz":300,"elapsed":9592,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"}}},"outputs":[],"source":["import torch\n","from torch import nn, optim\n","from torch.utils.data import DataLoader\n","import torchvision\n","from torchvision import datasets, transforms\n","import wandb\n","import os\n","import numpy as np\n","import torch.nn.functional as F\n","from einops import rearrange\n","from einops.layers.torch import Rearrange\n","import torch.nn.utils.parametrize as parametrize\n","import math"]},{"cell_type":"code","source":["# Set random seeds for reproducibility\n","torch.manual_seed(42)\n","torch.cuda.manual_seed_all(42)\n","np.random.seed(42)\n","\n","# Helper functions\n","def exists(v):\n","    return v is not None\n","\n","def default(v, d):\n","    return v if exists(v) else d\n","\n","def pair(t):\n","    return t if isinstance(t, tuple) else (t, t)\n","\n","def divisible_by(numer, denom):\n","    return (numer % denom) == 0\n","\n","def l2norm(t, dim=-1):\n","    return F.normalize(t, dim=dim, p=2)\n"],"metadata":{"id":"-hgbzHJjaI6e","executionInfo":{"status":"ok","timestamp":1732792002139,"user_tz":300,"elapsed":39,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# For use with parametrize\n","class L2Norm(nn.Module):\n","    def __init__(self, dim=-1):\n","        super().__init__()\n","        self.dim = dim\n","\n","    def forward(self, t):\n","        return l2norm(t, dim=self.dim)\n","\n","class NormLinear(nn.Module):\n","    def __init__(self, dim, dim_out, norm_dim_in=True):\n","        super().__init__()\n","        self.linear = nn.Linear(dim, dim_out, bias=False)\n","\n","        parametrize.register_parametrization(\n","            self.linear,\n","            'weight',\n","            L2Norm(dim=-1 if norm_dim_in else 0)\n","        )\n","\n","    @property\n","    def weight(self):\n","        return self.linear.weight\n","\n","    def forward(self, x):\n","        return self.linear(x)\n","\n","# Scaled dot product attention function\n","def scaled_dot_product_attention(q, k, v, dropout_p=0., training=True):\n","    d_k = q.size(-1)\n","    attn_weights = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n","    attn_weights = F.softmax(attn_weights, dim=-1)\n","    if training and dropout_p > 0.0:\n","        attn_weights = F.dropout(attn_weights, p=dropout_p)\n","    output = torch.matmul(attn_weights, v)\n","    return output"],"metadata":{"id":"oxkTM5_xaMI8","executionInfo":{"status":"ok","timestamp":1732792002140,"user_tz":300,"elapsed":39,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Attention and FeedForward classes\n","class Attention(nn.Module):\n","    def __init__(self, dim, *, dim_head=64, heads=8, dropout=0.):\n","        super().__init__()\n","        dim_inner = dim_head * heads\n","        self.to_q = NormLinear(dim, dim_inner)\n","        self.to_k = NormLinear(dim, dim_inner)\n","        self.to_v = NormLinear(dim, dim_inner)\n","\n","        self.dropout = dropout\n","\n","        self.q_scale = nn.Parameter(torch.ones(heads, 1, dim_head) * (dim_head ** 0.25))\n","        self.k_scale = nn.Parameter(torch.ones(heads, 1, dim_head) * (dim_head ** 0.25))\n","\n","        self.split_heads = Rearrange('b n (h d) -> b h n d', h=heads)\n","        self.merge_heads = Rearrange('b h n d -> b n (h d)')\n","\n","        self.to_out = NormLinear(dim_inner, dim, norm_dim_in=False)\n","\n","    def forward(self, x):\n","        q, k, v = self.to_q(x), self.to_k(x), self.to_v(x)\n","\n","        q, k, v = map(self.split_heads, (q, k, v))\n","\n","        # Query key rmsnorm\n","        q, k = map(l2norm, (q, k))\n","\n","        q = q * self.q_scale\n","        k = k * self.k_scale\n","\n","        out = scaled_dot_product_attention(\n","            q, k, v,\n","            dropout_p=self.dropout,\n","            training=self.training\n","        )\n","\n","        out = self.merge_heads(out)\n","        return self.to_out(out)\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, dim, *, dim_inner, dropout=0.):\n","        super().__init__()\n","        dim_inner = int(dim_inner * 2 / 3)\n","\n","        self.dim = dim\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.to_hidden = NormLinear(dim, dim_inner)\n","        self.to_gate = NormLinear(dim, dim_inner)\n","\n","        self.hidden_scale = nn.Parameter(torch.ones(dim_inner))\n","        self.gate_scale = nn.Parameter(torch.ones(dim_inner))\n","\n","        self.to_out = NormLinear(dim_inner, dim, norm_dim_in=False)\n","\n","    def forward(self, x):\n","        hidden, gate = self.to_hidden(x), self.to_gate(x)\n","\n","        hidden = hidden * self.hidden_scale\n","        gate = gate * self.gate_scale * (self.dim ** 0.5)\n","\n","        hidden = F.silu(gate) * hidden\n","\n","        hidden = self.dropout(hidden)\n","        return self.to_out(hidden)"],"metadata":{"id":"18bmT4uIaRzA","executionInfo":{"status":"ok","timestamp":1732792002140,"user_tz":300,"elapsed":39,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# nViT base class\n","class nViT(nn.Module):\n","    def __init__(\n","        self,\n","        *,\n","        image_size,\n","        patch_size,\n","        num_classes,\n","        dim,\n","        depth,\n","        heads,\n","        mlp_dim,\n","        dropout=0.,\n","        channels=3,\n","        dim_head=64,\n","        residual_lerp_scale_init=None\n","    ):\n","        super().__init__()\n","        image_height, image_width = pair(image_size)\n","\n","        assert divisible_by(image_height, patch_size) and divisible_by(image_width, patch_size), 'Image dimensions must be divisible by the patch size.'\n","\n","        patch_height_dim, patch_width_dim = (image_height // patch_size), (image_width // patch_size)\n","        patch_dim = channels * (patch_size ** 2)\n","        num_patches = patch_height_dim * patch_width_dim\n","\n","        self.channels = channels\n","        self.patch_size = patch_size\n","        self.num_patches = num_patches\n","        self.image_size = image_size\n","\n","        self.to_patch_embedding = nn.Sequential(\n","            Rearrange('b c (h p1) (w p2) -> b (h w) (c p1 p2)', p1=patch_size, p2=patch_size),\n","            NormLinear(patch_dim, dim, norm_dim_in=False),\n","        )\n","\n","        self.abs_pos_emb = NormLinear(dim, num_patches)\n","\n","        residual_lerp_scale_init = default(residual_lerp_scale_init, 1. / depth)\n","\n","        self.dim = dim\n","        self.scale = dim ** 0.5\n","\n","        self.layers = nn.ModuleList([])\n","        self.residual_lerp_scales = nn.ModuleList([])\n","\n","        for _ in range(depth):\n","            self.layers.append(nn.ModuleList([\n","                Attention(dim, dim_head=dim_head, heads=heads, dropout=dropout),\n","                FeedForward(dim, dim_inner=mlp_dim, dropout=dropout),\n","            ]))\n","\n","            self.residual_lerp_scales.append(nn.ParameterList([\n","                nn.Parameter(torch.ones(dim) * residual_lerp_scale_init / self.scale),\n","                nn.Parameter(torch.ones(dim) * residual_lerp_scale_init / self.scale),\n","            ]))\n","\n","        # Classification head\n","        self.mlp_head = nn.Sequential(\n","            nn.LayerNorm(dim),\n","            nn.Linear(dim, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        device = x.device\n","\n","        tokens = self.to_patch_embedding(x)\n","\n","        seq_len = tokens.shape[-2]\n","        pos_emb = self.abs_pos_emb.weight[torch.arange(seq_len, device=device)]\n","\n","        tokens = l2norm(tokens + pos_emb)\n","\n","        for (attn, ff), residual_scales in zip(self.layers, self.residual_lerp_scales):\n","            attn_alpha, ff_alpha = residual_scales\n","\n","            attn_out = l2norm(attn(tokens))\n","            tokens = l2norm(tokens.lerp(attn_out, attn_alpha * self.scale))\n","\n","            ff_out = l2norm(ff(tokens))\n","            tokens = l2norm(tokens.lerp(ff_out, ff_alpha * self.scale))\n","\n","        # Classification token (mean pooling)\n","        tokens = tokens.mean(dim=1)\n","        logits = self.mlp_head(tokens)\n","        return logits"],"metadata":{"id":"sN9kFE3baSc3","executionInfo":{"status":"ok","timestamp":1732792002141,"user_tz":300,"elapsed":39,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def main():\n","    # Initialize wandb\n","    wandb.init(project='nvit-cifar100', config={\n","        'model': 'nViT',\n","        'dataset': 'CIFAR-100',\n","        'epochs': 100,\n","        'batch_size': 128,\n","        'learning_rate': 3e-4,\n","        'weight_decay': 1e-4,\n","        'image_size': 32,\n","        'patch_size': 4,\n","        'dim': 384,\n","        'depth': 8,\n","        'heads': 8,\n","        'mlp_dim': 384 * 4,\n","        'dropout': 0.1,\n","        'num_classes': 100,\n","        'dim_head': 64\n","    })\n","    config = wandb.config\n","\n","    # Set device\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    # Data transforms\n","    transform_train = transforms.Compose([\n","        transforms.RandomCrop(config.image_size, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5071, 0.4867, 0.4408),\n","                             (0.2675, 0.2565, 0.2761)),\n","    ])\n","\n","    transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5071, 0.4867, 0.4408),\n","                             (0.2675, 0.2565, 0.2761)),\n","    ])\n","\n","    # Load CIFAR-100 dataset\n","    train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n","    test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2, pin_memory=True)\n","    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n","\n","    # Initialize model\n","    model = nViT(\n","        image_size=config.image_size,\n","        patch_size=config.patch_size,\n","        num_classes=config.num_classes,\n","        dim=config.dim,\n","        depth=config.depth,\n","        heads=config.heads,\n","        mlp_dim=config.mlp_dim,\n","        dropout=config.dropout,\n","        dim_head=config.dim_head\n","    ).to(device)\n","\n","    # Loss function and optimizer\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n","    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs)\n","\n","    # Training loop\n","    best_acc = 0.0\n","    for epoch in range(config.epochs):\n","        model.train()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        for batch_idx, (inputs, targets) in enumerate(train_loader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","            if batch_idx % 100 == 0:\n","                wandb.log({\n","                    'train_loss': running_loss / (batch_idx + 1),\n","                    'train_acc': 100. * correct / total,\n","                    'learning_rate': optimizer.param_groups[0]['lr']\n","                })\n","\n","        train_acc = 100. * correct / total\n","\n","        # Validation\n","        model.eval()\n","        test_loss = 0.0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, targets in test_loader:\n","                inputs, targets = inputs.to(device), targets.to(device)\n","                outputs = model(inputs)\n","                loss = criterion(outputs, targets)\n","\n","                test_loss += loss.item()\n","                _, predicted = outputs.max(1)\n","                total += targets.size(0)\n","                correct += predicted.eq(targets).sum().item()\n","\n","        acc = 100. * correct / total\n","        avg_test_loss = test_loss / len(test_loader)\n","        wandb.log({\n","            'test_loss': avg_test_loss,\n","            'test_acc': acc,\n","            'epoch': epoch\n","        })\n","\n","        print(f\"Epoch {epoch + 1}/{config.epochs} - \"\n","              f\"Train Loss: {running_loss / len(train_loader):.4f}, Train Acc: {train_acc:.2f}%, \"\n","              f\"Test Loss: {avg_test_loss:.4f}, Test Acc: {acc:.2f}%\")\n","\n","        # Save best model\n","        if acc > best_acc:\n","            best_acc = acc\n","            torch.save(model.state_dict(), os.path.join(wandb.run.dir, 'best_model.pth'))\n","\n","        scheduler.step()\n","\n","    print(f\"Training completed. Best Test Accuracy: {best_acc:.2f}%\")\n","    wandb.finish()\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":324},"id":"A98znO4BaU9o","outputId":"404d4b87-b31c-4567-9416-384f874a6212"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":[" ··········\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.18.7"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20241128_110653-h96j0x1m</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100/runs/h96j0x1m' target=\"_blank\">rosy-water-4</a></strong> to <a href='https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100/runs/h96j0x1m' target=\"_blank\">https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100/runs/h96j0x1m</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 169M/169M [00:09<00:00, 17.9MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-100-python.tar.gz to ./data\n","Files already downloaded and verified\n","Epoch 1/100 - Train Loss: 3.7337, Train Acc: 12.93%, Test Loss: 3.1524, Test Acc: 21.83%\n","Epoch 2/100 - Train Loss: 2.8073, Train Acc: 28.45%, Test Loss: 2.5423, Test Acc: 33.44%\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"LlMgDXK6aYIV"},"execution_count":null,"outputs":[]}]}