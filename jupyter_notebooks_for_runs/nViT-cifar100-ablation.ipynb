{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2966,"status":"ok","timestamp":1733631792046,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"},"user_tz":300},"id":"2N0PaH2oWQ_w","outputId":"f7505d3f-2e0b-4d5d-9ecf-35773d36959e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.60.0)\n","Requirement already satisfied: llvmlite\u003c0.44,\u003e=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.43.0)\n","Requirement already satisfied: numpy\u003c2.1,\u003e=1.22 in /usr/local/lib/python3.10/dist-packages (from numba) (1.26.4)\n"]}],"source":["!pip install numba\n","\n","from numba import cuda\n","device = cuda.get_current_device()\n","device.reset()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N3_WO4X0XK3p"},"outputs":[],"source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ptRpPobnee_R"},"outputs":[],"source":["import os\n","os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W0VfEzDQl9Dq"},"outputs":[],"source":["%reset -f"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":10033,"status":"ok","timestamp":1733658045706,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"},"user_tz":300},"id":"R3SPTnn24TEe"},"outputs":[],"source":["import torch\n","from torch import nn, optim\n","from torch.utils.data import DataLoader\n","import torchvision\n","from torchvision import datasets, transforms\n","import wandb\n","import os\n","import numpy as np\n","import torch.nn.functional as F\n","from einops.layers.torch import Rearrange\n","import torch.nn.utils.parametrize as parametrize\n","import math\n","import random\n","import copy"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1733658045707,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"},"user_tz":300},"id":"gWnrMTkm4aRW"},"outputs":[],"source":["####################\n","# Helper Functions #\n","####################\n","\n","def set_seed(seed=42):\n","    \"\"\"\n","    Set random seeds for reproducibility.\n","    \"\"\"\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","def exists(v):\n","    return v is not None\n","\n","def default(v, d):\n","    return v if exists(v) else d\n","\n","def pair(t):\n","    return t if isinstance(t, tuple) else (t, t)\n","\n","def divisible_by(numer, denom):\n","    return (numer % denom) == 0\n","\n","def l2norm(t, dim=-1):\n","    return F.normalize(t, dim=dim, p=2)\n","\n","#############################\n","# Parametrization Classes  #\n","#############################\n","\n","# L2Norm for parametrize\n","class L2Norm(nn.Module):\n","    def __init__(self, dim=-1):\n","        super().__init__()\n","        self.dim = dim\n","    def forward(self, t):\n","        return l2norm(t, dim=self.dim)\n","\n","class NormLinear(nn.Module):\n","    def __init__(self, dim, dim_out, norm_dim_in=True):\n","        super().__init__()\n","        self.linear = nn.Linear(dim, dim_out, bias=False)\n","        parametrize.register_parametrization(\n","            self.linear,\n","            'weight',\n","            L2Norm(dim=-1 if norm_dim_in else 0)\n","        )\n","\n","    @property\n","    def weight(self):\n","        return self.linear.weight\n","\n","    def forward(self, x):\n","        return self.linear(x)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1733658045707,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"},"user_tz":300},"id":"SEKwt7qd4dxV"},"outputs":[],"source":["####################\n","# ConvStem Module  #\n","####################\n","\n","class ConvStem(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels // 2, kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm2d(out_channels // 2),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels // 2, out_channels, kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","    def forward(self, x):\n","        return self.conv(x)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1733658045707,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"},"user_tz":300},"id":"XxlceIfI4jSI"},"outputs":[],"source":["###############################\n","# Scaled Dot Product Attention#\n","###############################\n","\n","def scaled_dot_product_attention(q, k, v, dropout_p=0., training=True):\n","    d_k = q.size(-1)\n","    attn_weights = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n","    attn_weights = F.softmax(attn_weights, dim=-1)\n","    if training and dropout_p \u003e 0.0:\n","        attn_weights = F.dropout(attn_weights, p=dropout_p)\n","    output = torch.matmul(attn_weights, v)\n","    return output\n","\n","class Attention(nn.Module):\n","    def __init__(self, dim, *, dim_head=64, heads=8, dropout=0.):\n","        super().__init__()\n","        dim_inner = dim_head * heads\n","        self.to_q = NormLinear(dim, dim_inner)\n","        self.to_k = NormLinear(dim, dim_inner)\n","        self.to_v = NormLinear(dim, dim_inner)\n","        self.dropout = dropout\n","        self.q_scale = nn.Parameter(torch.ones(heads,1,dim_head)*(dim_head**0.25))\n","        self.k_scale = nn.Parameter(torch.ones(heads,1,dim_head)*(dim_head**0.25))\n","        self.split_heads = Rearrange('b n (h d) -\u003e b h n d', h=heads)\n","        self.merge_heads = Rearrange('b h n d -\u003e b n (h d)')\n","        self.to_out = NormLinear(dim_inner, dim, norm_dim_in=False)\n","\n","    def forward(self,x):\n","        q,k,v = self.to_q(x), self.to_k(x), self.to_v(x)\n","        q,k,v = map(self.split_heads, (q,k,v))\n","        q,k = map(l2norm,(q,k))\n","        q = q*self.q_scale\n","        k = k*self.k_scale\n","        out = scaled_dot_product_attention(q,k,v, dropout_p=self.dropout, training=self.training)\n","        out = self.merge_heads(out)\n","        return self.to_out(out)\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, dim, *, dim_inner, dropout=0.):\n","        super().__init__()\n","        dim_inner = int(dim_inner*2/3)\n","        self.dim = dim\n","        self.dropout = nn.Dropout(dropout)\n","        self.to_hidden = NormLinear(dim, dim_inner)\n","        self.to_gate = NormLinear(dim, dim_inner)\n","        self.hidden_scale = nn.Parameter(torch.ones(dim_inner))\n","        self.gate_scale = nn.Parameter(torch.ones(dim_inner))\n","        self.to_out = NormLinear(dim_inner, dim, norm_dim_in=False)\n","\n","    def forward(self,x):\n","        hidden, gate = self.to_hidden(x), self.to_gate(x)\n","        hidden = hidden*self.hidden_scale\n","        gate = gate*self.gate_scale*(self.dim**0.5)\n","        hidden = F.silu(gate)*hidden\n","        hidden = self.dropout(hidden)\n","        return self.to_out(hidden)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1733658045707,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"},"user_tz":300},"id":"XhMWanhx4mOv"},"outputs":[],"source":["###################################\n","# nViT Model with Optional ConvStem#\n","###################################\n","\n","class nViT(nn.Module):\n","    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, dropout=0., channels=3, dim_head=64, residual_lerp_scale_init=None, use_convstem=False):\n","        super().__init__()\n","        assert divisible_by(image_size, patch_size), \"Image must be divisible by patch_size\"\n","        num_patches = (image_size//patch_size)*(image_size//patch_size)\n","        self.use_convstem = use_convstem\n","        self.dim = dim\n","        if use_convstem:\n","            # With ConvStem\n","            self.conv_stem = ConvStem(channels, dim)\n","            self.to_patch_embedding = nn.Sequential(\n","                Rearrange('b c h w -\u003e b (h w) c'),\n","                NormLinear(dim,dim,norm_dim_in=False),\n","            )\n","        else:\n","            # Without ConvStem\n","            patch_dim = channels*(patch_size**2)\n","            self.to_patch_embedding = nn.Sequential(\n","                Rearrange('b c (h p1) (w p2)-\u003e b (h w) (c p1 p2)', p1=patch_size, p2=patch_size),\n","                NormLinear(patch_dim, dim, norm_dim_in=False),\n","            )\n","        self.abs_pos_emb = NormLinear(dim, num_patches)\n","        residual_lerp_scale_init = default(residual_lerp_scale_init, 1./depth)\n","        self.scale = dim**0.5\n","\n","        self.layers = nn.ModuleList([])\n","        self.residual_lerp_scales = nn.ModuleList([])\n","        for _ in range(depth):\n","            self.layers.append(nn.ModuleList([\n","                Attention(dim, dim_head=dim_head, heads=heads, dropout=dropout),\n","                FeedForward(dim, dim_inner=mlp_dim, dropout=dropout)\n","            ]))\n","            self.residual_lerp_scales.append(nn.ParameterList([\n","                nn.Parameter(torch.ones(dim)*residual_lerp_scale_init/self.scale),\n","                nn.Parameter(torch.ones(dim)*residual_lerp_scale_init/self.scale),\n","            ]))\n","\n","        self.mlp_head = nn.Sequential(\n","            nn.LayerNorm(dim),\n","            nn.Linear(dim, num_classes)\n","        )\n","\n","    def forward(self,x):\n","        device = x.device\n","        if self.use_convstem:\n","            x = self.conv_stem(x) # shape: B, dim, H/4, W/4 if patch_size=4\n","            tokens = self.to_patch_embedding(x)\n","        else:\n","            tokens = self.to_patch_embedding(x)\n","\n","        seq_len = tokens.shape[-2]\n","        pos_emb = self.abs_pos_emb.weight[torch.arange(seq_len,device=device)]\n","        tokens = l2norm(tokens+pos_emb)\n","        for (attn, ff), residual_scales in zip(self.layers, self.residual_lerp_scales):\n","            attn_alpha, ff_alpha = residual_scales\n","            attn_out = l2norm(attn(tokens))\n","            tokens = l2norm(tokens.lerp(attn_out, attn_alpha*self.scale))\n","            ff_out = l2norm(ff(tokens))\n","            tokens = l2norm(tokens.lerp(ff_out, ff_alpha*self.scale))\n","        tokens = tokens.mean(dim=1)\n","        return self.mlp_head(tokens)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1733658045707,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"},"user_tz":300},"id":"sNMPynrm4o3z"},"outputs":[],"source":["#####################\n","# MixUp \u0026 CutMix    #\n","#####################\n","\n","def mixup_data(x, y, alpha=0.2):\n","    \"\"\"\n","    Returns mixed inputs, pairs of targets, and lambda\n","    \"\"\"\n","    if alpha \u003e 0:\n","        lam = np.random.beta(alpha, alpha)\n","    else:\n","        lam = 1\n","    batch_size = x.size(0)\n","    index = torch.randperm(batch_size).to(x.device)\n","    mixed_x = lam * x + (1 - lam) * x[index, :]\n","    y_a, y_b = y, y[index]\n","    return mixed_x, y_a, y_b, lam\n","\n","def mixup_criterion(criterion, pred, y_a, y_b, lam):\n","    \"\"\"\n","    Compute the MixUp loss\n","    \"\"\"\n","    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1733658045707,"user":{"displayName":"Pratyush Kumar","userId":"15062819489477391288"},"user_tz":300},"id":"95yvKGNX4uoN"},"outputs":[],"source":["###############################\n","# Experiment Runner Function  #\n","###############################\n","\n","def run_experiment(config):\n","    \"\"\"\n","    Runs a single experiment based on the provided configuration.\n","    Logs metrics and model checkpoints to wandb.\n","    \"\"\"\n","    # Construct run name based on configuration\n","    run_name = f\"nViT_CIFAR100_convstem={config['use_convstem']}_mixup={config['use_mixup']}_aug={config['augment_level']}\"\n","    wandb.init(project='nvit-cifar100-ablation', config=config, name=run_name)\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"\\nRunning Experiment: {run_name} on device: {device}\\n\")\n","\n","    # Data augmentation based on config\n","    if config['augment_level'] == 'baseline':\n","        # Minimal augmentations\n","        train_transform = transforms.Compose([\n","            transforms.RandomCrop(config['image_size'], padding=4),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5071, 0.4867, 0.4408),\n","                                 (0.2675, 0.2565, 0.2761)),\n","        ])\n","    elif config['augment_level'] == 'advanced':\n","        # Advanced augmentations\n","        train_transform = transforms.Compose([\n","            transforms.RandomCrop(config['image_size'], padding=4),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","            transforms.RandomRotation(15),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5071, 0.4867, 0.4408),\n","                                 (0.2675, 0.2565, 0.2761)),\n","        ])\n","    else:\n","        # Default to baseline if unknown\n","        train_transform = transforms.Compose([\n","            transforms.RandomCrop(config['image_size'], padding=4),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5071, 0.4867, 0.4408),\n","                                 (0.2675, 0.2565, 0.2761)),\n","        ])\n","\n","    test_transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5071, 0.4867, 0.4408),\n","                             (0.2675, 0.2565, 0.2761)),\n","    ])\n","\n","    # Load CIFAR-100 dataset\n","    train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n","    test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True,\n","                              num_workers=2, pin_memory=True)\n","    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False,\n","                             num_workers=2, pin_memory=True)\n","\n","    # Initialize model\n","    model = nViT(\n","        image_size=config['image_size'],\n","        patch_size=config['patch_size'],\n","        num_classes=config['num_classes'],\n","        dim=config['dim'],\n","        depth=config['depth'],\n","        heads=config['heads'],\n","        mlp_dim=config['mlp_dim'],\n","        dropout=config['dropout'],\n","        dim_head=config['dim_head'],\n","        use_convstem=config['use_convstem']\n","    ).to(device)\n","\n","    # Define Loss Function and Optimizer\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'],\n","                            weight_decay=config['weight_decay'])\n","    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['epochs'])\n","\n","    # Early Stopping Parameters\n","    best_acc = 0.0\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    patience = config['patience']\n","    trigger_times = 0\n","\n","    # Training Loop\n","    for epoch in range(config['epochs']):\n","        model.train()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        for batch_idx, (inputs, targets) in enumerate(train_loader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","\n","            # Apply MixUp if enabled\n","            if config['use_mixup']:\n","                inputs, y_a, y_b, lam = mixup_data(inputs, targets, alpha=0.2)\n","                outputs = model(inputs)\n","                loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)\n","            else:\n","                outputs = model(inputs)\n","                loss = criterion(outputs, targets)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient Clipping\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            if config['use_mixup']:\n","                # Approximate correct predictions with MixUp\n","                correct_preds = lam * predicted.eq(y_a).sum().item() + (1 - lam) * predicted.eq(y_b).sum().item()\n","                correct += correct_preds\n","            else:\n","                correct += predicted.eq(targets).sum().item()\n","\n","            # Logging intermediate batch metrics\n","            if batch_idx % 100 == 0:\n","                wandb.log({\n","                    'train_loss': running_loss / (batch_idx + 1),\n","                    'train_acc': 100. * correct / total,\n","                    'learning_rate': optimizer.param_groups[0]['lr']\n","                })\n","\n","        # Epoch-wise Training Metrics\n","        train_loss = running_loss / len(train_loader)\n","        train_acc = 100. * correct / total\n","\n","        # Validation Phase\n","        model.eval()\n","        test_loss = 0.0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, targets in test_loader:\n","                inputs, targets = inputs.to(device), targets.to(device)\n","                outputs = model(inputs)\n","                loss = criterion(outputs, targets)\n","\n","                test_loss += loss.item()\n","                _, predicted = outputs.max(1)\n","                total += targets.size(0)\n","                correct += predicted.eq(targets).sum().item()\n","\n","        # Epoch-wise Validation Metrics\n","        acc = 100. * correct / total\n","        avg_test_loss = test_loss / len(test_loader)\n","        wandb.log({\n","            'epoch': epoch,\n","            'test_loss': avg_test_loss,\n","            'test_acc': acc\n","        })\n","\n","        print(f\"Epoch {epoch + 1}/{config['epochs']} - \"\n","              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n","              f\"Test Loss: {avg_test_loss:.4f}, Test Acc: {acc:.2f}%\")\n","\n","        # Early Stopping Logic\n","        if acc \u003e best_acc:\n","            best_acc = acc\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","            trigger_times = 0\n","            # Save the best model\n","            torch.save(model.state_dict(), f'best_nvit_{run_name}.pth')\n","        else:\n","            trigger_times += 1\n","            if trigger_times \u003e= patience:\n","                print(\"Early stopping triggered!\")\n","                break\n","\n","        # Step the scheduler\n","        scheduler.step()\n","\n","        # Log best accuracy so far\n","        wandb.log({'best_test_acc': best_acc})\n","\n","    # Load Best Model Weights\n","    model.load_state_dict(best_model_wts)\n","    print(f\"Training completed. Best Test Accuracy: {best_acc:.2f}%\")\n","    wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"wwYxZsMN40Ax"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Starting Experiment 1/1\n"]},{"data":{"text/html":["Finishing last run (ID:8z2kucfz) before initializing another..."],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"098fb529d4fd4ef9954488c5820cd818","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.013 MB of 0.013 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    \u003cstyle\u003e\n","        .wandb-row {\n","            display: flex;\n","            flex-direction: row;\n","            flex-wrap: wrap;\n","            justify-content: flex-start;\n","            width: 100%;\n","        }\n","        .wandb-col {\n","            display: flex;\n","            flex-direction: column;\n","            flex-basis: 100%;\n","            flex: 1;\n","            padding: 10px;\n","        }\n","    \u003c/style\u003e\n","\u003cdiv class=\"wandb-row\"\u003e\u003cdiv class=\"wandb-col\"\u003e\u003ch3\u003eRun history:\u003c/h3\u003e\u003cbr/\u003e\u003ctable class=\"wandb\"\u003e\u003ctr\u003e\u003ctd\u003elearning_rate\u003c/td\u003e\u003ctd\u003e▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain_acc\u003c/td\u003e\u003ctd\u003e▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain_loss\u003c/td\u003e\u003ctd\u003e▁\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\u003cbr/\u003e\u003c/div\u003e\u003cdiv class=\"wandb-col\"\u003e\u003ch3\u003eRun summary:\u003c/h3\u003e\u003cbr/\u003e\u003ctable class=\"wandb\"\u003e\u003ctr\u003e\u003ctd\u003elearning_rate\u003c/td\u003e\u003ctd\u003e0.0003\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain_acc\u003c/td\u003e\u003ctd\u003e0.78125\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain_loss\u003c/td\u003e\u003ctd\u003e4.8909\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\u003cbr/\u003e\u003c/div\u003e\u003c/div\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run \u003cstrong style=\"color:#cdcd00\"\u003enViT_CIFAR100_convstem=True_mixup=False_aug=advanced\u003c/strong\u003e at: \u003ca href='https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100-ablation/runs/8z2kucfz' target=\"_blank\"\u003ehttps://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100-ablation/runs/8z2kucfz\u003c/a\u003e\u003cbr/\u003e View project at: \u003ca href='https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100-ablation' target=\"_blank\"\u003ehttps://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100-ablation\u003c/a\u003e\u003cbr/\u003eSynced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: \u003ccode\u003e./wandb/run-20241208_215200-8z2kucfz/logs\u003c/code\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:8z2kucfz). Initializing new run:\u003cbr/\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.7"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in \u003ccode\u003e/content/wandb/run-20241208_225758-8d73kng6\u003c/code\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run \u003cstrong\u003e\u003ca href='https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100-ablation/runs/8d73kng6' target=\"_blank\"\u003enViT_CIFAR100_convstem=True_mixup=False_aug=advanced\u003c/a\u003e\u003c/strong\u003e to \u003ca href='https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100-ablation' target=\"_blank\"\u003eWeights \u0026 Biases\u003c/a\u003e (\u003ca href='https://wandb.me/developer-guide' target=\"_blank\"\u003edocs\u003c/a\u003e)\u003cbr/\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at \u003ca href='https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100-ablation' target=\"_blank\"\u003ehttps://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100-ablation\u003c/a\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at \u003ca href='https://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100-ablation/runs/8d73kng6' target=\"_blank\"\u003ehttps://wandb.ai/pratkumar-stony-brook-university/nvit-cifar100-ablation/runs/8d73kng6\u003c/a\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Running Experiment: nViT_CIFAR100_convstem=True_mixup=False_aug=advanced on device: cuda\n","\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Epoch 1/100 - Train Loss: 3.9015, Train Acc: 9.64%, Test Loss: 3.4737, Test Acc: 15.62%\n","Epoch 2/100 - Train Loss: 3.2741, Train Acc: 19.93%, Test Loss: 2.9250, Test Acc: 25.94%\n","Epoch 3/100 - Train Loss: 2.8576, Train Acc: 27.54%, Test Loss: 2.6162, Test Acc: 32.60%\n","Epoch 4/100 - Train Loss: 2.5658, Train Acc: 33.92%, Test Loss: 2.3554, Test Acc: 38.32%\n","Epoch 5/100 - Train Loss: 2.3349, Train Acc: 38.48%, Test Loss: 2.2077, Test Acc: 41.93%\n","Epoch 6/100 - Train Loss: 2.1345, Train Acc: 43.15%, Test Loss: 2.0475, Test Acc: 45.51%\n","Epoch 7/100 - Train Loss: 1.9598, Train Acc: 47.40%, Test Loss: 1.9054, Test Acc: 48.93%\n","Epoch 8/100 - Train Loss: 1.8144, Train Acc: 50.28%, Test Loss: 1.8298, Test Acc: 50.43%\n","Epoch 9/100 - Train Loss: 1.6642, Train Acc: 54.04%, Test Loss: 1.7566, Test Acc: 51.92%\n","Epoch 10/100 - Train Loss: 1.5371, Train Acc: 56.82%, Test Loss: 1.6887, Test Acc: 54.25%\n","Epoch 11/100 - Train Loss: 1.3988, Train Acc: 60.39%, Test Loss: 1.6599, Test Acc: 55.44%\n","Epoch 12/100 - Train Loss: 1.2791, Train Acc: 63.09%, Test Loss: 1.6402, Test Acc: 56.11%\n","Epoch 13/100 - Train Loss: 1.1496, Train Acc: 66.37%, Test Loss: 1.6855, Test Acc: 55.29%\n","Epoch 14/100 - Train Loss: 1.0328, Train Acc: 69.39%, Test Loss: 1.6105, Test Acc: 57.58%\n","Epoch 15/100 - Train Loss: 0.9283, Train Acc: 72.13%, Test Loss: 1.6180, Test Acc: 58.33%\n","Epoch 16/100 - Train Loss: 0.8294, Train Acc: 74.96%, Test Loss: 1.6850, Test Acc: 57.62%\n","Epoch 17/100 - Train Loss: 0.7360, Train Acc: 77.58%, Test Loss: 1.7030, Test Acc: 57.73%\n","Epoch 18/100 - Train Loss: 0.6564, Train Acc: 79.69%, Test Loss: 1.7216, Test Acc: 58.09%\n","Epoch 19/100 - Train Loss: 0.5807, Train Acc: 81.82%, Test Loss: 1.7464, Test Acc: 58.44%\n","Epoch 20/100 - Train Loss: 0.5313, Train Acc: 83.37%, Test Loss: 1.8217, Test Acc: 57.55%\n","Epoch 21/100 - Train Loss: 0.4724, Train Acc: 85.23%, Test Loss: 1.8390, Test Acc: 58.22%\n","Epoch 22/100 - Train Loss: 0.4329, Train Acc: 86.38%, Test Loss: 1.9025, Test Acc: 57.69%\n","Epoch 23/100 - Train Loss: 0.3948, Train Acc: 87.41%, Test Loss: 1.8744, Test Acc: 58.25%\n","Epoch 24/100 - Train Loss: 0.3660, Train Acc: 88.28%, Test Loss: 1.9533, Test Acc: 58.11%\n","Epoch 25/100 - Train Loss: 0.3303, Train Acc: 89.56%, Test Loss: 2.0037, Test Acc: 58.05%\n","Epoch 26/100 - Train Loss: 0.3107, Train Acc: 90.12%, Test Loss: 2.0117, Test Acc: 58.83%\n","Epoch 27/100 - Train Loss: 0.2866, Train Acc: 90.74%, Test Loss: 2.0059, Test Acc: 58.84%\n","Epoch 28/100 - Train Loss: 0.2687, Train Acc: 91.29%, Test Loss: 2.0670, Test Acc: 58.57%\n"]}],"source":["#############################\n","# Experiment Configurations #\n","#############################\n","\n","if __name__ == '__main__':\n","    # Set seeds for reproducibility\n","    set_seed(42)\n","\n","    # Define the list of experiment configurations\n","    experiments = [\n","\n","        # 3. ConvStem only (baseline augmentations, no MixUp)\n","        {\n","            'epochs': 100,\n","            'batch_size': 128,\n","            'learning_rate': 3e-4,\n","            'weight_decay': 1e-4,\n","            'image_size': 32,\n","            'patch_size': 4,\n","            'dim': 512,\n","            'depth': 8,\n","            'heads': 8,\n","            'mlp_dim': 512 * 4,\n","            'dropout': 0.1,\n","            'num_classes': 100,\n","            'dim_head': 64,\n","            'patience': 20,\n","            'use_convstem': True,\n","            'use_mixup': False,\n","            'augment_level': 'baseline'  # Baseline augmentations\n","        },\n","\n","        # 4. ConvStem + MixUp (baseline augmentations)\n","        {\n","            'epochs': 100,\n","            'batch_size': 128,\n","            'learning_rate': 3e-4,\n","            'weight_decay': 1e-4,\n","            'image_size': 32,\n","            'patch_size': 4,\n","            'dim': 512,\n","            'depth': 8,\n","            'heads': 8,\n","            'mlp_dim': 512 * 4,\n","            'dropout': 0.1,\n","            'num_classes': 100,\n","            'dim_head': 64,\n","            'patience': 20,\n","            'use_convstem': True,\n","            'use_mixup': True,\n","            'augment_level': 'baseline'  # Baseline augmentations\n","        },\n","\n","        # 5. ConvStem + Advanced Augmentations (no MixUp)\n","        {\n","            'epochs': 100,\n","            'batch_size': 128,\n","            'learning_rate': 3e-4,\n","            'weight_decay': 1e-4,\n","            'image_size': 32,\n","            'patch_size': 4,\n","            'dim': 512,\n","            'depth': 8,\n","            'heads': 8,\n","            'mlp_dim': 512 * 4,\n","            'dropout': 0.1,\n","            'num_classes': 100,\n","            'dim_head': 64,\n","            'patience': 20,\n","            'use_convstem': True,\n","            'use_mixup': False,\n","            'augment_level': 'advanced'  # Advanced augmentations\n","        },\n","\n","    ]\n","\n","    # Optionally, remove any experiments you have already run to prevent duplication\n","    # For example, if you have already run ConvStem + Advanced Augmentations + MixUp,\n","    # ensure it's not in the list. Adjust the experiments list accordingly.\n","    # Run all experiments sequentially\n","    for idx, exp_cfg in enumerate(experiments):\n","        print(f\"\\nStarting Experiment {idx+1}/{len(experiments)}\")\n","        run_experiment(exp_cfg)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LRZkk7oGjZM9"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOrOmTjV3hHnQBMxqbAeDWd","gpuType":"L4","machine_shape":"hm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1f44a9cf4ceb4611980492c57068e93e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"564d6d60213f4a8ea19cf6621e7c8d3e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c3b128c09cc45fc86455f1022348288","placeholder":"​","style":"IPY_MODEL_1f44a9cf4ceb4611980492c57068e93e","value":"0.021 MB of 0.021 MB uploaded\r"}},"573c09b917404d8293627993ef3dc395":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_82c6982d9f8c494f82a2305fcc65a2ff","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7c9b814a2f614c6897a4f5cb0ef1d0ce","value":1}},"5c3b128c09cc45fc86455f1022348288":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c9b814a2f614c6897a4f5cb0ef1d0ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"82c6982d9f8c494f82a2305fcc65a2ff":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a25bdd77b3bb4e2c9d7b14bc373a607c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1d25268023c4914bdab0abb5f231204":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_564d6d60213f4a8ea19cf6621e7c8d3e","IPY_MODEL_573c09b917404d8293627993ef3dc395"],"layout":"IPY_MODEL_a25bdd77b3bb4e2c9d7b14bc373a607c"}}}}},"nbformat":4,"nbformat_minor":0}